
Title:
DBSCAN
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Density-based data clustering algorithm
.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}Part of a series onMachine learningand data mining
showProblems
Classification
Clustering
Regression
Anomaly detection
Data Cleaning
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction

showSupervised learning.mw-parser-output .nobold{font-weight:normal}(classificationÂ â¢ regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)

hideClustering
BIRCH
CURE
Hierarchical
k-means
Expectationâmaximization (EM)
DBSCAN
OPTICS
Mean shift

showDimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t-SNE

showStructured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

showAnomaly detection
k-NN
Local outlier factor

showArtificial neural network
Autoencoder
Cognitive computing
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
ESN
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net
Transformer
Vision
Spiking neural network
Memtransistor
Electrochemical RAM (ECRAM)

showReinforcement learning
Q-learning
SARSA
Temporal difference (TD)

showTheory
Kernel machines
Biasâvariance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory

showMachine-learning venues
NeurIPS
ICML
ML
JMLR
ArXiv:cs.LG

showRelated articles
Glossary of artificial intelligence
List of datasets for machine-learning research
Outline of machine learning
.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vte
Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, JÃ¶rg Sander and Xiaowei Xu in 1996.[1]
It is a density-based clustering non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).
DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.[2]
In 2014, the algorithm was awarded the test of time award (an award given to algorithms which have received substantial attention in theory and practice) at the leading data mining conference, ACM SIGKDD.[3] As of JulyÂ 2020[update], the follow-up paper "DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN"[4] appears in the list of the 8 most downloaded articles of the prestigious ACM Transactions on Database Systems (TODS) journal.[5]

Contents

1 History
2 Preliminary
3 Algorithm

3.1 Original query-based algorithm
3.2 Abstract algorithm


4 Complexity
5 Advantages
6 Disadvantages
7 Parameter estimation
8 Relationship to spectral clustering
9 Extensions
10 Availability
11 Notes
12 References



History[edit]
In 1972, Robert F. Ling published a closely related algorithm in "The Theory and Construction of k-Clusters"[6] in The Computer Journal with an estimated runtime complexity of O(nÂ³).[6] DBSCAN has a worst-case of O(nÂ²), and the database-oriented range-query formulation of DBSCAN allows for index acceleration. The algorithms slightly differ in their handling of border points.

Preliminary[edit]
Consider a set of points in some space to be clustered. Let Îµ be a parameter specifying the radius of a neighborhood with respect to some point. For the purpose of DBSCAN clustering, the points are classified as core points, (density-) reachable points and outliers, as follows:

A point p is a core point if at least minPts points are within distance Îµ of it (including p).
A point q is directly reachable from p if point q is within distance Îµ from core point p. Points are only said to be directly reachable from core points.
A point q is reachable from p if there is a path p1, ..., pn with p1 = p and pn = q, where each pi+1 is directly reachable from pi. Note that this implies that the initial point and all points on the path must be core points, with the possible exception of q.
All points not reachable from any other point are outliers or noise points.
Now if p is a core point, then it forms a cluster together with all points (core or non-core) that are reachable from it. Each cluster contains at least one core point; non-core points can be part of a cluster, but they form its "edge", since they cannot be used to reach more points.

  In this diagram, minPts = 4. Point A and the other red points are core points, because the area surrounding these points in an Îµ radius contain at least 4 points (including the point itself). Because they are all reachable from one another, they form a single cluster.  Points B and C are not core points, but are reachable from A (via other core points) and thus belong to the cluster as well. Point N is a noise point that is neither a core point nor directly-reachable.
Reachability is not a symmetric relation: by definition, only core points can reach non-core points. The opposite is not true, so a non-core point may be reachable, but nothing can be reached from it. Therefore, a further notion of connectedness is needed to formally define the extent of the clusters found by DBSCAN. Two points p and q are density-connected if there is a point o such that both p and q are reachable from o. Density-connectedness is symmetric.
A cluster then satisfies two properties:

All points within the cluster are mutually density-connected.
If a point is density-reachable from some point of the cluster, it is part of the cluster as well.
Algorithm[edit]
Original query-based algorithm[edit]
DBSCAN requires two parameters: Îµ (eps) and the minimum number of points required to form a dense region[a] (minPts). It starts with an arbitrary starting point that has not been visited. This point's Îµ-neighborhood is retrieved, and if it contains sufficiently many points, a cluster is started. Otherwise, the point is labeled as noise. Note that this point might later be found in a sufficiently sized Îµ-environment of a different point and hence be made part of a cluster.
If a point is found to be a dense part of a cluster, its Îµ-neighborhood is also part of that cluster. Hence, all points that are found within the Îµ-neighborhood are added, as is their own Îµ-neighborhood when they are also dense. This process continues until the density-connected cluster is completely found. Then, a new unvisited point is retrieved and processed, leading to the discovery of a further cluster or noise.
DBSCAN can be used with any distance function[1][4] (as well as similarity functions or other predicates).[7] The distance function (dist) can therefore be seen as an additional parameter.
The algorithm can be expressed in pseudocode as follows:[4]

DBSCAN(DB, distFunc, eps, minPts) {
    CÂ := 0                                                  /* Cluster counter */
    for each point P in database DB {
        if label(P) â  undefined then continue               /* Previously processed in inner loop */
        Neighbors NÂ := RangeQuery(DB, distFunc, P, eps)     /* Find neighbors */
        if |N| < minPts then {                              /* Density check */
            label(P)Â := Noise                               /* Label as Noise */
            continue
        }
        CÂ := C + 1                                          /* next cluster label */
        label(P)Â := C                                       /* Label initial point */
        SeedSet SÂ := N \ {P}                                /* Neighbors to expand */
        for each point Q in S {                             /* Process every seed point Q */
            if label(Q) = Noise then label(Q)Â := C          /* Change Noise to border point */
            if label(Q) â  undefined then continue           /* Previously processed (e.g., border point) */
            label(Q)Â := C                                   /* Label neighbor */
            Neighbors NÂ := RangeQuery(DB, distFunc, Q, eps) /* Find neighbors */
            if |N| â¥ minPts then {                          /* Density check (if Q is a core point) */
                SÂ := S âª N                                  /* Add new neighbors to seed set */
            }
        }
    }
}

where RangeQuery can be implemented using a database index for better performance, or using a slow linear scan:

RangeQuery(DB, distFunc, Q, eps) {
    Neighbors NÂ := empty list
    for each point P in database DB {                      /* Scan all points in the database */
        if distFunc(Q, P) â¤ eps then {                     /* Compute distance and check epsilon */
            NÂ := N âª {P}                                   /* Add to result */
        }
    }
    return N
}

Abstract algorithm[edit]
The DBSCAN algorithm can be abstracted into the following steps:[4]

Find the points in the Îµ (eps) neighborhood of every point, and identify the core points with more than minPts neighbors.
Find the connected components of core points on the neighbor graph, ignoring all non-core points.
Assign each non-core point to a nearby cluster if the cluster is an Îµ (eps) neighbor, otherwise assign it to noise.
A naive implementation of this requires storing the neighborhoods in step 1, thus requiring substantial memory. The original DBSCAN algorithm does not require this by performing these steps for one point at a time.

Complexity[edit]
DBSCAN visits each point of the database, possibly multiple times (e.g., as candidates to different clusters). For practical considerations, however, the time complexity is mostly governed by the number of regionQuery invocations. DBSCAN executes exactly one such query for each point, and if an indexing structure is used that executes a neighborhood query in O(log n), an overall average runtime complexity of O(n log n) is obtained (if parameter Îµ is chosen in a meaningful way, i.e. such that on average only O(log n) points are returned). Without the use of an accelerating index structure, or on degenerated data (e.g. all points within a distance less than Îµ), the worst case run time complexity remains O(nÂ²). The distance matrix of size (nÂ²-n)/2 can be materialized to avoid distance recomputations, but this needs O(nÂ²) memory, whereas a non-matrix based implementation of DBSCAN only needs O(n) memory.

  DBSCAN can find non-linearly separable clusters. This dataset cannot be adequately clustered with k-means or Gaussian Mixture EM clustering.
Advantages[edit]
DBSCAN does not require one to specify the number of clusters in the data a priori, as opposed to k-means.
DBSCAN can find arbitrarily-shaped clusters. It can even find a cluster completely surrounded by (but not connected to) a different cluster. Due to the MinPts parameter, the so-called single-link effect (different clusters being connected by a thin line of points) is reduced.
DBSCAN has a notion of noise, and is robust to outliers.
DBSCAN requires just two parameters and is mostly insensitive to the ordering of the points in the database. (However, points sitting on the edge of two different clusters might swap cluster membership if the ordering of the points is changed, and the cluster assignment is unique only up to isomorphism.)
DBSCAN is designed for use with databases that can accelerate region queries, e.g. using an R* tree.
The parameters minPts and Îµ can be set by a domain expert, if the data is well understood.
Disadvantages[edit]
DBSCAN is not entirely deterministic: border points that are reachable from more than one cluster can be part of either cluster, depending on the order the data are processed. For most data sets and domains, this situation does not arise often and has little impact on the clustering result:[4] both on core points and noise points, DBSCAN is deterministic. DBSCAN*[8] is a variation that treats border points as noise, and this way achieves a fully deterministic result as well as a more consistent statistical interpretation of density-connected components.
The quality of DBSCAN depends on the distance measure used in the function regionQuery(P,Îµ). The most common distance metric used is Euclidean distance. Especially for high-dimensional data, this metric can be rendered almost useless due to the so-called "Curse of dimensionality", making it difficult to find an appropriate value for Îµ. This effect, however, is also present in any other algorithm based on Euclidean distance.
DBSCAN cannot cluster data sets well with large differences in densities, since the minPts-Îµ combination cannot then be chosen appropriately for all clusters.[9]
If the data and scale are not well understood, choosing a meaningful distance threshold Îµ can be difficult.
See the section below on extensions for algorithmic modifications to handle these issues.

Parameter estimation[edit]
Every data mining task has the problem of parameters. Every parameter influences the algorithm in specific ways. For DBSCAN, the parameters Îµ and minPts are needed. The parameters must be specified by the user. Ideally, the value of Îµ is given by the problem to solve (e.g. a physical distance), and minPts is then the desired minimum cluster size.[a]

MinPts: As a rule of thumb, a minimum minPts can be derived from the number of dimensions D in the data set, as minPts â¥ D + 1. The low value of minPts = 1 does not make sense, as then every point is a core point by definition. With minPts â¤ 2, the result will be the same as of hierarchical clustering with the single link metric, with the dendrogram cut at height Îµ. Therefore, minPts must be chosen at least 3. However, larger values are usually better for data sets with noise and will yield more significant clusters. As a rule of thumb, minPts = 2Â·dim can be used,[7] but it may be necessary to choose larger values for very large data, for noisy data or for data that contains many duplicates.[4]
Îµ: The value for Îµ can then be chosen by using a k-distance graph, plotting the distance to the k = minPts-1 nearest neighbor ordered from the largest to the smallest value.[4] Good values of Îµ are where this plot shows an "elbow":[1][7][4] if Îµ is chosen much too small, a large part of the data will not be clustered; whereas for a too high value of Îµ, clusters will merge and the majority of objects will be in the same cluster. In general, small values of Îµ are preferable,[4] and as a rule of thumb only a small fraction of points should be within this distance of each other. Alternatively, an OPTICS plot can be used to choose Îµ,[4] but then the OPTICS algorithm itself can be used to cluster the data.
Distance function: The choice of distance function is tightly coupled to the choice of Îµ, and has a major impact on the results. In general, it will be necessary to first identify a reasonable measure of similarity for the data set, before the parameter Îµ can be chosen. There is no estimation for this parameter, but the distance functions needs to be chosen appropriately for the data set. For example, on geographic data, the great-circle distance is often a good choice.
OPTICS can be seen as a generalization of DBSCAN that replaces the Îµ parameter with a maximum value that mostly affects performance. MinPts then essentially becomes the minimum cluster size to find. While the algorithm is much easier to parameterize than DBSCAN, the results are a bit more difficult to use, as it will usually produce a hierarchical clustering instead of the simple data partitioning that DBSCAN produces.
Recently, one of the original authors of DBSCAN has revisited DBSCAN and OPTICS, and published a refined version of hierarchical DBSCAN (HDBSCAN*),[8] which no longer has the notion of border points. Instead, only the core points form the cluster.

Relationship to spectral clustering[edit]
DBSCAN can be seen as special (efficient) variant of spectral clustering: Connected components correspond to optimal spectral clusters (no edges cut â spectral clustering tries to partition the data with a minimum cut); DBSCAN finds connected components on the (asymmetric) reachability graph.[10] However, spectral clustering can be computationally intensive (up to 
  
    
      
        O
        (
        
          n
          
            3
          
        
        )
      
    
    {\displaystyle O(n^{3})}
  
 without approximation and further assumptions), and one has to choose the number of clusters 
  
    
      
        k
      
    
    {\displaystyle k}
  
 for both the number of eigenvectors to choose and the number of clusters to produce with k-means on the spectral embedding. Thus, for performance reasons, the original DBSCAN algorithm remains preferable to a spectral implementation, and this relationship is so far only of theoretical interest.

Extensions[edit]
Generalized DBSCAN (GDBSCAN)[7][11] is a generalization by the same authors to arbitrary "neighborhood" and "dense" predicates. The Îµ and minPts parameters are removed from the original algorithm and moved to the predicates. For example, on polygon data, the "neighborhood" could be any intersecting polygon, whereas the density predicate uses the polygon areas instead of just the object count.
Various extensions to the DBSCAN algorithm have been proposed, including methods for parallelization, parameter estimation, and support for uncertain data. The basic idea has been extended to hierarchical clustering by the OPTICS algorithm. DBSCAN is also used as part of subspace clustering algorithms like PreDeCon and SUBCLU. HDBSCAN[8] is a hierarchical version of DBSCAN which is also faster than OPTICS, from which a flat partition consisting of the most prominent clusters can be extracted from the hierarchy.[12]

Availability[edit]
Different implementations of the same algorithm were found to exhibit enormous performance differences, with the fastest on a test data set finishing in 1.4 seconds, the slowest taking 13803 seconds.[13] The differences can be attributed to implementation quality, language and compiler differences, and the use of indexes for acceleration.

Apache Commons Math contains a Java implementation of the algorithm running in quadratic time.
ELKI offers an implementation of DBSCAN as well as GDBSCAN and other variants. This implementation can use various index structures for sub-quadratic runtime and supports arbitrary distance functions and arbitrary data types, but it may be outperformed by low-level optimized (and specialized) implementations on small data sets.
mlpack includes an implementation of DBSCAN accelerated with dual-tree range search techniques.
PostGIS includes ST_ClusterDBSCAN â a 2D implementation of DBSCAN that uses R-tree index. Any geometry type is supported, e.g. Point, LineString, Polygon, etc.
R contains implementations of DBSCAN in the packages dbscan and fpc. Both packages support arbitrary distance functions via distance matrices. The package fpc does not have index support (and thus has quadratic runtime and memory complexity) and is rather slow due to the R interpreter. The package dbscan provides a fast C++ implementation using k-d trees (for Euclidean distance only) and also includes implementations of DBSCAN*, HDBSCAN*, OPTICS, OPTICSXi, and other related methods.
scikit-learn includes a Python implementation of DBSCAN for arbitrary Minkowski metrics, which can be accelerated using k-d trees and ball trees but which uses worst-case quadratic memory. A contribution to scikit-learn provides an implementation of the HDBSCAN* algorithm.
pyclustering library includes a Python and C++ implementation of DBSCAN for Euclidean distance only as well as OPTICS algorithm.
SPMF includes an implementation of the DBSCAN algorithm with k-d tree support for Euclidean distance only.
Weka contains (as an optional package in latest versions) a basic implementation of DBSCAN that runs in quadratic time and linear memory.
Notes[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ Jump up to: a b While minPts intuitively is the minimum cluster size, in some cases DBSCAN can produce smaller clusters.[4] A DBSCAN cluster consists of at least one core point.[4] As other points may be border points to more than one cluster, there is no guarantee that at least minPts points are included in every cluster.


References[edit]


^ Jump up to: a b c .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Ester, Martin; Kriegel, Hans-Peter; Sander, JÃ¶rg; Xu, Xiaowei (1996).  Simoudis, Evangelos; Han, Jiawei; Fayyad, Usama M. (eds.). A density-based algorithm for discovering clusters in large spatial databases with noise. Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96). AAAI Press. pp.Â 226â231. CiteSeerXÂ 10.1.1.121.9220. ISBNÂ 1-57735-004-9.

^ "Archived copy". Archived from the original on April 21, 2010. Retrieved 2010-04-18.{{cite web}}:  CS1 maint: archived copy as title (link) Most cited data mining articles according to Microsoft academic search; DBSCAN is on rank 24.

^ "2014 SIGKDD Test of Time Award". ACM SIGKDD. 2014-08-18. Retrieved 2016-07-27.

^ Jump up to: a b c d e f g h i j k l Schubert, Erich; Sander, JÃ¶rg; Ester, Martin; Kriegel, Hans Peter; Xu, Xiaowei (July 2017). "DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN". ACM Trans. Database Syst. 42 (3): 19:1â19:21. doi:10.1145/3068335. ISSNÂ 0362-5915. S2CIDÂ 5156876.

^ "TODS Home". tods.acm.org. Association for Computing Machinery. Retrieved 2020-07-16.

^ Jump up to: a b Ling, R. F. (1972-01-01). "On the theory and construction of k-clusters". The Computer Journal. 15 (4): 326â332. doi:10.1093/comjnl/15.4.326. ISSNÂ 0010-4620.

^ Jump up to: a b c d Sander, JÃ¶rg; Ester, Martin; Kriegel, Hans-Peter; Xu, Xiaowei (1998). "Density-Based Clustering in Spatial Databases: The Algorithm GDBSCAN and Its Applications". Data Mining and Knowledge Discovery. Berlin: Springer-Verlag. 2 (2): 169â194. doi:10.1023/A:1009745219419. S2CIDÂ 445002.

^ Jump up to: a b c Campello, Ricardo J. G. B.; Moulavi, Davoud; Zimek, Arthur; Sander, JÃ¶rg (2015). "Hierarchical Density Estimates for Data Clustering, Visualization, and Outlier Detection". ACM Transactions on Knowledge Discovery from Data. 10 (1): 1â51. doi:10.1145/2733381. ISSNÂ 1556-4681. S2CIDÂ 2887636.

^ Kriegel, Hans-Peter; KrÃ¶ger, Peer; Sander, JÃ¶rg; Zimek, Arthur (2011). "Density-based Clustering". WIREs Data Mining and Knowledge Discovery. 1 (3): 231â240. doi:10.1002/widm.30.

^ Schubert, Erich; Hess, Sibylle; Morik, Katharina (2018). The Relationship of DBSCAN to Matrix Factorization and Spectral Clustering (PDF). Lernen, Wissen, Daten, Analysen (LWDA). pp.Â 330â334 â via CEUR-WS.org.

^ Sander, JÃ¶rg (1998). Generalized Density-Based Clustering for Spatial Data Mining. MÃ¼nchen: Herbert Utz Verlag. ISBNÂ 3-89675-469-6.

^ Campello, R. J. G. B.; Moulavi, D.; Zimek, A.; Sander, J. (2013). "A framework for semi-supervised and unsupervised optimal extraction of clusters from hierarchies". Data Mining and Knowledge Discovery. 27 (3): 344. doi:10.1007/s10618-013-0311-4. S2CIDÂ 8144686.

^ Kriegel, Hans-Peter; Schubert, Erich; Zimek, Arthur (2016). "The (black) art of runtime evaluation: Are we comparing algorithms or implementations?". Knowledge and Information Systems. 52 (2): 341. doi:10.1007/s10115-016-1004-2. ISSNÂ 0219-1377. S2CIDÂ 40772241.






<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=DBSCAN&oldid=1043021240"
		Categories: Cluster analysis algorithmsHidden categories: CS1 maint: archived copy as titleArticles with short descriptionShort description is different from WikidataArticles containing potentially dated statements from July 2020All articles containing potentially dated statements
	
