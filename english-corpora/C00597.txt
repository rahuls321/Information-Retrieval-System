
Title:
Concurrent computing
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Form of computing in which several computations are executing during overlapping time periods
.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}For a more theoretical discussion, see Concurrency (computer science).
This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources:Â "Concurrent computing"Â âÂ newsÂ Â· newspapersÂ Â· booksÂ Â· scholarÂ Â· JSTOR  (February 2014) (Learn how and when to remove this template message)
.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}Programming paradigms
Action
Agent-oriented
Array-oriented
Automata-based
Concurrent computing
Choreographic programming
Relativistic programming
Data-driven
Declarative (contrast: Imperative)
Functional
Functional logic
Purely functional
Logic
Abductive logic
Answer set
Concurrent logic
Functional logic
Inductive logic
Constraint
Constraint logic
Concurrent constraint logic
Dataflow
Flow-based
Reactive
Functional reactive
Ontology
Query language
Differentiable
Dynamic/scripting
Event-driven
Function-level (contrast: Value-level)
Point-free style
Concatenative
Generic
Imperative (contrast: Declarative)
Procedural
Object-oriented
Polymorphic
Intentional
Language-oriented
Domain-specific
Literate
Natural-language programming
Metaprogramming
Automatic
Inductive programming
Reflective
Attribute-oriented
Macro
Template
Non-structured (contrast: Structured)
Array
Nondeterministic
Parallel computing
Process-oriented
Probabilistic
Quantum
Set-theoretic
Stack-based
Structured (contrast: Non-structured)
Block-structured
Structured concurrency
Object-oriented
Actor-based
Class-based
Concurrent
Prototype-based
By separation of concerns:
Aspect-oriented
Role-oriented
Subject-oriented
Recursive
Symbolic
Value-level (contrast: Function-level)
.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vte
Concurrent computing is a form of computing in which several computations are executed concurrentlyâduring overlapping time periodsâinstead of sequentiallyâwith one completing before the next starts.
This is a property of a systemâwhether a program, computer, or a networkâwhere there is a separate execution point or "thread of control" for each process. A concurrent system is one where a computation can advance without waiting for all other computations to complete.[1]
Concurrent computing is a form of modular programming.  In its paradigm an overall computation is factored into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include Edsger Dijkstra, Per Brinch Hansen, and C.A.R. Hoare.

Contents

1 Introduction

1.1 Coordinating access to shared resources
1.2 Advantages


2 Models

2.1 Consistency models


3 Implementation

3.1 Interaction and communication


4 History
5 Prevalence
6 Languages supporting concurrent programming
7 See also
8 Notes
9 References
10 Sources
11 Further reading
12 External links



Introduction[edit]
See also: Parallel computing
This section has multiple issues. Please help improve it or discuss these issues on the talk page. (Learn how and when to remove these template messages)

      This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.  (December 2016) (Learn how and when to remove this template message)
This section possibly contains original research. Please improve it by verifying the claims made and adding inline citations. Statements consisting only of original research should be removed.  (December 2016) (Learn how and when to remove this template message)
    
 (Learn how and when to remove this template message)
The concept of concurrent computing is frequently confused with the related but distinct concept of parallel computing,[2][3] although both can be described as "multiple processes executing during the same period of time". In parallel computing, execution occurs at the same physical instant: for example, on separate processors of a multi-processor machine, with the goal of speeding up computationsâparallel computing is impossible on a (one-core) single processor, as only one computation can occur at any instant (during any single clock cycle).[a] By contrast, concurrent computing consists of process lifetimes overlapping, but execution need not happen at the same instant. The goal here is to model processes in the outside world that happen concurrently, such as multiple clients accessing a server at the same time. Structuring software systems as composed of multiple concurrent, communicating parts can be useful for tackling complexity, regardless of whether the parts can be executed in parallel.[4]:â1â
For example, concurrent processes can be executed on one core by interleaving the execution steps of each process via time-sharing slices: only one process runs at a time, and if it does not complete during its time slice, it is paused, another process begins or resumes, and then later the original process is resumed. In this way, multiple processes are part-way through execution at a single instant, but only one process is being executed at that instant.[citation needed]
Concurrent computations may be executed in parallel,[2][5] for example, by assigning each process to a separate processor or processor core, or distributing a computation across a network. In general, however, the languages, tools, and techniques for parallel programming might not be suitable for concurrent programming, and vice versa.[citation needed]
The exact timing of when tasks in a concurrent system are executed depend on the scheduling, and tasks need not always be executed concurrently. For example, given two tasks, T1 and T2:[citation needed]

T1 may be executed and finished before T2 or vice versa (serial and sequential)
T1 and T2 may be executed alternately (serial and concurrent)
T1 and T2 may be executed simultaneously at the same instant of time (parallel and concurrent)
The word "sequential" is used as an antonym for both "concurrent" and "parallel"; when these are explicitly distinguished, concurrent/sequential and parallel/serial are used as opposing pairs.[6] A schedule in which tasks execute one at a time (serially, no parallelism), without interleaving (sequentially, no concurrency: no task begins until the prior task ends) is called a serial schedule. A set of tasks that can be scheduled serially is serializable, which simplifies concurrency control.[citation needed]

Coordinating access to shared resources[edit]
The main challenge in designing concurrent programs is concurrency control: ensuring the correct sequencing of the interactions or communications between different computational executions, and coordinating access to resources that are shared among executions.[5] Potential problems include race conditions, deadlocks, and resource starvation. For example, consider the following algorithm to make withdrawals from a checking account represented by the shared resource balance:

bool withdraw(int withdrawal)
{
    if (balance >= withdrawal)
    {
        balance -= withdrawal;
        return true;
    } 
    return false;
}

Suppose balance = 500, and two concurrent threads make the calls withdraw(300) and withdraw(350). If line 3 in both operations executes before line 5 both operations will find that balance >= withdrawal evaluates to true, and execution will proceed to subtracting the withdrawal amount. However, since both processes perform their withdrawals, the total amount withdrawn will end up being more than the original balance. These sorts of problems with shared resources benefit from the use of concurrency control, or non-blocking algorithms.

Advantages[edit]
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed.  (December 2006) (Learn how and when to remove this template message)
The advantages of concurrent computing include:

Increased program throughputâparallel execution of a concurrent program allows the number of tasks completed in a given time to increase proportionally to the number of processors according to Gustafson's law
High responsiveness for input/outputâinput/output-intensive programs mostly wait for input or output operations to complete. Concurrent programming allows the time that would be spent waiting to be used for another task.[citation needed]
More appropriate program structureâsome problems and problem domains are well-suited to representation as concurrent tasks or processes.[citation needed]
Models[edit]
Introduced in 1962, Petri nets were an early attempt to codify the rules of concurrent execution. Dataflow theory later built upon these, and Dataflow architectures were created to physically implement the ideas of dataflow theory. Beginning in the late 1970s, process calculi such as Calculus of Communicating Systems (CCS) and Communicating Sequential Processes (CSP) were developed to permit algebraic reasoning about systems composed of interacting components. The Ï-calculus added the capability for reasoning about dynamic topologies.
Input/output automata were introduced in 1987.
Logics such as Lamport's TLA+, and mathematical models such as traces and Actor event diagrams, have also been developed to describe the behavior of concurrent systems.
Software transactional memory borrows from database theory the concept of atomic transactions and applies them to memory accesses.

Consistency models[edit]
Main article: Consistency model
Concurrent programming languages and multiprocessor programs must have a consistency model (also known as a memory model). The consistency model defines rules for how operations on computer memory occur and how results are produced.
One of the first consistency models was Leslie Lamport's sequential consistency model. Sequential consistency is the property of a program that its execution produces the same results as a sequential program. Specifically, a program is sequentially consistent if "the results of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program".[7]

See also: Relaxed sequential
Implementation[edit]
This section needs expansion. You can help by adding to it.  (February 2014)
A number of different methods can be used to implement concurrent programs, such as implementing each computational execution as an operating system process, or implementing the computational processes as a set of threads within a single operating system process.

Interaction and communication[edit]
In some concurrent computing systems, communication between the concurrent components is hidden from the programmer (e.g., by using futures), while in others it must be handled explicitly. Explicit communication can be divided into two classes:

Shared memory communication
Concurrent components communicate by altering the contents of shared memory locations (exemplified by Java and C#). This style of concurrent programming usually needs the use of some form of locking (e.g., mutexes, semaphores, or monitors) to coordinate between threads. A program that properly implements any of these is said to be thread-safe.
Message passing communication
Concurrent components communicate by exchanging messages (exemplified by MPI, Go, Scala, Erlang and occam). The exchange of messages may be carried out asynchronously, or may use a synchronous "rendezvous" style in which the sender blocks until the message is received. Asynchronous message passing may be reliable or unreliable (sometimes referred to as "send and pray"). Message-passing concurrency tends to be far easier to reason about than shared-memory concurrency, and is typically considered a more robust form of concurrent programming.[citation needed] A wide variety of mathematical theories to understand and analyze message-passing systems are available, including the actor model, and various process calculi. Message passing can be efficiently implemented via symmetric multiprocessing, with or without shared memory cache coherence.
Shared memory and message passing concurrency have different performance characteristics. Typically (although not always), the per-process memory overhead and task switching overhead is lower in a message passing system, but the overhead of message passing is greater than for a procedure call. These differences are often overwhelmed by other performance factors.

History[edit]
Concurrent computing developed out of earlier work on railroads and telegraphy, from the 19th and early 20th century, and some terms date to this period, such as semaphores. These arose to address the question of how to handle multiple trains on the same railroad system (avoiding collisions and maximizing efficiency) and how to handle multiple transmissions over a given set of wires (improving efficiency), such as via time-division multiplexing (1870s).
The academic study of concurrent algorithms started in the 1960s, with Dijkstra (1965) credited with being the first paper in this field, identifying and solving mutual exclusion.[8]

Prevalence[edit]
Concurrency is pervasive in computing, occurring from low-level hardware on a single chip to worldwide networks. Examples follow.
At the programming language level:

Channel
Coroutine
Futures and promises
At the operating system level:

Computer multitasking, including both cooperative multitasking and preemptive multitasking
Time-sharing, which replaced sequential batch processing of jobs with concurrent use of a system
Process
Thread
At the network level, networked systems are generally concurrent by their nature, as they consist of separate devices.

Languages supporting concurrent programming[edit]
Concurrent programming languages are programming languages that use language constructs for concurrency. These constructs may involve multi-threading, support for distributed computing, message passing, shared resources (including shared memory) or futures and promises. Such languages are sometimes described as concurrency-oriented languages or concurrency-oriented programming languages (COPL).[9]
Today, the most commonly used programming languages that have specific constructs for concurrency are Java and C#. Both of these languages fundamentally use a shared-memory concurrency model, with locking provided by monitors (although message-passing models can and have been implemented on top of the underlying shared-memory model). Of the languages that use a message-passing concurrency model, Erlang is probably the most widely used in industry at present.[citation needed]
Many concurrent programming languages have been developed more as research languages (e.g. Pict) rather than as languages for production use. However, languages such as Erlang, Limbo, and occam have seen industrial use at various times in the last 20 years. A non-exhaustive list of languages which use or provide concurrent programming facilities:

Adaâgeneral purpose, with native support for message passing and monitor based concurrency
Alefâconcurrent, with threads and message passing, for system programming in early versions of Plan 9 from Bell Labs
Aliceâextension to Standard ML, adds support for concurrency via futures
Ateji PXâextension to Java with parallel primitives inspired from Ï-calculus
Axumâdomain specific, concurrent, based on actor model and .NET Common Language Runtime using a C-like syntax
BMDFMâBinary Modular DataFlow Machine
C++âstd::thread
CÏ (C omega)âfor research, extends C#, uses asynchronous communication
C#âsupports concurrent computing using lock, yield, also since version 5.0 async and await keywords introduced
Clojureâmodern, functional dialect of Lisp on the Java platform
Concurrent Cleanâfunctional programming, similar to Haskell
Concurrent Collections (CnC)âAchieves implicit parallelism independent of memory model by explicitly defining flow of data and control
Concurrent Haskellâlazy, pure functional language operating concurrent processes on shared memory
Concurrent MLâconcurrent extension of Standard ML
Concurrent Pascalâby Per Brinch Hansen
Curry
Dâmulti-paradigm system programming language with explicit support for concurrent programming (actor model)
Eâuses promises to preclude deadlocks
ECMAScriptâuses promises for asynchronous operations
Eiffelâthrough its SCOOP mechanism based on the concepts of Design by Contract
Elixirâdynamic and functional meta-programming aware language running on the Erlang VM.
Erlangâuses asynchronous message passing with nothing shared
FAUSTâreal-time functional, for signal processing, compiler provides automatic parallelization via OpenMP or a specific work-stealing scheduler
Fortranâcoarrays and do concurrent are part of Fortran 2008 standard
Goâfor system programming, with a concurrent programming model based on CSP
Haskellâconcurrent, and parallel functional programming language[10]
Humeâfunctional, concurrent, for bounded space and time environments where automata processes are described by synchronous channels patterns and message passing
Ioâactor-based concurrency
Janusâfeatures distinct askers and tellers to logical variables, bag channels; is purely declarative
Javaâthread class or Runnable interface
Juliaâ"concurrent programming primitives: Tasks, async-wait, Channels."[11]
JavaScriptâvia web workers, in a browser environment, promises, and callbacks.
JoCamlâconcurrent and distributed channel based, extension of OCaml, implements the join-calculus of processes
Join Javaâconcurrent, based on Java language
Jouleâdataflow-based, communicates by message passing
Joyceâconcurrent, teaching, built on Concurrent Pascal with features from CSP by Per Brinch Hansen
LabVIEWâgraphical, dataflow, functions are nodes in a graph, data is wires between the nodes; includes object-oriented language
Limboârelative of Alef, for system programming in Inferno (operating system)
MultiLispâScheme variant extended to support parallelism
Modula-2âfor system programming, by N. Wirth as a successor to Pascal with native support for coroutines
Modula-3âmodern member of Algol family with extensive support for threads, mutexes, condition variables
Newsqueakâfor research, with channels as first-class values; predecessor of Alef
occamâinfluenced heavily by communicating sequential processes (CSP)
occam-Ïâa modern variant of occam, which incorporates ideas from Milner's Ï-calculus
Orcâheavily concurrent, nondeterministic, based on Kleene algebra
Oz-Mozartâmultiparadigm, supports shared-state and message-passing concurrency, and futures
ParaSailâobject-oriented, parallel, free of pointers, race conditions
Pictâessentially an executable implementation of Milner's Ï-calculus
Raku includes classes for threads, promises and channels by default[12]
Python â uses thread-based parallelism and process-based parallelism [13]
Reiaâuses asynchronous message passing between shared-nothing objects
Red/Systemâfor system programming, based on Rebol
Rustâfor system programming, using message-passing with move semantics, shared immutable memory, and shared mutable memory.[14]
Scalaâgeneral purpose, designed to express common programming patterns in a concise, elegant, and type-safe way
SequenceLâgeneral purpose functional, main design objectives are ease of programming, code clarity-readability, and automatic parallelization for performance on multicore hardware, and provably free of race conditions
SRâfor research
SuperPascalâconcurrent, for teaching, built on Concurrent Pascal and Joyce by Per Brinch Hansen
Uniconâfor research
TNSDLâfor developing telecommunication exchanges, uses asynchronous message passing
VHSIC Hardware Description Language (VHDL)âIEEE STD-1076
XCâconcurrency-extended subset of C language developed by XMOS, based on communicating sequential processes, built-in constructs for programmable I/O
Many other languages provide support for concurrency in the form of libraries, at levels roughly comparable with the above list.

See also[edit]
Asynchronous I/O
Chu space
Flow-based programming
Java ConcurrentMap
List of important publications in concurrent, parallel, and distributed computing
Ptolemy Project
Race condition Â§Â Computing
Sheaf (mathematics)
Transaction processing
Notes[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ This is discounting parallelism internal to a processor core, such as pipelining or vectorized instructions. A one-core, one-processor machine may be capable of some parallelism, such as with a coprocessor, but the processor alone is not.


References[edit]


^ Operating System Concepts 9th edition, Abraham Silberschatz. "Chapter 4: Threads"

^ Jump up to: a b Pike, Rob (2012-01-11). "Concurrency is not Parallelism". Waza conference, 11 January 2012. Retrieved from http://talks.golang.org/2012/waza.slide (slides) and http://vimeo.com/49718712 (video).

^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}"Parallelism vs. Concurrency". Haskell Wiki.

^ Schneider, Fred B. (1997-05-06). On Concurrent Programming. Springer. ISBNÂ 9780387949420.

^ Jump up to: a b Ben-Ari, Mordechai (2006). Principles of Concurrent and Distributed Programming (2ndÂ ed.). Addison-Wesley. ISBNÂ 978-0-321-31283-9.

^ Patterson & Hennessy 2013, p.Â 503.

^ Lamport, Leslie (1 September 1979). "How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs". IEEE Transactions on Computers. C-28 (9): 690â691. doi:10.1109/TC.1979.1675439. S2CIDÂ 5679366.

^ "PODC Influential Paper Award: 2002", ACM Symposium on Principles of Distributed Computing, retrieved 2009-08-24

^ Armstrong, Joe (2003). "Making reliable distributed systems in the presence of software errors" (PDF).

^  Marlow, Simon (2013) Parallel and Concurrent Programming in HaskellÂ : Techniques for Multicore and Multithreaded Programming ISBNÂ 9781449335946

^ https://juliacon.talkfunnel.com/2015/21-concurrent-and-parallel-programming-in-julia Concurrent and Parallel programming in Julia

^ "Concurrency". docs.perl6.org. Retrieved 2017-12-24.

^ DocumentationÂ Â» The Python Standard LibraryÂ Â» Concurrent Execution

^ Blum, Ben (2012). "Typesafe Shared Mutable State". Retrieved 2012-11-14.


Sources[edit]
.mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}.mw-parser-output .refbegin-hanging-indents>ul{margin-left:0}.mw-parser-output .refbegin-hanging-indents>ul>li{margin-left:0;padding-left:3.2em;text-indent:-3.2em}.mw-parser-output .refbegin-hanging-indents ul,.mw-parser-output .refbegin-hanging-indents ul li{list-style:none}@media(max-width:720px){.mw-parser-output .refbegin-hanging-indents>ul>li{padding-left:1.6em;text-indent:-1.6em}}.mw-parser-output .refbegin-columns{margin-top:0.3em}.mw-parser-output .refbegin-columns ul{margin-top:0}.mw-parser-output .refbegin-columns li{page-break-inside:avoid;break-inside:avoid-column}
Patterson, David A.; Hennessy, John L. (2013). Computer Organization and Design: The Hardware/Software Interface. The Morgan Kaufmann Series in Computer Architecture and Design (5Â ed.). Morgan Kaufmann. ISBNÂ 978-0-12407886-4.

Further reading[edit]
Dijkstra, E. W. (1965). "Solution of a problem in concurrent programming control". Communications of the ACM. 8 (9): 569. doi:10.1145/365559.365617. S2CIDÂ 19357737.
Herlihy, Maurice (2008) [2008]. The Art of Multiprocessor Programming. Morgan Kaufmann. ISBNÂ 978-0123705914.
Downey, Allen B. (2005) [2005]. The Little Book of Semaphores (PDF). Green Tea Press. ISBNÂ 978-1-4414-1868-5. Archived from the original (PDF) on 2016-03-04. Retrieved 2009-11-21.
Filman, Robert E.; Daniel P. Friedman (1984). Coordinated Computing: Tools and Techniques for Distributed Software. New York: McGraw-Hill. p.Â 370. ISBNÂ 978-0-07-022439-1.
LeppÃ¤jÃ¤rvi, Jouni (2008). A pragmatic, historically oriented survey on the universality of synchronization primitives (PDF). University of Oulu.
Taubenfeld, Gadi (2006). Synchronization Algorithms and Concurrent Programming. Pearson / Prentice Hall. p.Â 433. ISBNÂ 978-0-13-197259-9.
External links[edit]
 Media related to Concurrent programming at Wikimedia Commons
Concurrent Systems Virtual Library
.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}vteEdsger DijkstraNotableworks
A Primer of ALGOL 60 Programming (book)
Structured Programming (book)
A Discipline of Programming (book)
A Method of Programming (book)
Predicate Calculus and Program Semantics (book)
Selected Writings on Computing: A Personal Perspective (book)
A Note on Two Problems in Connexion with Graphs
Cooperating Sequential Processes
Solution of a Problem in Concurrent Programming Control
The Structure of the 'THE'-Multiprogramming System
Go To Statement Considered Harmful
Notes on Structured Programming
The Humble Programmer
Programming Considered as a Human Activity
How Do We Tell Truths That Might Hurt?
On the Role of Scientific Thought
Self-stabilizing Systems in Spite of Distributed Control
On the Cruelty of Really Teaching Computer Science
Selected papers
EWD manuscripts
Main researchareas
Theoretical computing science
Software engineering
Systems science
Algorithm design
Concurrent computing
Distributed computing
Formal methods
Programming methodology
Programming language research
Program design and development
Software architecture
Philosophy of computer programming and computing science
ScientificcontributionsConcepts,methods,principlesand theories
DijkstraâZonneveld ALGOL 60 compiler (first complete working ALGOL 60 compiler)
Call stack
Concurrency
Concurrent programming
Cooperating sequential processes
Critical section
Deadly embrace (deadlock)
Dining philosophers problem
Dutch national flag problem
Fault-tolerant system
Goto-less programming
Guarded Command Language
Layered structure in software architecture
Levels of abstraction
Multithreaded programming
Mutual exclusion (mutex)
Producerâconsumer problem (bounded buffer problem)
Program families
Predicate transformer semantics
Process synchronization
Self-stabilizing distributed system
Semaphore (programming)
Separation of concerns
Sleeping barber problem
Software crisis
Structured analysis
Structured programming
THE multiprogramming system
Unbounded nondeterminism
Weakest precondition calculus
Algorithms
Banker's algorithm
Dijkstra's algorithm
DJP algorithm (Prim's algorithm)
Dijkstra-Scholten algorithm
Dekker's algorithm (generalization)
Smoothsort
Shunting-yard algorithm
Tri-color marking algorithm
Concurrent algorithms
Distributed algorithms
Deadlock prevention algorithms
Mutual exclusion algorithms
Self-stabilizing algorithms
Relatedpeople
Shlomi Dolev
Per Brinch Hansen
Tony Hoare
Ole-Johan Dahl
Leslie Lamport
David Parnas
Jaap Zonneveld
Carel S. Scholten 
Adriaan van Wijngaarden
Niklaus Wirth
Othertopics
Centrum Wiskunde & Informatica (CWI Amsterdam)
Electrologica X1
Electrologica X8
Dijkstra Prize (Edsger W. Dijkstra Prize in Distributed Computing)
Dijkstra Fellowship
International Symposium on Stabilization, Safety, and Security of Distributed Systems
E.W. Dijkstra Archive (University of Texas at Austin)
List of important publications in computer science
List of important publications in theoretical computer science
List of important publications in concurrent, parallel, and distributed computing
List of pioneers in computer science
List of people considered father or mother of a technical field

 Wikiquote

vteConcurrent computingGeneral
Concurrency
Concurrency control
Process calculi
CSP
CCS
ACP
LOTOS
Ï-calculus
Ambient calculus
API-Calculus
PEPA
Join-calculus
Classic problems
ABA problem
Cigarette smokers problem
Deadlock
Dining philosophers problem
Producerâconsumer problem
Race condition
Readersâwriters problem
Sleeping barber problem

Â Category: Concurrent computing

vteTypes of programming languagesParadigm
Actor-based
Array
Aspect-oriented
Class-based
Concatenative
Concurrent
Dataflow
Declarative
Differentiable
Domain-specific
Dynamic
Esoteric
Event-driven
Extensible
Functional
Imperative
Logic
Macro
Metaprogramming
Object-based
Object-oriented
Pipeline
Procedural
Prototype-based
Reflective
Rule-based
Scripting
Stack-oriented
Synchronous
Tactile
Template
Level
Assembly
Compiled
Interpreted
Machine
Low-level
High-level
Very high-level
Generation
First
Second
Third
Fourth
Fifth
Related
Non-English-based
Visual





<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=Concurrent_computing&oldid=1040057524"
		Categories: Concurrent computingOperating system technologyEdsger W. DijkstraDutch inventionsHidden categories: Articles with short descriptionShort description matches WikidataArticles needing additional references from February 2014All articles needing additional referencesArticles needing additional references from December 2016Articles that may contain original research from December 2016All articles that may contain original researchArticles with multiple maintenance issuesAll articles with unsourced statementsArticles with unsourced statements from December 2016Articles needing additional references from December 2006Articles to be expanded from February 2014All articles to be expandedArticles using small message boxesArticles with unsourced statements from May 2013Articles with unsourced statements from August 2010Commons category link is on Wikidata
	
