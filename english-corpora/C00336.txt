
Title:
Ensemble learning
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Statistics and machine learning technique
.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}Part of a series onMachine learningand data mining
Problems
Classification
Clustering
Regression
Anomaly detection
Data Cleaning
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction

Supervised learning.mw-parser-output .nobold{font-weight:normal}(classificationÂ â¢ regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)

Clustering
BIRCH
CURE
Hierarchical
k-means
Expectationâmaximization (EM)
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t-SNE

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
k-NN
Local outlier factor

Artificial neural network
Autoencoder
Cognitive computing
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
ESN
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net
Transformer
Vision
Spiking neural network
Memtransistor
Electrochemical RAM (ECRAM)

Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)

Theory
Kernel machines
Biasâvariance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory

Machine-learning venues
NeurIPS
ICML
ML
JMLR
ArXiv:cs.LG

Related articles
Glossary of artificial intelligence
List of datasets for machine-learning research
Outline of machine learning
.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vte
In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.[1][2][3]
Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.

Contents

1 Overview
2 Ensemble theory
3 Ensemble size
4 Common types of ensembles

4.1 Bayes optimal classifier
4.2 Bootstrap aggregating (bagging)
4.3 Boosting
4.4 Bayesian model averaging
4.5 Bayesian model combination
4.6 Bucket of models
4.7 Stacking


5 Implementations in statistics packages
6 Ensemble learning applications

6.1 Remote sensing

6.1.1 Land cover mapping
6.1.2 Change detection


6.2 Computer security

6.2.1 Distributed denial of service
6.2.2 Malware Detection
6.2.3 Intrusion detection


6.3 Face recognition
6.4 Emotion recognition
6.5 Fraud detection
6.6 Financial decision-making
6.7 Medicine


7 See also
8 References
9 Further reading
10 External links



Overview[edit]
Supervised learning algorithms perform the task of searching through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem.[4] Even if the hypothesis space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form a (hopefully) better hypothesis. The term ensemble is usually reserved for methods that generate multiple hypotheses using the same base learner.[according to whom?]
The broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner.[citation needed]
Evaluating the prediction of an ensemble typically requires more computation than evaluating the prediction of a single model. In one sense, ensemble learning may be thought of as a way to compensate for poor learning algorithms by performing a lot of extra computation. On the other hand, the alternative is to do a lot more learning on one non-ensemble system. An ensemble system may be more efficient at improving overall accuracy for the same increase in compute, storage, or communication resources by using that increase on two or more methods, than would have been improved by increasing resource use for a single method.  Fast algorithms such as decision trees are commonly used in ensemble methods (for example, random forests), although slower algorithms can benefit from ensemble techniques as well.
By analogy, ensemble techniques have been used also in unsupervised learning scenarios, for example in consensus clustering or in anomaly detection.

Ensemble theory[edit]
Empirically, ensembles tend to yield better results when there is a significant diversity among the models.[5][6] Many ensemble methods, therefore, seek to promote diversity among the models they combine.[7][8] Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees).[9] Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity.[10] It is possible to increase diversity in the training stage of the model using correlation for regression tasks [11] or using information measures such as cross entropy for classification tasks.[12]

Ensemble size[edit]
While the number of component classifiers of an ensemble has a great impact on the accuracy of prediction, there is a limited number of studies addressing this problem. A priori determining of ensemble size and the volume and velocity of big data streams make this even more crucial for online ensemble classifiers. Mostly statistical tests were used for determining the proper number of components. More recently, a theoretical framework suggested that there is an ideal number of component classifiers for an ensemble such that having more or less than this number of classifiers would deteriorate the accuracy. It is called "the law of diminishing returns in ensemble construction." Their theoretical framework shows that using the same number of independent component classifiers as class labels gives the highest accuracy.[13][14]

Common types of ensembles[edit]
Bayes optimal classifier[edit]
The Bayes optimal classifier is a classification technique. It is an ensemble of all the hypotheses in the hypothesis space. On average, no other ensemble can outperform it.[15] The naive Bayes optimal classifier is a version of this that assumes that the data is conditionally independent on the class and makes the computation more feasible. Each hypothesis is given a vote proportional to the likelihood that the training dataset would be sampled from a system if that hypothesis were true. To facilitate training data of finite size, the vote of each hypothesis is also multiplied by the prior probability of that hypothesis. The Bayes optimal classifier can be expressed with the following equation:


  
    
      
        y
        =
        
          
            
              a
              r
              g
              m
              a
              x
            
            
              
                c
                
                  j
                
              
              â
              C
            
          
        
        
          â
          
            
              h
              
                i
              
            
            â
            H
          
        
        
          P
          (
          
            c
            
              j
            
          
          
            |
          
          
            h
            
              i
            
          
          )
          P
          (
          T
          
            |
          
          
            h
            
              i
            
          
          )
          P
          (
          
            h
            
              i
            
          
          )
        
      
    
    {\displaystyle y={\underset {c_{j}\in C}{\mathrm {argmax} }}\sum _{h_{i}\in H}{P(c_{j}|h_{i})P(T|h_{i})P(h_{i})}}
  

where 
  
    
      
        y
      
    
    {\displaystyle y}
  
 is the predicted class, 
  
    
      
        C
      
    
    {\displaystyle C}
  
 is the set of all possible classes, 
  
    
      
        H
      
    
    {\displaystyle H}
  
 is the hypothesis space, 
  
    
      
        P
      
    
    {\displaystyle P}
  
 refers to a probability, and 
  
    
      
        T
      
    
    {\displaystyle T}
  
 is the training data. As an ensemble, the Bayes optimal classifier represents a hypothesis that is not necessarily in 
  
    
      
        H
      
    
    {\displaystyle H}
  
. The hypothesis represented by the Bayes optimal classifier, however, is the optimal hypothesis in ensemble space (the space of all possible ensembles consisting only of hypotheses in 
  
    
      
        H
      
    
    {\displaystyle H}
  
).
This formula can be restated using Bayes' theorem, which says that the posterior is proportional to the likelihood times the prior:


  
    
      
        P
        (
        
          h
          
            i
          
        
        
          |
        
        T
        )
        â
        P
        (
        T
        
          |
        
        
          h
          
            i
          
        
        )
        P
        (
        
          h
          
            i
          
        
        )
      
    
    {\displaystyle P(h_{i}|T)\propto P(T|h_{i})P(h_{i})}
  

hence,


  
    
      
        y
        =
        
          
            
              a
              r
              g
              m
              a
              x
            
            
              
                c
                
                  j
                
              
              â
              C
            
          
        
        
          â
          
            
              h
              
                i
              
            
            â
            H
          
        
        
          P
          (
          
            c
            
              j
            
          
          
            |
          
          
            h
            
              i
            
          
          )
          P
          (
          
            h
            
              i
            
          
          
            |
          
          T
          )
        
      
    
    {\displaystyle y={\underset {c_{j}\in C}{\mathrm {argmax} }}\sum _{h_{i}\in H}{P(c_{j}|h_{i})P(h_{i}|T)}}
  

Bootstrap aggregating (bagging)[edit]
.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}Main article: Bootstrap aggregating 
The first step in the bootstrap aggregating, or bagging process, is the generation of what are called bootstrapped data sets. For each bootstrapped set, the number of elements selected is the same as the original training dataset, but elements are chosen randomly with replacement. Therefore, a given sample from the original training set may occur zero, one, or multiple times in a given bootstrapped set. The bootstrapping process also generates a side-product of out-of-bag sets. An out-of-bag dataset is the set of all items in the original training set that are not in a specific bootstrapped set. As such, each bootstrapped dataset will have one out-of-bag set, even if the out-of-bag set is the empty set. 

  An Example of possible bootstrapped datasets from an original training set. As these are chosen with replacement, a single sample from the training set can occur in a bootstrapped set more than once.
After the bootstrapped sets are created, each one given to an instance of the learning model used by the ensemble (e.g., a decision tree). The model instance uses the bootstrapped set for training, where once trained it will become an individual in the ensemble. Since each bootstrapped set is randomly selected, the sets will have variety, and as a result, the individuals in the ensemble will each have a different perspective of the original training set. During the training process, and in addition to having randomized bootstrapped sets for training, the individual models can also have limits on the scope of features considered whenever a decision is made (e.g., nodes of a decision tree). Limiting this scope can encourage the individuals of an ensemble to explore features that may otherwise not be considered.[16] If desired, after constructing the individuals in this way, each individual's respective out-of-bag set can be used for validation purposes to reduce the risk of overfitting the individual models to their training sets.[17]
The variance of local information in the bootstrap sets and feature considerations promotes diversity among the individuals of the ensemble, in keeping with ensemble theory, and can strengthen the ensemble.[18] To utilize the strength of this diversity, aggregation is employed. Aggregation is the way an ensemble translates from a series of individual assessments to one single collective assessment of a sample. The process of aggregation for an ensemble entails collecting the individual assessments of each of the models of the ensemble. Then, the individual assessments are counted, and the conclusion reached by the most individuals in the ensemble is considered to be the ensemble's overall assessment.
Below is an illustration of the aggregation process on an ensemble of four decision trees of single-node depth. A single sample is given to each of the four trees to be classified. Each makes its own individual classification of the sample, which are counted. Since three classified the sample with a positive classification, but only one yielded a negative classification, the ensemble's overall classification of the sample is positive.


One common application of the bootstrap aggregating process are random forests like the one shown. Random forests are collections of decision trees in an ensemble to achieve higher classification accuracy than an individual decision tree.

Boosting[edit]
Main article: Boosting (meta-algorithm)
Boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models mis-classified. In some cases, boosting has been shown to yield better accuracy than bagging, but it also tends to be more likely to over-fit the training data. By far, the most common implementation of boosting is Adaboost, although some newer algorithms are reported to achieve better results.[citation needed]
In Boosting, an equal weight (uniform probability distribution) is given to the sample training data (say D1) at the very starting round. This data (D1) is then given to a base learner (say L1). The mis-classified instances by L1 are assigned a weight higher than the correctly classified instances, but keeping in mind that the total probability distribution will be equal to 1. This boosted data (say D2) is then given to second base learner (say L2) and so on. The results are then combined in the form of voting.

Bayesian model averaging[edit]
Bayesian model averaging (BMA) makes predictions using an average over several models with weights given by the posterior probability of each model given the data.[19] BMA is known to generally give better answers than a single model, obtained, e.g., via stepwise regression, especially where very different models have nearly identical performance in the training set but may otherwise perform quite differently.
The most obvious question with any technique that uses Bayes' theorem is the prior, i.e., a specification of the probability (subjective, perhaps) that each model is the best to use for a given purpose.  Conceptually, BMA can be used with any prior.  The ensembleBMA[20] and BMA[21] packages for R use the prior implied by the Bayesian information criterion, (BIC), following Raftery (1995).[22] The BAS package for R supports the use of the priors implied by Akaike information criterion (AIC) and other criteria over the alternative models as well as priors over the coefficients.[23]
The difference between BIC and AIC is the strength of preference for parsimony.  The penalty for model complexity is 
  
    
      
        ln
        â¡
        (
        n
        )
        k
      
    
    {\displaystyle \ln(n)k}
  
 for the BIC and 
  
    
      
        2
        k
      
    
    {\displaystyle 2k}
  
 for the AIC.  Large sample asymptotic theory has established that if there is a best model then with increasing sample sizes, BIC is strongly consistent, i.e., will almost certainly find it, while AIC may not, because AIC may continue to place excessive posterior probability on models that are more complicated than they need to be.  If on the other hand we are more concerned with efficiency, i.e., minimum mean square prediction error, then asymptotically, AIC and AICc are âefficientâ while BIC is not.[24]
Burnham and Anderson (1998, 2002) contributed greatly to introducing a wider audience to the basic ideas of Bayesian model averaging and popularizing the methodology.[25] The availability of software, including other free open-source packages for R beyond those mentioned above, helped make the methods accessible to a wider audience.[26]
Haussler et al. (1994) showed that when BMA is used for classification, its expected error is at most twice the expected error of the Bayes optimal classifier.[27]

Bayesian model combination[edit]
Bayesian model combination (BMC) is an algorithmic correction to Bayesian model averaging (BMA). Instead of sampling each model in the ensemble individually, it samples from the space of possible ensembles (with model weightings drawn randomly from a Dirichlet distribution having uniform parameters). This modification overcomes the tendency of BMA to converge toward giving all of the weight to a single model. Although BMC is somewhat more computationally expensive than BMA, it tends to yield dramatically better results. The results from BMC have been shown to be better on average (with statistical significance) than BMA, and bagging.[28]
The use of Bayes' law to compute model weights necessitates computing the probability of the data given each model. Typically, none of the models in the ensemble are exactly the distribution from which the training data were generated, so all of them correctly receive a value close to zero for this term. This would work well if the ensemble were big enough to sample the entire model-space, but such is rarely possible. Consequently, each pattern in the training data will cause the ensemble weight to shift toward the model in the ensemble that is closest to the distribution of the training data. It essentially reduces to an unnecessarily complex method for doing model selection.
The possible weightings for an ensemble can be visualized as lying on a simplex. At each vertex of the simplex, all of the weight is given to a single model in the ensemble. BMA converges toward the vertex that is closest to the distribution of the training data. By contrast, BMC converges toward the point where this distribution projects onto the simplex. In other words, instead of selecting the one model that is closest to the generating distribution, it seeks the combination of models that is closest to the generating distribution.
The results from BMA can often be approximated by using cross-validation to select the best model from a bucket of models. Likewise, the results from BMC may be approximated by using cross-validation to select the best ensemble combination from a random sampling of possible weightings.

Bucket of models[edit]
A "bucket of models" is an ensemble technique in which a model selection algorithm is used to choose the best model for each problem. When tested with only one problem, a bucket of models can produce no better results than the best model in the set, but when evaluated across many problems, it will typically produce much better results, on average, than any model in the set.
The most common approach used for model-selection is cross-validation selection (sometimes called a "bake-off contest"). It is described with the following pseudo-code:

For each model m in the bucket:
    Do c times: (where 'c' is some constant)
        Randomly divide the training dataset into two datasets: A, and B.
        Train m with A
        Test m with B
Select the model that obtains the highest average score

Cross-Validation Selection can be summed up as: "try them all with the training set, and pick the one that works best".[29]
Gating is a generalization of Cross-Validation Selection. It involves training another learning model to decide which of the models in the bucket is best-suited to solve the problem. Often, a perceptron is used for the gating model. It can be used to pick the "best" model, or it can be used to give a linear weight to the predictions from each model in the bucket.
When a bucket of models is used with a large set of problems, it may be desirable to avoid training some of the models that take a long time to train. Landmark learning is a meta-learning approach that seeks to solve this problem. It involves training only the fast (but imprecise) algorithms in the bucket, and then using the performance of these algorithms to help determine which slow (but accurate) algorithm is most likely to do best.[30]

Stacking[edit]
Stacking (sometimes called stacked generalization) involves training a learning algorithm to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm is trained to make a final prediction using all the predictions of the other algorithms as additional inputs. If an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the ensemble techniques described in this article, although, in practice, a logistic regression model is often used as the combiner.
Stacking typically yields performance better than any single one of the trained models.[31] 
It has been successfully used on both supervised learning tasks 
(regression,[32] classification and distance learning [33])
and unsupervised learning (density estimation).[34] It has also been used to
estimate bagging's error rate.[3][35] It has been reported to out-perform Bayesian model-averaging.[36]
The two top-performers in the Netflix competition utilized blending, which may be considered to be a form of stacking.[37]

Implementations in statistics packages[edit]
R: at least three packages offer Bayesian model averaging tools,[38] including the .mw-parser-output .monospaced{font-family:monospace,monospace}BMS (an acronym for Bayesian Model Selection) package,[39] the BAS (an acronym for Bayesian Adaptive Sampling) package,[40] and the BMA package.[41]
Python: Scikit-learn, a package for machine learning in Python offers packages for ensemble learning including packages for bagging and averaging methods.
MATLAB: classification ensembles are implemented in Statistics and Machine Learning Toolbox.[42]
Ensemble learning applications[edit]
In the recent years, due to the growing computational power which allows training large ensemble learning in a reasonable time frame, the number of its applications has grown increasingly.[43] Some of the applications of ensemble classifiers include:

Remote sensing[edit]
Main article: Remote sensing
Land cover mapping[edit]
Land cover mapping is one of the major applications of Earth observation satellite sensors, using remote sensing and geospatial data, to identify the materials and objects which are located on the surface of target areas. Generally, the classes of target materials include roads, buildings, rivers, lakes, and vegetation.[44] Some different ensemble learning approaches based on artificial neural networks,[45] kernel principal component analysis (KPCA),[46] decision trees with boosting,[47] random forest[44] and automatic design of multiple classifier systems,[48] are proposed to efficiently identify land cover objects.

Change detection[edit]
Change detection is an image analysis problem, consisting of the identification of places where the land cover has changed over time. Change detection is widely used in fields such as urban growth, forest and vegetation dynamics, land use and disaster monitoring.[49]
The earliest applications of ensemble classifiers in change detection are designed with the majority voting,[50] Bayesian average and the maximum posterior probability.[51]

Computer security[edit]
Distributed denial of service[edit]
Distributed denial of service is one of the most threatening cyber-attacks that may happen to an internet service provider.[43] By combining the output of single classifiers, ensemble classifiers reduce the total error of detecting and discriminating such attacks from legitimate flash crowds.[52]

Malware Detection[edit]
Classification of malware codes such as computer viruses, computer worms, trojans, ransomware and spywares with the usage of machine learning techniques, is inspired by the document categorization problem.[53] Ensemble learning systems have shown a proper efficacy in this area.[54][55]

Intrusion detection[edit]
An intrusion detection system monitors computer network or computer systems to identify intruder codes like an anomaly detection process. Ensemble learning successfully aids such monitoring systems to reduce their total error.[56][57]

Face recognition[edit]
Main article: Face recognition
Face recognition, which recently has become one of the most popular research areas of pattern recognition, copes with identification or verification of a person by their digital images.[58]
Hierarchical ensembles based on Gabor Fisher classifier and independent component analysis preprocessing techniques are some of the earliest ensembles employed in this field.[59][60][61]

Emotion recognition[edit]
Main article: Emotion recognition
While speech recognition is mainly based on deep learning because most of the industry players in this field like Google, Microsoft and IBM reveal that the core technology of their speech recognition is based on this approach, speech-based emotion recognition can also have a satisfactory performance with ensemble learning.[62][63]
It is also being successfully used in facial emotion recognition.[64][65][66]

Fraud detection[edit]
Main article: Fraud detection
Fraud detection deals with the identification of bank fraud, such as money laundering, credit card fraud and telecommunication fraud, which have vast domains of research and applications of machine learning. Because ensemble learning improves the robustness of the normal behavior modelling, it has been proposed as an efficient technique to detect such fraudulent cases and activities in banking and credit card systems.[67][68]

Financial decision-making[edit]
The accuracy of prediction of business failure is a very crucial issue in financial decision-making. Therefore, different ensemble classifiers are proposed to predict financial crises and financial distress.[69] Also, in the trade-based manipulation problem, where traders attempt to manipulate stock prices by buying and selling activities, ensemble classifiers are required to analyze the changes in the stock market data and detect suspicious symptom of stock price manipulation.[69]

Medicine[edit]
Ensemble classifiers have been successfully applied in neuroscience, proteomics and medical diagnosis like in neuro-cognitive disorder (i.e. Alzheimer or myotonic dystrophy) detection based on MRI datasets,[70][71][72] and cervical cytology classification.[73][74]

See also[edit]
Ensemble averaging (machine learning)
Bayesian structural time series (BSTS)
References[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Opitz, D.; Maclin, R. (1999). "Popular ensemble methods: An empirical study". Journal of Artificial Intelligence Research. 11: 169â198. doi:10.1613/jair.614.

^ Polikar, R. (2006). "Ensemble based systems in decision making". IEEE Circuits and Systems Magazine. 6 (3): 21â45. doi:10.1109/MCAS.2006.1688199. S2CIDÂ 18032543.

^ Jump up to: a b Rokach, L. (2010). "Ensemble-based classifiers". Artificial Intelligence Review. 33 (1â2): 1â39. doi:10.1007/s10462-009-9124-7. S2CIDÂ 11149239.

^ Blockeel H. (2011). "Hypothesis Space". Encyclopedia of Machine Learning: 511â513. doi:10.1007/978-0-387-30164-8_373. ISBNÂ 978-0-387-30768-8.

^ Kuncheva, L. and Whitaker, C., Measures of diversity in classifier ensembles, Machine Learning, 51, pp. 181-207, 2003

^ Sollich, P. and Krogh, A., Learning with ensembles: How overfitting can be useful, Advances in Neural Information Processing Systems, volume 8, pp. 190-196, 1996.

^ Brown, G. and Wyatt, J. and Harris, R. and Yao, X., Diversity creation methods: a survey and categorisation., Information Fusion, 6(1), pp.5-20, 2005.

^ Adeva, J. J. GarcÃ­a; CerviÃ±o, Ulises; Calvo, R. (December 2005). "Accuracy and Diversity in Ensembles of Text Categorisers" (PDF). CLEI Journal. 8 (2): 1:1â1:12. doi:10.19153/cleiej.8.2.1.

^ Ho, T., Random Decision Forests, Proceedings of the Third International Conference on Document Analysis and Recognition, pp. 278-282, 1995.

^ Gashler, M.; Giraud-Carrier, C.; Martinez, T. (2008). "Decision Tree Ensemble: Small Heterogeneous Is Better Than Large Homogeneous" (PDF). The Seventh International Conference on Machine Learning and Applications. 2008: 900â905. doi:10.1109/ICMLA.2008.154. ISBNÂ 978-0-7695-3495-4. S2CIDÂ 614810.

^ Liu, Y.; Yao, X. (December 1999). "Ensemble learning via negative correlation". Neural Networks. 12 (10): 1399â1404. doi:10.1016/S0893-6080(99)00073-8. ISSNÂ 0893-6080. PMIDÂ 12662623.

^ Shoham, Ron; Permuter, Haim (2019). "Amended Cross-Entropy Cost: An Approach for Encouraging Diversity in Classification Ensemble (Brief Announcement)". Cyber Security Cryptography and Machine Learning. Lecture Notes in Computer Science. 11527: 202â207. doi:10.1007/978-3-030-20951-3_18. ISBNÂ 978-3-030-20950-6.

^ R. Bonab, Hamed; Can, Fazli (2016). A Theoretical Framework on the Ideal Number of Classifiers for Online Ensembles in Data Streams. CIKM. USA: ACM. p.Â 2053.

^ R. Bonab, Hamed; Can, Fazli (2019). Less Is More: A Comprehensive Framework for the Number of Components of Ensemble Classifiers. TNNLS. USA: IEEE. arXiv:1709.02925.

^ Tom M. Mitchell, Machine Learning, 1997, pp. 175

^ Salman, R., Alzaatreh, A., Sulieman, H., & Faisal, S. (2021). A Bootstrap Framework for Aggregating within and between Feature Selection Methods. Entropy (Basel, Switzerland), 23(2), 200. doi:10.3390/e23020200

^ Brodeur, Z. P., Herman, J. D., & Steinschneider, S. (2020). Bootstrap aggregation and cross-validation methods to reduce overfitting in reservoir control policy search. Water Resources Research, 56, e2020WR027184. doi:10.1029/2020WR027184

^ Breiman, L., Bagging Predictors, Machine Learning, 24(2), pp.123-140, 1996. doi:10.1007/BF00058655

^ e.g., Jennifer A. Hoeting; David Madigan; Adrian Raftery; Chris Volinsky (1999). "Bayesian Model Averaging: A Tutorial". Statistical Science. ISSNÂ 0883-4237. WikidataÂ Q98974344.

^ Chris Fraley; Adrian Raftery; J. McLean Sloughter; Tilmann Gneiting, ensembleBMA: Probabilistic Forecasting using Ensembles and Bayesian Model Averaging, WikidataÂ Q98972500

^ Adrian Raftery; Jennifer A. Hoeting; Chris Volinsky; Ian Painter; Ka Yee Yeung, BMA: Bayesian Model Averaging, WikidataÂ Q91674106.

^ Adrian Raftery (1995). "Bayesian model selection in social research". Sociological Methodology: 111â196. ISSNÂ 0081-1750. WikidataÂ Q91670340.

^ Merlise A. Clyde; Michael L. Littman; Quanli Wang; Joyee Ghosh; Yingbo Li; Don van den Bergh, BAS: Bayesian Variable Selection and Model Averaging using Bayesian Adaptive Sampling, WikidataÂ Q98974089.

^ Gerda Claeskens; Nils Lid Hjort (2008), Model selection and model averaging, Cambridge University Press, WikidataÂ Q62568358, ch. 4.

^ Kenneth P. Burnham; David R. Anderson (1998), Model Selection and Multimodel Inference: A practical information-theoretic approach, WikidataÂ Q62670082 and Kenneth P. Burnham; David R. Anderson (2002), Model Selection and Multimodel Inference: A practical information-theoretic approach, Springer Science+Business Media, WikidataÂ Q76889160.

^ The Wikiversity article on Searching R Packages mentions several ways to find available packages for something like this.  For example, âsos::findFn('{Bayesian model averaging}')â from within R will search for help files in contributed packages that includes the search term and open two tabs in the default browser.  The first will list all the help files found sorted by package.  The second summarizes the packages found, sorted by the apparent strength of the match.

^ Haussler, David; Kearns, Michael; Schapire, Robert E. (1994). "Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension". Machine Learning. 14: 83â113. doi:10.1007/bf00993163.

^ Monteith, Kristine; Carroll, James; Seppi, Kevin; Martinez, Tony. (2011). Turning Bayesian Model Averaging into Bayesian Model Combination (PDF). Proceedings of the International Joint Conference on Neural Networks IJCNN'11. pp.Â 2657â2663.

^ Saso Dzeroski, Bernard Zenko, Is Combining Classifiers Better than Selecting the Best One, Machine Learning, 2004, pp. 255-273

^ Bensusan, Hilan; Giraud-Carrier, Christophe (2000). "Discovering Task Neighbourhoods through Landmark Learning Performances" (PDF). Principles of Data Mining and Knowledge Discovery. Lecture Notes in Computer Science. Vol.Â 1910. pp.Â 325â330. doi:10.1007/3-540-45372-5_32. ISBNÂ 978-3-540-41066-9.

^ Wolpert (1992). "Stacked Generalization". Neural Networks. 5 (2): 241â259. doi:10.1016/s0893-6080(05)80023-1.

^ Breiman, Leo (1996). "Stacked regressions". Machine Learning. 24: 49â64. doi:10.1007/BF00117832.

^ Ozay, M.; Yarman Vural, F. T. (2013). "A New Fuzzy Stacked Generalization Technique and Analysis of its Performance". arXiv:1204.0171. Bibcode:2012arXiv1204.0171O. {{cite journal}}: Cite journal requires |journal= (help)

^ Smyth, P. and Wolpert, D. H., Linearly Combining Density Estimators via Stacking, Machine
Learning Journal, 36, 59-83, 1999

^ Wolpert, D.H., and Macready, W.G., An Efficient Method to Estimate Baggingâs Generalization Error, Machine Learning Journal, 35, 41-55, 1999

^ Clarke, B., Bayes model averaging and stacking when model approximation error cannot be ignored, Journal of Machine Learning Research, pp 683-712, 2003

^ Sill, J.; Takacs, G.; Mackey, L.; Lin, D. (2009). "Feature-Weighted Linear Stacking". arXiv:0911.0460. Bibcode:2009arXiv0911.0460S. {{cite journal}}: Cite journal requires |journal= (help)

^ Amini, Shahram M.; Parmeter, Christopher F. (2011). "Bayesian model averaging in R" (PDF). Journal of Economic and Social Measurement. 36 (4): 253â287. doi:10.3233/JEM-2011-0350.

^ "BMS: Bayesian Model Averaging Library". The Comprehensive R Archive Network. 2015-11-24. Retrieved September 9, 2016.

^ "BAS: Bayesian Model Averaging using Bayesian Adaptive Sampling". The Comprehensive R Archive Network. Retrieved September 9, 2016.

^ "BMA: Bayesian Model Averaging". The Comprehensive R Archive Network. Retrieved September 9, 2016.

^ "Classification Ensembles". MATLAB & Simulink. Retrieved June 8, 2017.

^ Jump up to: a b WoÅºniak, MichaÅ; GraÃ±a, Manuel; Corchado, Emilio (March 2014). "A survey of multiple classifier systems as hybrid systems". Information Fusion. 16: 3â17. doi:10.1016/j.inffus.2013.04.006. hdl:10366/134320.

^ Jump up to: a b Rodriguez-Galiano, V.F.; Ghimire, B.; Rogan, J.; Chica-Olmo, M.; Rigol-Sanchez, J.P. (January 2012). "An assessment of the effectiveness of a random forest classifier for land-cover classification". ISPRS Journal of Photogrammetry and Remote Sensing. 67: 93â104. Bibcode:2012JPRS...67...93R. doi:10.1016/j.isprsjprs.2011.11.002.

^ Giacinto, Giorgio; Roli, Fabio (August 2001). "Design of effective neural network ensembles for image classification purposes". Image and Vision Computing. 19 (9â10): 699â707. CiteSeerXÂ 10.1.1.11.5820. doi:10.1016/S0262-8856(01)00045-2.

^ Xia, Junshi; Yokoya, Naoto; Iwasaki, Yakira (March 2017). A novel ensemble classifier of hyperspectral and LiDAR data using morphological features. 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp.Â 6185â6189. doi:10.1109/ICASSP.2017.7953345. ISBNÂ 978-1-5090-4117-6. S2CIDÂ 40210273.

^ Mochizuki, S.; Murakami, T. (November 2012). "Accuracy comparison of land cover mapping using the object-oriented image classification with machine learning algorithms". 33rd Asian Conference on Remote Sensing 2012, ACRS 2012. 1: 126â133.

^ Giacinto, G.; Roli, F.; Fumera, G. (September 2000). Design of effective multiple classifier systems by clustering of classifiers. Proceedings 15th International Conference on Pattern Recognition. ICPR-2000. Vol.Â 2. pp.Â 160â163. CiteSeerXÂ 10.1.1.11.5328. doi:10.1109/ICPR.2000.906039. ISBNÂ 978-0-7695-0750-7. S2CIDÂ 2625643.

^ Du, Peijun; Liu, Sicong; Xia, Junshi; Zhao, Yindi (January 2013). "Information fusion techniques for change detection from multi-temporal remote sensing images". Information Fusion. 14 (1): 19â27. doi:10.1016/j.inffus.2012.05.003.

^ Defined by Bruzzone et al. (2002) as "The data class that receives the largest number of votes is taken as the class of the input pattern", this is simple majority, more accurately described as plurality voting.

^ Bruzzone, Lorenzo; Cossu, Roberto; Vernazza, Gianni (December 2002). "Combining parametric and non-parametric algorithms for a partially unsupervised classification of multitemporal remote-sensing images" (PDF). Information Fusion. 3 (4): 289â297. doi:10.1016/S1566-2535(02)00091-X.

^ Raj Kumar, P. Arun; Selvakumar, S. (July 2011). "Distributed denial of service attack detection using an ensemble of neural classifier". Computer Communications. 34 (11): 1328â1341. doi:10.1016/j.comcom.2011.01.012.

^ Shabtai, Asaf; Moskovitch, Robert; Elovici, Yuval; Glezer, Chanan (February 2009). "Detection of malicious code by applying machine learning classifiers on static features: A state-of-the-art survey". Information Security Technical Report. 14 (1): 16â29. doi:10.1016/j.istr.2009.03.003.

^ Zhang, Boyun; Yin, Jianping; Hao, Jingbo; Zhang, Dingxing; Wang, Shulin (2007). Malicious Codes Detection Based on Ensemble Learning. Autonomic and Trusted Computing. Lecture Notes in Computer Science. Vol.Â 4610. pp.Â 468â477. doi:10.1007/978-3-540-73547-2_48. ISBNÂ 978-3-540-73546-5.

^ Menahem, Eitan; Shabtai, Asaf; Rokach, Lior; Elovici, Yuval (February 2009). "Improving malware detection by applying multi-inducer ensemble". Computational Statistics & Data Analysis. 53 (4): 1483â1494. CiteSeerXÂ 10.1.1.150.2722. doi:10.1016/j.csda.2008.10.015.

^ Locasto, Michael E.; Wang, Ke; Keromytis, Angeles D.; Salvatore, J. Stolfo (2005). FLIPS: Hybrid Adaptive Intrusion Prevention. Recent Advances in Intrusion Detection. Lecture Notes in Computer Science. Vol.Â 3858. pp.Â 82â101. CiteSeerXÂ 10.1.1.60.3798. doi:10.1007/11663812_5. ISBNÂ 978-3-540-31778-4.

^ Giacinto, Giorgio; Perdisci, Roberto; Del Rio, Mauro; Roli, Fabio (January 2008). "Intrusion detection in computer networks by a modular ensemble of one-class classifiers". Information Fusion. 9 (1): 69â82. CiteSeerXÂ 10.1.1.69.9132. doi:10.1016/j.inffus.2006.10.002.

^ Mu, Xiaoyan; Lu, Jiangfeng; Watta, Paul; Hassoun, Mohamad H. (July 2009). Weighted voting-based ensemble classifiers with application to human face recognition and voice recognition. 2009 International Joint Conference on Neural Networks. pp.Â 2168â2171. doi:10.1109/IJCNN.2009.5178708. ISBNÂ 978-1-4244-3548-7. S2CIDÂ 18850747.

^ Yu, Su; Shan, Shiguang; Chen, Xilin; Gao, Wen (April 2006). Hierarchical ensemble of Gabor Fisher classifier for face recognition. Automatic Face and Gesture Recognition, 2006. FGR 2006. 7th International Conference on Automatic Face and Gesture Recognition (FGR06). pp.Â 91â96. doi:10.1109/FGR.2006.64. ISBNÂ 978-0-7695-2503-7. S2CIDÂ 1513315.

^ Su, Y.; Shan, S.; Chen, X.; Gao, W. (September 2006). Patch-based gabor fisher classifier for face recognition. Proceedings - International Conference on Pattern Recognition. Vol.Â 2. pp.Â 528â531. doi:10.1109/ICPR.2006.917. ISBNÂ 978-0-7695-2521-1. S2CIDÂ 5381806.

^ Liu, Yang; Lin, Yongzheng; Chen, Yuehui (July 2008). Ensemble Classification Based on ICA for Face Recognition. Proceedings - 1st International Congress on Image and Signal Processing, IEEE Conference, CISP 2008. pp.Â 144â148. doi:10.1109/CISP.2008.581. ISBNÂ 978-0-7695-3119-9. S2CIDÂ 16248842.

^ Rieger, Steven A.; Muraleedharan, Rajani; Ramachandran, Ravi P. (2014). Speech based emotion recognition using spectral feature extraction and an ensemble of kNN classifiers. Proceedings of the 9th International Symposium on Chinese Spoken Language Processing, ISCSLP 2014. pp.Â 589â593. doi:10.1109/ISCSLP.2014.6936711. ISBNÂ 978-1-4799-4219-0. S2CIDÂ 31370450.

^ Krajewski, Jarek; Batliner, Anton; Kessel, Silke (October 2010). Comparing Multiple Classifiers for Speech-Based Detection of Self-Confidence - A Pilot Study (PDF). 2010 20th International Conference on Pattern Recognition. pp.Â 3716â3719. doi:10.1109/ICPR.2010.905. ISBNÂ 978-1-4244-7542-1. S2CIDÂ 15431610.

^ Rani, P. Ithaya; Muneeswaran, K. (25 May 2016). "Recognize the facial emotion in video sequences using eye and mouth temporal Gabor features". Multimedia Tools and Applications. 76 (7): 10017â10040. doi:10.1007/s11042-016-3592-y. S2CIDÂ 20143585.

^ Rani, P. Ithaya; Muneeswaran, K. (August 2016). "Facial Emotion Recognition Based on Eye and Mouth Regions". International Journal of Pattern Recognition and Artificial Intelligence. 30 (7): 1655020. doi:10.1142/S021800141655020X.

^ Rani, P. Ithaya; Muneeswaran, K (28 March 2018). "Emotion recognition based on facial components". SÄdhanÄ. 43 (3). doi:10.1007/s12046-018-0801-6.

^ Louzada, Francisco; Ara, Anderson (October 2012). "Bagging k-dependence probabilistic networks: An alternative powerful fraud detection tool". Expert Systems with Applications. 39 (14): 11583â11592. doi:10.1016/j.eswa.2012.04.024.

^ Sundarkumar, G. Ganesh; Ravi, Vadlamani (January 2015). "A novel hybrid undersampling method for mining unbalanced datasets in banking and insurance". Engineering Applications of Artificial Intelligence. 37: 368â377. doi:10.1016/j.engappai.2014.09.019.

^ Jump up to: a b Kim, Yoonseong; Sohn, So Young (August 2012). "Stock fraud detection using peer group analysis". Expert Systems with Applications. 39 (10): 8986â8992. doi:10.1016/j.eswa.2012.02.025.

^ Savio, A.; GarcÃ­a-SebastiÃ¡n, M.T.; Chyzyk, D.; Hernandez, C.; GraÃ±a, M.; Sistiaga, A.; LÃ³pez de Munain, A.; VillanÃºa, J. (August 2011). "Neurocognitive disorder detection based on feature vectors extracted from VBM analysis of structural MRI". Computers in Biology and Medicine. 41 (8): 600â610. doi:10.1016/j.compbiomed.2011.05.010. PMIDÂ 21621760.

^ Ayerdi, B.; Savio, A.; GraÃ±a, M. (June 2013). Meta-ensembles of classifiers for Alzheimer's disease detection using independent ROI features. Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). Lecture Notes in Computer Science. Vol.Â 7931. pp.Â 122â130. doi:10.1007/978-3-642-38622-0_13. ISBNÂ 978-3-642-38621-3.

^ Gu, Quan; Ding, Yong-Sheng; Zhang, Tong-Liang (April 2015). "An ensemble classifier based prediction of G-protein-coupled receptor classes in low homology". Neurocomputing. 154: 110â118. doi:10.1016/j.neucom.2014.12.013.

^ Xue, Dan; Zhou, Xiaomin; Li, Chen; Yao, Yudong; Rahaman, Md Mamunur; Zhang, Jinghua; Chen, Hao; Zhang, Jinpeng; Qi, Shouliang; Sun, Hongzan (2020). "An Application of Transfer Learning and Ensemble Learning Techniques for Cervical Histopathology Image Classification". IEEE Access. 8: 104603â104618. doi:10.1109/ACCESS.2020.2999816. ISSNÂ 2169-3536. S2CIDÂ 219689893.

^ Manna, Ankur; Kundu, Rohit; Kaplun, Dmitrii; Sinitca, Aleksandr; Sarkar, Ram (December 2021). "A fuzzy rank-based ensemble of CNN models for classification of cervical cytology". Scientific Reports. 11 (1): 14538. doi:10.1038/s41598-021-93783-8. ISSNÂ 2045-2322. PMCÂ 8282795. PMIDÂ 34267261.


Further reading[edit]
Zhou Zhihua (2012). Ensemble Methods: Foundations and Algorithms. Chapman and Hall/CRC. ISBNÂ 978-1-439-83003-1.
Robert Schapire; Yoav Freund (2012). Boosting: Foundations and Algorithms. MIT. ISBNÂ 978-0-262-01718-3.
External links[edit]
Robi Polikar (ed.). "Ensemble learning". Scholarpedia.
The Waffles (machine learning) toolkit contains implementations of Bagging, Boosting, Bayesian Model Averaging, Bayesian Model Combination, Bucket-of-models, and other ensemble techniques




<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=Ensemble_learning&oldid=1067251577"
		Categories: Ensemble learningHidden categories: CS1 errors: missing periodicalArticles with short descriptionShort description is different from WikidataAll articles with specifically marked weasel-worded phrasesArticles with specifically marked weasel-worded phrases from December 2017All articles with unsourced statementsArticles with unsourced statements from December 2017Articles with unsourced statements from January 2012
	
