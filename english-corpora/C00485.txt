
Title:
Anomaly detection
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Approach in data analysis
.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}For broader coverage of this topic, see Outlier.
.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}Part of a series onMachine learningand data mining
Problems
Classification
Clustering
Regression
Anomaly detection
Data Cleaning
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction

Supervised learning.mw-parser-output .nobold{font-weight:normal}(classificationÂ â¢ regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)

Clustering
BIRCH
CURE
Hierarchical
k-means
Expectationâmaximization (EM)
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t-SNE

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
k-NN
Local outlier factor

Artificial neural network
Autoencoder
Cognitive computing
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
ESN
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net
Transformer
Vision
Spiking neural network
Memtransistor
Electrochemical RAM (ECRAM)

Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)

Theory
Kernel machines
Biasâvariance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory

Machine-learning venues
NeurIPS
ICML
ML
JMLR
ArXiv:cs.LG

Related articles
Glossary of artificial intelligence
List of datasets for machine-learning research
Outline of machine learning
.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vte
Part of a series onInformation security
Related security categories
Computer security
Automotive security
Cybercrime
Cybersex trafficking
Computer fraud
Cybergeddon
Cyberterrorism
Cyberwarfare
Electronic warfare
Information warfare
Internet security
Mobile security
Network security
Copy protection
Digital rights management

Threats
Adware
Advanced persistent threat
Arbitrary code execution
Backdoors
Hardware backdoors
Code injection
Crimeware
Cross-site scripting
Cryptojacking malware
Botnets
Data breach
Drive-by download
browser helper objects
Computer crime
Viruses
Data scraping
Denial of service
Eavesdropping
Email fraud
Email spoofing
Exploits
Keyloggers
Logic bombs
Time bombs
Fork bombs
Zip bombs
Fraudulent dialers
Malware
Payload
Phishing
Polymorphic engine
Privilege escalation
Ransomware
Rootkits
Bootkits
Scareware
Shellcode
Spamming
Social engineering (security)
Screen scraping
Spyware
Software bugs
Trojan horses
Hardware Trojans
Remote access trojans
Vulnerability
Web shells
Wiper
Worms
SQL injection
Rogue security software
Zombie

Defenses
Application security
Secure coding
Secure by default
Secure by design
Misuse case
Computer access control
Authentication
Multi-factor authentication
Authorization
Computer security software
Antivirus software
Security-focused operating system
Data-centric security
Code obfuscation
Data masking
Encryption
Firewall
Intrusion detection system
Host-based intrusion detection system (HIDS)
Anomaly detection
Security information and event management (SIEM)
Mobile secure gateway
Runtime application self-protection
vte
In data analysis, anomaly detection (also referred to as outlier detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data. Such examples may arouse suspicions of being generated by a different mechanism, or appear inconsistent with the data.[1] 
Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.[2]
In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro clusters formed by these patterns.[3]
Three broad categories of anomaly detection techniques exist.[4] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as "normal" and "abnormal" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the utilized model.

Contents

1 Applications
2 Popular techniques
3 Application to data security
4 In data pre-processing
5 Software
6 Datasets
7 See also
8 References



Applications[edit]
Anomaly detection is applicable in a very large number and variety of domains, and is an important subarea of unsupervised machine learning. As such it has applications in cyber-security intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, detecting ecosystem disturbances, defect detection in images using machine vision, medical diagnosis and law enforcement.[5] 
It is often used in preprocessing to remove anomalous data from the dataset. This is done for a number of reasons. Statistics of data such as the mean and standard deviation are more accurate after the removal of anomalies, and the visualisation of data can also be improved. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy.[6][7] Anomalies are also often the most important observations in the data to be found such as in intrusion detection or detecting abnormalities in medical images.

Popular techniques[edit]
Several anomaly detection techniques have been proposed in literature.[4][8] Some of the popular techniques are:

Statistical: Z-score, modified Z-score, IQR (Tukey's method) and Grubbs test.
Distance based methods:
Density-based techniques (k-nearest neighbor,[9][10][11] local outlier factor,[12] isolation forests,[13][14] and many more variations of this concept[15]).
Subspace-,[16] correlation-based[17] and tensor-based [18] outlier detection for high-dimensional data.[19]
One-class support vector machines.[20]
Replicator neural networks,[21] autoencoders, variational autoencoders,[22] long short-term memory neural networks[23]
Bayesian networks.[21]
Hidden Markov models (HMMs).[21]
Minimum Covariance Determinant[24][25]
Clustering: DBSCAN, KMeans, Cluster analysis-based outlier detection.[26][27]
Deviations from association rules and frequent itemsets.
Fuzzy logic-based outlier detection.
Ensemble techniques, using feature bagging,[28][29] score normalization[30][31] and different sources of diversity.[32][33]
The performance of different methods depends a lot on the data set and parameters, and methods have little systematic advantages over another when compared across many data sets and parameters.[34][35]

Application to data security[edit]
Anomaly detection was proposed for intrusion detection systems (IDS) by Dorothy Denning in 1986.[36] Anomaly detection for IDS is normally accomplished with thresholds and statistics, but can also be done with soft computing, and inductive learning.[37] Types of statistics proposed by 1999 included profiles of users, workstations, networks, remote hosts, groups of users, and programs based on frequencies, means, variances, covariances, and standard deviations.[38]  The counterpart of anomaly detection in intrusion detection is misuse detection.

In data pre-processing[edit]
In supervised learning, anomaly detection is often an important step in data pre-processing to provide the learning algorithm a proper dataset to learn on. This is also known as Data cleansing.  After detecting anomalous samples classifiers remove them, however, at times corrupted data can still provide useful samples for learning. A common method for finding appropriate samples to use is identifying Noisy data. One approach to find noisy values is to create a probabilistic model from data using models of uncorrupted data and corrupted data.[39]
Below is an example of the Iris flower data set with an anomaly added. With an anomaly included, classification algorithm may have difficulties properly finding patterns, or run into errors. 


Fischer's Iris Data with an Anomaly


Dataset order

Sepal length

Sepal width

Petal length

Petal width

Species


1

5.1

3.5

1.4

0.2

I. setosa


2

4.9

3.0

1.4

0.2

I. setosa


3

4.7

3.2

1.3

0.2

I. setosa


4

4.6

3.1

1.5

0.2

I. setosa


5

5.0

NULL

1.4

NULL

I. setosa

By removing the anomaly, training will be enabled to find patterns in classifications more easily.
In data mining, high-dimensional data will also propose high computing challenges with intensely large sets of data. By removing numerous samples that can find itself irrelevant to a classifier or detection algorithm, runtime can be significantly reduced on even the largest sets of data.

Software[edit]
ELKI is an open-source Java data mining toolkit that contains several anomaly detection algorithms, as well as index acceleration for them.
Scikit-Learn is an open-source Python library that has built functionality to provide unsupervised anomaly detection.
Datasets[edit]
Anomaly detection benchmark data repository with carefully chosen data sets of the Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen; Mirror at University of SÃ£o Paulo.
ODDS â ODDS: A large collection of publicly available outlier detection datasets with ground truth in different domains.
Unsupervised Anomaly Detection Benchmark at Harvard Dataverse: Datasets for Unsupervised Anomaly Detection with ground truth.
KMASH Data Repository  at Research Data Australia having more than 12,000 anomaly detection datasets with ground truth.
See also[edit]
Change detection
Statistical process control
Novelty detection
Hierarchical temporal memory
References[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Hawkins, Douglas M. (1980). Identification of Outliers. Chapman and Hall London; New York. 

^ Hodge, V. J.; Austin, J. (2004). "A Survey of Outlier Detection Methodologies" (PDF). Artificial Intelligence Review. 22 (2): 85â126. CiteSeerXÂ 10.1.1.318.4023. doi:10.1007/s10462-004-4304-y. S2CIDÂ 3330313.

^ Dokas, Paul; Ertoz, Levent; Kumar, Vipin; Lazarevic, Aleksandar; Srivastava, Jaideep; Tan, Pang-Ning (2002). "Data mining for network intrusion detection" (PDF). Proceedings NSF Workshop on Next Generation Data Mining.

^ Jump up to: a b Chandola, V.; Banerjee, A.; Kumar, V. (2009). "Anomaly detection: A survey". ACM Computing Surveys. 41 (3): 1â58. doi:10.1145/1541880.1541882. S2CIDÂ 207172599.

^ Aggarwal, Charu (2017). Outlier Analysis. Springer Publishing Company, Incorporated. ISBNÂ 3319475770.

^ Tomek, Ivan (1976). "An Experiment with the Edited Nearest-Neighbor Rule". IEEE Transactions on Systems, Man, and Cybernetics. 6 (6): 448â452. doi:10.1109/TSMC.1976.4309523.

^ Smith, M. R.; Martinez, T. (2011). "Improving classification accuracy by identifying and removing instances that should be misclassified" (PDF). The 2011 International Joint Conference on Neural Networks. p.Â 2690. CiteSeerXÂ 10.1.1.221.1371. doi:10.1109/IJCNN.2011.6033571. ISBNÂ 978-1-4244-9635-8. S2CIDÂ 5809822.

^ Zimek, Arthur; Filzmoser, Peter (2018). "There and back again: Outlier detection between statistical reasoning and data mining algorithms" (PDF). Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery. 8 (6): e1280. doi:10.1002/widm.1280. ISSNÂ 1942-4787.

^ Knorr, E. M.; Ng, R. T.; Tucakov, V. (2000). "Distance-based outliers: Algorithms and applications". The VLDB Journal the International Journal on Very Large Data Bases. 8 (3â4): 237â253. CiteSeerXÂ 10.1.1.43.1842. doi:10.1007/s007780050006. S2CIDÂ 11707259.

^ Ramaswamy, S.; Rastogi, R.; Shim, K. (2000). Efficient algorithms for mining outliers from large data sets. Proceedings of the 2000 ACM SIGMOD international conference on Management of data â SIGMOD '00. p.Â 427. doi:10.1145/342009.335437. ISBNÂ 1-58113-217-4.

^ Angiulli, F.; Pizzuti, C. (2002). Fast Outlier Detection in High Dimensional Spaces. Principles of Data Mining and Knowledge Discovery. Lecture Notes in Computer Science. Vol.Â 2431. p.Â 15. doi:10.1007/3-540-45681-3_2. ISBNÂ 978-3-540-44037-6.

^ Breunig, M. M.; Kriegel, H.-P.; Ng, R. T.; Sander, J. (2000). LOF: Identifying Density-based Local Outliers (PDF). Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data. SIGMOD. pp.Â 93â104. doi:10.1145/335191.335388. ISBNÂ 1-58113-217-4.

^ Liu, Fei Tony; Ting, Kai Ming; Zhou, Zhi-Hua (December 2008). Isolation Forest. 2008 Eighth IEEE International Conference on Data Mining. pp.Â 413â422. doi:10.1109/ICDM.2008.17. ISBNÂ 9780769535029. S2CIDÂ 6505449.

^ Liu, Fei Tony; Ting, Kai Ming; Zhou, Zhi-Hua (March 2012). "Isolation-Based Anomaly Detection". ACM Transactions on Knowledge Discovery from Data. 6 (1): 1â39. doi:10.1145/2133360.2133363. S2CIDÂ 207193045.

^ Schubert, E.; Zimek, A.; Kriegel, H. -P. (2012). "Local outlier detection reconsidered: A generalized view on locality with applications to spatial, video, and network outlier detection". Data Mining and Knowledge Discovery. 28: 190â237. doi:10.1007/s10618-012-0300-z. S2CIDÂ 19036098.

^ Kriegel, H. P.; KrÃ¶ger, P.; Schubert, E.; Zimek, A. (2009). Outlier Detection in Axis-Parallel Subspaces of High Dimensional Data. Advances in Knowledge Discovery and Data Mining. Lecture Notes in Computer Science. Vol.Â 5476. p.Â 831. doi:10.1007/978-3-642-01307-2_86. ISBNÂ 978-3-642-01306-5.

^ Kriegel, H. P.; Kroger, P.; Schubert, E.; Zimek, A. (2012). Outlier Detection in Arbitrarily Oriented Subspaces. 2012 IEEE 12th International Conference on Data Mining. p.Â 379. doi:10.1109/ICDM.2012.21. ISBNÂ 978-1-4673-4649-8.

^ Fanaee-T, H.; Gama, J. (2016). "Tensor-based anomaly detection: An interdisciplinary survey". Knowledge-Based Systems. 98: 130â147. doi:10.1016/j.knosys.2016.01.027.

^ Zimek, A.; Schubert, E.; Kriegel, H.-P. (2012). "A survey on unsupervised outlier detection in high-dimensional numerical data". Statistical Analysis and Data Mining. 5 (5): 363â387. doi:10.1002/sam.11161.

^ SchÃ¶lkopf, B.; Platt, J. C.; Shawe-Taylor, J.; Smola, A. J.; Williamson, R. C. (2001). "Estimating the Support of a High-Dimensional Distribution". Neural Computation. 13 (7): 1443â71. CiteSeerXÂ 10.1.1.4.4106. doi:10.1162/089976601750264965. PMIDÂ 11440593. S2CIDÂ 2110475.

^ Jump up to: a b c Hawkins, Simon; He, Hongxing; Williams, Graham; Baxter, Rohan (2002). "Outlier Detection Using Replicator Neural Networks". Data Warehousing and Knowledge Discovery. Lecture Notes in Computer Science. Vol.Â 2454. pp.Â 170â180. CiteSeerXÂ 10.1.1.12.3366. doi:10.1007/3-540-46145-0_17. ISBNÂ 978-3-540-44123-6.

^ J. An and S. Cho, "Variational autoencoder based anomaly detection using reconstruction probability", 2015.

^ Malhotra, Pankaj; Vig, Lovekesh; Shroff, Gautman; Agarwal, Puneet (22â24 April 2015). Long Short Term Memory Networks for Anomaly Detection in Time Series. European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning. Bruges (Belgium).

^ Hubert, M., Debruyne, M. and Rousseeuw, P.J., 2018. Minimum covariance determinant and extensions. Wiley Interdisciplinary Reviews: Computational Statistics, 10(3), p.e1421.

^ Hubert, M. and Debruyne, M., 2010. Minimum covariance determinant. Wiley interdisciplinary reviews: Computational statistics, 2(1), pp.36-43.

^ He, Z.; Xu, X.; Deng, S. (2003). "Discovering cluster-based local outliers". Pattern Recognition Letters. 24 (9â10): 1641â1650. CiteSeerXÂ 10.1.1.20.4242. doi:10.1016/S0167-8655(03)00003-5.

^ Campello, R. J. G. B.; Moulavi, D.; Zimek, A.; Sander, J. (2015). "Hierarchical Density Estimates for Data Clustering, Visualization, and Outlier Detection". ACM Transactions on Knowledge Discovery from Data. 10 (1): 5:1â51. doi:10.1145/2733381. S2CIDÂ 2887636.

^ Lazarevic, A.; Kumar, V. (2005). Feature bagging for outlier detection. Proc. 11th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining. pp.Â 157â166. CiteSeerXÂ 10.1.1.399.425. doi:10.1145/1081870.1081891. ISBNÂ 978-1-59593-135-1. S2CIDÂ 2054204.

^ Nguyen, H. V.; Ang, H. H.; Gopalkrishnan, V. (2010). Mining Outliers with Ensemble of Heterogeneous Detectors on Random Subspaces. Database Systems for Advanced Applications. Lecture Notes in Computer Science. Vol.Â 5981. p.Â 368. doi:10.1007/978-3-642-12026-8_29. ISBNÂ 978-3-642-12025-1.

^ Kriegel, H. P.; KrÃ¶ger, P.; Schubert, E.; Zimek, A. (2011). Interpreting and Unifying Outlier Scores. Proceedings of the 2011 SIAM International Conference on Data Mining. pp.Â 13â24. CiteSeerXÂ 10.1.1.232.2719. doi:10.1137/1.9781611972818.2. ISBNÂ 978-0-89871-992-5.

^ Schubert, E.; Wojdanowski, R.; Zimek, A.; Kriegel, H. P. (2012). On Evaluation of Outlier Rankings and Outlier Scores. Proceedings of the 2012 SIAM International Conference on Data Mining. pp.Â 1047â1058. doi:10.1137/1.9781611972825.90. ISBNÂ 978-1-61197-232-0.

^ Zimek, A.; Campello, R. J. G. B.; Sander, J. R. (2014). "Ensembles for unsupervised outlier detection". ACM SIGKDD Explorations Newsletter. 15: 11â22. doi:10.1145/2594473.2594476. S2CIDÂ 8065347.

^ Zimek, A.; Campello, R. J. G. B.; Sander, J. R. (2014). Data perturbation for outlier detection ensembles. Proceedings of the 26th International Conference on Scientific and Statistical Database Management â SSDBM '14. p.Â 1. doi:10.1145/2618243.2618257. ISBNÂ 978-1-4503-2722-0.

^ Campos, Guilherme O.; Zimek, Arthur; Sander, JÃ¶rg; Campello, Ricardo J. G. B.; MicenkovÃ¡, Barbora; Schubert, Erich; Assent, Ira; Houle, Michael E. (2016). "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study". Data Mining and Knowledge Discovery. 30 (4): 891. doi:10.1007/s10618-015-0444-8. ISSNÂ 1384-5810. S2CIDÂ 1952214.

^ Anomaly detection benchmark data repository of the Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen; Mirror at University of SÃ£o Paulo.

^ Denning, D. E. (1987). "An Intrusion-Detection Model" (PDF). IEEE Transactions on Software Engineering. SE-13 (2): 222â232. CiteSeerXÂ 10.1.1.102.5127. doi:10.1109/TSE.1987.232894. S2CIDÂ 10028835.

^ Teng, H. S.; Chen, K.; Lu, S. C. (1990). Adaptive real-time anomaly detection using inductively generated sequential patterns (PDF). Proceedings of the IEEE Computer Society Symposium on Research in Security and Privacy. pp.Â 278â284. doi:10.1109/RISP.1990.63857. ISBNÂ 978-0-8186-2060-7. S2CIDÂ 35632142.

^ Jones, Anita K.; Sielken, Robert S. (1999). "Computer System Intrusion Detection: A Survey". Technical Report, Department of Computer Science, University of Virginia, Charlottesville, VA. CiteSeerXÂ 10.1.1.24.7802.

^ Kubica, J.; Moore, A. (2003). "Probabilistic noise identification and data cleaning". Third IEEE International Conference on Data Mining. IEEE Comput. Soc: 131â138. doi:10.1109/icdm.2003.1250912. ISBNÂ 0-7695-1978-4.


.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}Authority control: National libraries  
United States





<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=Anomaly_detection&oldid=1069421664"
		Categories: Data miningMachine learningData securityStatistical outliersHidden categories: CS1: long volume valueArticles with short descriptionShort description is different from WikidataArticles with LCCN identifiers
	
