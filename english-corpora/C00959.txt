
Title:
Timeline of machine learning
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		This article needs to be updated. Please help update this article to reflect recent events or newly available information.  (August 2021)
This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events are included.

Contents

1 Overview
2 Timeline
3 See also
4 References



Overview[edit]



Decade
Summary


<1950s
Statistical methods are discovered and refined.


1950s
Pioneering machine learning research is conducted using simple algorithms.


1960s
Bayesian methods are introduced for probabilistic inference in machine learning.[1]


1970s
'AI Winter' caused by pessimism about machine learning effectiveness.


1980s
Rediscovery of backpropagation causes a resurgence in machine learning research.


1990s
Work on Machine learning shifts from a knowledge-driven approach to a data-driven approach. Scientists begin creating programs for computers to analyze large amounts of data and draw conclusionsÂ â  or "learn"Â â  from the results.[2] Support-vector machines (SVMs) and recurrent neural networks (RNNs) become popular.[3] The fields of computational complexity via neural networks and super-Turing computation started.[4]


2000s
Support-Vector Clustering[5]  and other kernel methods[6] and unsupervised machine learning methods become widespread.[7]


2010s
Deep learning becomes feasible, which leads to machine learning becoming integral to many widely used software services and applications.

Timeline[edit]
  A simple neural network with two input units and one output unit





Year
Event type
Caption
Event


1763
Discovery
The Underpinnings of Bayes' Theorem
Thomas Bayes's work An Essay towards solving a Problem in the Doctrine of Chances is published two years after his death, having been amended and edited by a friend of Bayes, Richard Price.[8] The essay presents work which underpins Bayes theorem.


1805
Discovery
Least Square
Adrien-Marie Legendre describes the "mÃ©thode des moindres carrÃ©s", known in English as the least squares method.[9] The least squares method is used widely in data fitting.


1812

Bayes' Theorem
Pierre-Simon Laplace publishes ThÃ©orie Analytique des ProbabilitÃ©s, in which he expands upon the work of Bayes and defines what is now known as Bayes' Theorem.[10]


1913
Discovery
Markov Chains
Andrey Markov first describes techniques he used to analyse a poem. The techniques later become known as Markov chains.[11]


1943

Discovery

Artificial Neuron

Warren McCulloch and Walter Pitts develop a mathematical model that imitates the functioning of a biological neuron, the artificial neuron which is considered to be the first neural model invented.[12]


1950

Turing's Learning Machine
Alan Turing proposes a 'learning machine' that could learn and become artificially intelligent. Turing's specific proposal foreshadows genetic algorithms.[13]


1951

First Neural Network Machine
Marvin Minsky and Dean Edmonds build the first neural network machine, able to learn, the SNARC.[14]


1952

Machines Playing Checkers
Arthur Samuel joins IBM's Poughkeepsie Laboratory and begins working on some of the very first machine learning programs, first creating programs that play checkers.[15]


1957
Discovery
Perceptron
Frank Rosenblatt invents the perceptron while working at the Cornell Aeronautical Laboratory.[16] The invention of the perceptron generated a great deal of excitement and was widely covered in the media.[17]


1963
Achievement
Machines Playing Tic-Tac-Toe
Donald Michie creates a 'machine' consisting of 304 match boxes and beads, which uses reinforcement learning to play Tic-tac-toe (also known as noughts and crosses).[18]


1967

Nearest Neighbor
The nearest neighbor algorithm was created, which is the start of basic pattern recognition. The algorithm was used to map routes.[2]


1969

Limitations of Neural Networks
Marvin Minsky and Seymour Papert publish their book Perceptrons, describing some of the limitations of perceptrons and neural networks. The interpretation that the book shows that neural networks are fundamentally limited is seen as a hindrance for research into neural networks.[19][20]


1970

Automatic Differentiation (Backpropagation)
Seppo Linnainmaa publishes the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions.[21][22] This corresponds to the modern version of backpropagation, but is not yet named as such.[23][24][25][26]


1979

Stanford Cart
Students at Stanford University develop a cart that can navigate and avoid obstacles in a room.[2]


1979
Discovery
Neocognitron
Kunihiko Fukushima first publishes his work on the neocognitron, a type of artificial neural network (ANN).[27][28] Neocognition later inspires convolutional neural networks (CNNs).[29]


1981

Explanation Based Learning
Gerald Dejong introduces Explanation Based Learning, where a computer algorithm analyses data and creates a general rule it can follow and discard unimportant data.[2]


1982
Discovery
Recurrent Neural Network
John Hopfield popularizes Hopfield networks, a type of recurrent neural network that can serve as content-addressable memory systems.[30]


1985

NetTalk
A program that learns to pronounce words the same way a baby does, is developed by Terry Sejnowski.[2]


1986
Application
Backpropagation
Seppo Linnainmaa's reverse mode of automatic differentiation (first applied to neural networks by Paul Werbos) is used in experiments by David Rumelhart, Geoff Hinton and Ronald J. Williams to learn internal representations.[31]


1989
Discovery
Reinforcement Learning
Christopher Watkins develops Q-learning, which greatly improves the practicality and feasibility of reinforcement learning.[32]


1989
Commercialization
Commercialization of Machine Learning on Personal Computers
Axcelis, Inc. releases Evolver, the first software package to commercialize the use of genetic algorithms on personal computers.[33]


1992
Achievement
Machines Playing Backgammon
Gerald Tesauro develops TD-Gammon, a computer backgammon program that uses an artificial neural network trained using temporal-difference learning (hence the 'TD' in the name). TD-Gammon is able to rival, but not consistently surpass, the abilities of top human backgammon players.[34]


1995
Discovery
Random Forest Algorithm
Tin Kam Ho publishes a paper describing random decision forests.[35]


1995
Discovery
Support-Vector Machines
Corinna Cortes and Vladimir Vapnik publish their work on support-vector machines.[36][37]


1997
Achievement
IBM Deep Blue Beats Kasparov
IBM's Deep Blue beats the world champion at chess.[2]


1997
Discovery
LSTM
Sepp Hochreiter and JÃ¼rgen Schmidhuber invent long short-term memory (LSTM) recurrent neural networks,[38] greatly improving the efficiency and practicality of recurrent neural networks.


1998

MNIST database
A team led by Yann LeCun releases the MNIST database, a dataset comprising a mix of handwritten digits from American Census Bureau employees and American high school students.[39] The MNIST database has since become a benchmark for evaluating handwriting recognition.


2002

Torch Machine Learning Library
Torch, a software library for machine learning, is first released.[40]


2006

The Netflix Prize
The Netflix Prize competition is launched by Netflix. The aim of the competition was to use machine learning to beat Netflix's own recommendation software's accuracy in predicting a user's rating for a film given their ratings for previous films by at least 10%.[41] The prize was won in 2009.


2009

Achievement

ImageNet

ImageNet is created. ImageNet is a large visual database envisioned by Fei-Fei Li from Stanford University, who realized that the best machine learning algorithms wouldn't work well if the data didn't reflect the real world.[42] For many, ImageNet was the catalyst for the AI boom[43] of the 21st century.


2010

Kaggle Competition
Kaggle, a website that serves as a platform for machine learning competitions, is launched.[44]


2011
Achievement
Beating Humans in Jeopardy
Using a combination of machine learning, natural language processing and information retrieval techniques, IBM's Watson beats two human champions in a Jeopardy! competition.[45]


2012
Achievement
Recognizing Cats on YouTube
The Google Brain team, led by Andrew Ng and Jeff Dean, create a neural network that learns to recognize cats by watching unlabeled images taken from frames of YouTube videos.[46][47]


2014

Leap in Face Recognition
Facebook researchers publish their work on DeepFace, a system that uses neural networks that identifies faces with 97.35% accuracy. The results are an improvement of more than 27% over previous systems and rivals human performance.[48]


2014

Sibyl
Researchers from Google detail their work on Sibyl,[49] a proprietary platform for massively parallel machine learning used internally by Google to make predictions about user behavior and provide recommendations.[50]


2016
Achievement
Beating Humans in Go
Google's AlphaGo program becomes the first Computer Go program to beat an unhandicapped professional human player[51] using a combination of machine learning and tree search techniques.[52] Later improved as AlphaGo Zero and then in 2017 generalized to Chess and more two-player games with AlphaZero.

See also[edit]
History of artificial intelligence
Machine learning
Timeline of artificial intelligence
Timeline of machine translation
References[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ Solomonoff, Ray J. "A formal theory of inductive inference. Part II." Information and control 7.2 (1964): 224â254.

^ a b c d e f .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Marr, Bernard. "A Short History of Machine Learning â Every Manager Should Read". Forbes. Retrieved 28 Sep 2016.

^ Siegelmann, Hava; Sontag, Eduardo (1995). "Computational Power of Neural Networks". Journal of Computer and System Sciences. 50 (1): 132â150. doi:10.1006/jcss.1995.1013.

^ Siegelmann, Hava (1995). "Computation Beyond the Turing Limit". Journal of Computer and System Sciences. 238 (28): 632â637. Bibcode:1995Sci...268..545S. doi:10.1126/science.268.5210.545. PMIDÂ 17756722. S2CIDÂ 17495161.

^ Ben-Hur, Asa; Horn, David; Siegelmann, Hava; Vapnik, Vladimir (2001). "Support vector clustering". Journal of Machine Learning Research. 2: 51â86.

^ Hofmann, Thomas; SchÃ¶lkopf, Bernhard; Smola, Alexander J. (2008). "Kernel methods in machine learning". The Annals of Statistics. 36 (3): 1171â1220. arXiv:math/0701907. doi:10.1214/009053607000000677. JSTORÂ 25464664.

^ Bennett, James; Lanning, Stan (2007). "The netflix prize" (PDF). Proceedings of KDD Cup and Workshop 2007.

^ Bayes, Thomas (1 January 1763). "An Essay towards solving a Problem in the Doctrine of Chance". Philosophical Transactions. 53: 370â418. doi:10.1098/rstl.1763.0053. JSTORÂ 105741.

^ Legendre, Adrien-Marie (1805). Nouvelles mÃ©thodes pour la dÃ©termination des orbites des comÃ¨tes (in French). Paris: Firmin Didot. p.Â viii. Retrieved 13 June 2016.

^ O'Connor, J J; Robertson, E F. "Pierre-Simon Laplace". School of Mathematics and Statistics, University of St Andrews, Scotland. Retrieved 15 June 2016.

^ Hayes, Brian (2013). "First Links in the Markov Chain". American Scientist. Sigma Xi, The Scientific Research Society. 101 (MarchâApril 2013): 92. doi:10.1511/2013.101.1. Retrieved 15 June 2016. Delving into the text of Alexander Pushkin's novel in verse Eugene Onegin, Markov spent hours sifting through patterns of vowels and consonants. On January 23, 1913, he summarized his findings in an address to the Imperial Academy of Sciences in St. Petersburg. His analysis did not alter the understanding or appreciation of Pushkin's poem, but the technique he developedânow known as a Markov chainâextended the theory of probability in a new direction.

^ McCulloch, Warren S.; Pitts, Walter (1943-12-01). "A logical calculus of the ideas immanent in nervous activity". The Bulletin of Mathematical Biophysics. 5 (4): 115â133. doi:10.1007/BF02478259. ISSNÂ 1522-9602.

^ Turing, Alan (October 1950). "Computing Machinery and Intelligence". Mind. 59 (236): 433â460. doi:10.1093/mind/LIX.236.433. Retrieved 8 June 2016.

^ Crevier 1993, pp.Â 34â35 harvnb error: no target: CITEREFCrevier1993 (help) and Russell & Norvig 2003, p.Â 17 harvnb error: no target: CITEREFRussellNorvig2003 (help)

^ McCarthy, John; Feigenbaum, Ed. "Arthur Samuel: Pioneer in Machine Learning". AI Magazine. No.Â 3. Association for the Advancement of Artificial Intelligence. p.Â 10. Retrieved 5 June 2016.

^ Rosenblatt, Frank (1958). "The perceptron: A probabilistic model for information storage and organization in the brain" (PDF). Psychological Review. 65 (6): 386â408. doi:10.1037/h0042519. PMIDÂ 13602029.

^ Mason, Harding; Stewart, D; Gill, Brendan (6 December 1958). "Rival". The New Yorker. Retrieved 5 June 2016.

^ Child, Oliver (13 March 2016). "Menace: the Machine Educable Noughts And Crosses Engine Read". Chalkdust Magazine. Retrieved 16 Jan 2018.

^ Cohen, Harvey. "The Perceptron". Retrieved 5 June 2016.

^ Colner, Robert (4 March 2016). "A brief history of machine learning". SlideShare. Retrieved 5 June 2016.

^ Seppo Linnainmaa (1970). "The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors." Master's Thesis (in Finnish), Univ. Helsinki, 6â7.

^ Linnainmaa, Seppo (1976). "Taylor expansion of the accumulated rounding error". BIT Numerical Mathematics. 16 (2): 146â160. doi:10.1007/BF01931367. S2CIDÂ 122357351.

^ Griewank, Andreas (2012). "Who Invented the Reverse Mode of Differentiation?". Documenta Matematica, Extra Volume ISMP: 389â400.

^ Griewank, Andreas and Walther, A. Principles and Techniques of Algorithmic Differentiation, Second Edition. SIAM, 2008.

^ Schmidhuber, JÃ¼rgen (2015). "Deep learning in neural networks: An overview". Neural Networks. 61: 85â117. arXiv:1404.7828. Bibcode:2014arXiv1404.7828S. doi:10.1016/j.neunet.2014.09.003. PMIDÂ 25462637. S2CIDÂ 11715509.

^ Schmidhuber, JÃ¼rgen (2015). "Deep Learning (Section on Backpropagation)". Scholarpedia. 10 (11): 32832. Bibcode:2015SchpJ..1032832S. doi:10.4249/scholarpedia.32832.

^ Fukushima, Kunihiko (October 1979). "ä½ç½®ããã«å½±é¿ãããªããã¿ã¼ã³èªè­æ©æ§ã®ç¥çµåè·¯ã®ã¢ãã« --- ããªã³ã°ããã­ã³ ---" [Neural network model for a mechanism of pattern recognition unaffected by shift in position â Neocognitron â]. Trans. IECE (in Japanese). J62-A (10): 658â665.

^ Fukushima, Kunihiko (April 1980). "Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern The Recognitron Unaffected by Shift in Position" (PDF). Biological Cybernetics. 36 (4): 193â202. doi:10.1007/bf00344251. PMIDÂ 7370364. S2CIDÂ 206775608. Retrieved 5 June 2016.

^ Le Cun, Yann. "Deep Learning". CiteSeerXÂ 10.1.1.297.6176. {{cite journal}}: Cite journal requires |journal= (help)

^ Hopfield, John (April 1982). "Neural networks and physical systems with emergent collective computational abilities" (PDF). Proceedings of the National Academy of Sciences of the United States of America. 79 (8): 2554â2558. Bibcode:1982PNAS...79.2554H. doi:10.1073/pnas.79.8.2554. PMCÂ 346238. PMIDÂ 6953413. Retrieved 8 June 2016.

^ Rumelhart, David; Hinton, Geoffrey; Williams, Ronald (9 October 1986). "Learning representations by back-propagating errors" (PDF). Nature. 323 (6088): 533â536. Bibcode:1986Natur.323..533R. doi:10.1038/323533a0. S2CIDÂ 205001834. Retrieved 5 June 2016.

^ Watksin, Christopher (1 May 1989). "Learning from Delayed Rewards" (PDF). {{cite journal}}: Cite journal requires |journal= (help)

^ Markoff, John (29 August 1990). "BUSINESS TECHNOLOGY; What's the Best Answer? It's Survival of the Fittest". New York Times. Retrieved 8 June 2016.

^ Tesauro, Gerald (March 1995). "Temporal Difference Learning and TD-Gammon". Communications of the ACM. 38 (3): 58â68. doi:10.1145/203330.203343. S2CIDÂ 8763243.

^ Ho, Tin Kam (August 1995). "Random Decision Forests" (PDF). Proceedings of the Third International Conference on Document Analysis and Recognition. Montreal, Quebec: IEEE. 1: 278â282. doi:10.1109/ICDAR.1995.598994. ISBNÂ 0-8186-7128-9. Retrieved 5 June 2016.

^ Golge, Eren. "BRIEF HISTORY OF MACHINE LEARNING". A Blog From a Human-engineer-being. Retrieved 5 June 2016.

^ Cortes, Corinna; Vapnik, Vladimir (September 1995). "Support-vector networks". Machine Learning. Kluwer Academic Publishers. 20 (3): 273â297. doi:10.1007/BF00994018. ISSNÂ 0885-6125.

^ Hochreiter, Sepp; Schmidhuber, JÃ¼rgen (1997). "Long Short-Term Memory" (PDF). Neural Computation. 9 (8): 1735â1780. doi:10.1162/neco.1997.9.8.1735. PMIDÂ 9377276. S2CIDÂ 1915014. Archived from the original (PDF) on 2015-05-26.

^ LeCun, Yann; Cortes, Corinna; Burges, Christopher. "THE MNIST DATABASE of handwritten digits". Retrieved 16 June 2016.

^ Collobert, Ronan; Benigo, Samy; Mariethoz, Johnny (30 October 2002). "Torch: a modular machine learning software library" (PDF). Retrieved 5 June 2016. {{cite journal}}: Cite journal requires |journal= (help)

^ "The Netflix Prize Rules". Netflix Prize. Netflix. Archived from the original on 3 March 2012. Retrieved 16 June 2016.

^ Gershgorn, Dave. "ImageNet: the data that spawned the current AI boom â Quartz". qz.com. Retrieved 2018-03-30.

^ Hardy, Quentin (2016-07-18). "Reasons to Believe the A.I. Boom Is Real". The New York Times. ISSNÂ 0362-4331. Retrieved 2018-03-30.

^ "About". Kaggle. Kaggle Inc. Retrieved 16 June 2016.

^ Markoff, John (17 February 2011). "Computer Wins on 'Jeopardy!': Trivial, It's Not". New York Times. p.Â A1. Retrieved 5 June 2016.

^ Le, Quoc V.; Ranzato, Marc'Aurelio; Monga, Rajat; Devin, Matthieu; Corrado, Greg; Chen, Kai; Dean, Jeffrey; Ng, Andrew Y. (2012). "Building high-level features using large scale unsupervised learning" (PDF). Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress. arXiv:1112.6209. Bibcode:2011arXiv1112.6209L.

^ Markoff, John (26 June 2012). "How Many Computers to Identify a Cat? 16,000". New York Times. p.Â B1. Retrieved 5 June 2016.

^ Taigman, Yaniv; Yang, Ming; Ranzato, Marc'Aurelio; Wolf, Lior (24 June 2014). "DeepFace: Closing the Gap to Human-Level Performance in Face Verification". Conference on Computer Vision and Pattern Recognition. Retrieved 8 June 2016.

^ Canini, Kevin; Chandra, Tushar; Ie, Eugene; McFadden, Jim; Goldman, Ken; Gunter, Mike; Harmsen, Jeremiah; LeFevre, Kristen; Lepikhin, Dmitry; Llinares, Tomas Lloret; Mukherjee, Indraneel; Pereira, Fernando; Redstone, Josh; Shaked, Tal; Singer, Yoram. "Sibyl: A system for large scale supervised machine learning" (PDF). Jack Baskin School of Engineering. UC Santa Cruz. Retrieved 8 June 2016.

^ Woodie, Alex (17 July 2014). "Inside Sibyl, Google's Massively Parallel Machine Learning Platform". Datanami. Tabor Communications. Retrieved 8 June 2016.

^ "Google achieves AI 'breakthrough' by beating Go champion". BBC News. BBC. 27 January 2016. Retrieved 5 June 2016.

^ "AlphaGo". Google DeepMind. Google Inc. Retrieved 5 June 2016.


.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vteTimelines of computingComputing
Before 1950
1950â1979
1980s
1990s
2000s
2010s
2020s
Scientific
Women in computing
Computer science
Algorithms
Artificial intelligence
Binary prefixes
Cryptography
Machine learning
Quantum computing
Software
Free and open-source software
Hypertext technology
Operating systems
DOS family
Windows
Linux
Programming languages
Virtualization development
Malware
Internet
Internet conflicts
Web browsers
Web search engines
Notable people
Kathleen Antonelli
John Vincent Atanasoff
Charles Babbage
John Backus
Jean Bartik
George Boole
Vint Cerf
John Cocke
Stephen Cook
Edsger W. Dijkstra
J. Presper Eckert
Adele Goldstine
Lois Haibt
Betty Holberton
Margaret Hamilton
Grace Hopper
David A. Huffman
Bob Kahn
Brian Kernighan
Andrew Koenig
Semyon Korsakov
Nancy Leveson
Ada Lovelace
Donald Knuth
Joseph Kruskal
Douglas McIlroy
Marlyn Meltzer
John von Neumann
KlÃ¡ra DÃ¡n von Neumann
Dennis Ritchie
Guido van Rossum
Frances Spence
Bjarne Stroustrup
Ruth Teitelbaum
Ken Thompson
Linus Torvalds
Alan Turing
Paul Vixie
Larry Wall
Stephen Wolfram
Niklaus Wirth
Steve Wozniak
Konrad Zuse





<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=Timeline_of_machine_learning&oldid=1064772486"
		Categories: Machine learningComputing timelinesHidden categories: CS1: JulianâGregorian uncertaintyCS1 French-language sources (fr)Harv and Sfn no-target errorsCS1: long volume valueCS1 Japanese-language sources (ja)CS1 errors: missing periodicalWikipedia articles in need of updating from August 2021All Wikipedia articles in need of updating
	
