
Title:
Comparison sort
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Type of sorting algorithm that works by comparing pairs of elements
  Sorting a set of unlabelled weights by weight using only a balance scale requires a comparison sort algorithm.
A comparison sort is a type of sorting algorithm that only reads the list elements through a single abstract comparison operation (often a "less than or equal to" operator or a three-way comparison) that determines which of two elements should occur first in the final sorted list. The only requirement is that the operator forms a total preorder over the data, with:

if a â¤ b and b â¤ c then a â¤ c (transitivity)
for all a and b, a â¤ b or b â¤ a (connexity).
It is possible that both a â¤ b and b â¤ a; in this case either may come first in the sorted list. In a stable sort, the input order determines the sorted order in this case.
A metaphor for thinking about comparison sorts is that someone has a set of unlabelled weights and a balance scale. Their goal is to line up the weights in order by their weight without any information except that obtained by placing two weights on the scale and seeing which one is heavier (or if they weigh the same).

Contents

1 Examples
2 Performance limits and advantages of different sorting techniques
3 Alternatives
4 Number of comparisons required to sort a list

4.1 Lower bound for the average number of comparisons
4.2 Sorting a pre-sorted list


5 Notes
6 References



Examples[edit]
  Quicksort in action on a list of numbers. The horizontal lines are pivot values.
Some of the most well-known comparison sorts include:

Quicksort
Heapsort
Shellsort
Merge sort
Introsort
Insertion sort
Selection sort
Bubble sort
Oddâeven sort
Cocktail shaker sort
Cycle sort
Merge-insertion sort
Smoothsort
Timsort
Block sort
Performance limits and advantages of different sorting techniques[edit]
There are fundamental limits on the performance of comparison sorts. A comparison sort must have an average-case lower bound of Î©(n log n) comparison operations,[1] which is known as linearithmic time. This is a consequence of the limited information available through comparisons alone â or, to put it differently, of the vague algebraic structure of totally ordered sets. In this sense, mergesort, heapsort, and introsort are asymptotically optimal in terms of the number of comparisons they must perform, although this metric neglects other operations. Non-comparison sorts (such as the examples discussed below) can achieve O(n) performance by using operations other than comparisons, allowing them to sidestep this lower bound (assuming elements are constant-sized).
Comparison sorts may run faster on some lists; many adaptive sorts such as insertion sort run in O(n) time on an already-sorted or nearly-sorted list. The Î©(n log n) lower bound applies only to the case in which the input list can be in any possible order.
Real-world measures of sorting speed may need to take into account the ability of some algorithms to optimally use relatively fast cached computer memory, or the application may benefit from sorting methods where sorted data begins to appear to the user quickly (and then user's speed of reading will be the limiting factor) as opposed to sorting methods where no output is available until the whole list is sorted.
Despite these limitations, comparison sorts offer the notable practical advantage that control over the comparison function allows sorting of many different datatypes and fine control over how the list is sorted. For example, reversing the result of the comparison function allows the list to be sorted in reverse; and one can sort a list of tuples in lexicographic order by just creating a comparison function that compares each part in sequence:

function tupleCompare((lefta, leftb, leftc), (righta, rightb, rightc))
    if lefta â  righta
        return compare(lefta, righta)
    else if leftb â  rightb
        return compare(leftb, rightb)
    else
        return compare(leftc, rightc)

Balanced ternary notation allows comparisons to be made in one step, whose result will be one of "less than", "greater than" or "equal to".
Comparison sorts generally adapt more easily to complex orders such as the order of floating-point numbers. Additionally, once a comparison function is written, any comparison sort can be used without modification; non-comparison sorts typically require specialized versions for each datatype.
This flexibility, together with the efficiency of the above comparison sorting algorithms on modern computers, has led to widespread preference for comparison sorts in most practical work.

Alternatives[edit]
Some sorting problems admit a strictly faster solution than the Î©(n log n) bound for comparison sorting by using non-comparison sorts; an example is integer sorting, where all keys are integers. When the keys form a small (compared to n) range, counting sort is an example algorithm that runs in linear time. Other integer sorting algorithms, such as radix sort, are not asymptotically faster than comparison sorting, but can be faster in practice.
The problem of sorting pairs of numbers by their sum is not subject to the Î©(nÂ² log n) bound either (the square resulting from the pairing up); the best known algorithm still takes O(nÂ² log n) time, but only O(nÂ²) comparisons.

Number of comparisons required to sort a list[edit]




n

  
    
      
        
          â
          
            
              log
              
                2
              
            
            â¡
            (
            n
            !
            )
          
          â
        
      
    
    {\displaystyle \left\lceil \log _{2}(n!)\right\rceil }
  

Minimum


1

0

0


2

1

1


3

3

3


4

5

5


5

7

7


6

10

10


7

13

13


8

16

16


9

19

19


10

22

22


11

26

26


12

29

30[2][3]


13

33

34[4][5][6]


14

37

38[6]


15

41

42[7][8][9]


16

45
45 or 46[10]


17

49
49 or 50


18

53
53 or 54


19

57

58[9]


20

62

62


21

66

66


22

70

71[6]







n

  
    
      
        
          â
          
            
              log
              
                2
              
            
            â¡
            (
            n
            !
            )
          
          â
        
      
    
    {\displaystyle \left\lceil \log _{2}(n!)\right\rceil }
  


  
    
      
        n
        
          log
          
            2
          
        
        â¡
        n
        â
        
          
            n
            
              ln
              â¡
              2
            
          
        
      
    
    {\displaystyle n\log _{2}n-{\frac {n}{\ln 2}}}
  



10

22

19


100

525

521


1 000

8 530

8 524


10 000

118 459

118 451


100 000

1 516 705

1 516 695


1 000 000

18 488 885

18 488 874


Above: A comparison of the lower bound 
  
    
      
        
          â
          
            
              log
              
                2
              
            
            â¡
            (
            n
            !
            )
          
          â
        
      
    
    {\displaystyle \left\lceil \log _{2}(n!)\right\rceil }
  
 to the actual minimum number of comparisons (from OEIS:Â A036604) required to sort a list of n items (for the worst case). Below: Using Stirling's approximation, this lower bound is well-approximated by 
  
    
      
        n
        
          log
          
            2
          
        
        â¡
        n
        â
        
          
            n
            
              ln
              â¡
              2
            
          
        
      
    
    {\displaystyle n\log _{2}n-{\frac {n}{\ln 2}}}
  
.


The number of comparisons that a comparison sort algorithm requires increases in proportion to 
  
    
      
        n
        log
        â¡
        (
        n
        )
      
    
    {\displaystyle n\log(n)}
  
, where 
  
    
      
        n
      
    
    {\displaystyle n}
  
 is the number of elements to sort.  This bound is asymptotically tight.
Given a list of distinct numbers (we can assume this because this is a worst-case analysis), there are 
  
    
      
        n
      
    
    {\displaystyle n}
  
 factorial permutations exactly one of which is the list in sorted order. The sort algorithm must gain enough information from the comparisons to identify the correct permutation. If the algorithm always completes after at most 
  
    
      
        f
        (
        n
        )
      
    
    {\displaystyle f(n)}
  
 steps, it cannot distinguish more than 
  
    
      
        
          2
          
            f
            (
            n
            )
          
        
      
    
    {\displaystyle 2^{f(n)}}
  
 cases because the keys are distinct and each comparison has only two possible outcomes. Therefore,


  
    
      
        
          2
          
            f
            (
            n
            )
          
        
        â¥
        n
        !
      
    
    {\displaystyle 2^{f(n)}\geq n!}
  
, or equivalently 
  
    
      
        f
        (
        n
        )
        â¥
        
          log
          
            2
          
        
        â¡
        (
        n
        !
        )
        .
      
    
    {\displaystyle f(n)\geq \log _{2}(n!).}
  

By looking at the first 
  
    
      
        n
        
          /
        
        2
      
    
    {\displaystyle n/2}
  
 factors of 
  
    
      
        n
        !
        =
        n
        (
        n
        â
        1
        )
        â¯
        1
      
    
    {\displaystyle n!=n(n-1)\cdots 1}
  
, we obtain


  
    
      
        
          log
          
            2
          
        
        â¡
        (
        n
        !
        )
        â¥
        
          log
          
            2
          
        
        â¡
        
          (
          
            
              (
              
                
                  n
                  2
                
              
              )
            
            
              
                n
                2
              
            
          
          )
        
        =
        
          
            n
            2
          
        
        
          
            
              log
              â¡
              n
            
            
              log
              â¡
              2
            
          
        
        â
        
          
            n
            2
          
        
        =
        Î
        (
        n
        log
        â¡
        n
        )
        .
      
    
    {\displaystyle \log _{2}(n!)\geq \log _{2}\left(\left({\frac {n}{2}}\right)^{\frac {n}{2}}\right)={\frac {n}{2}}{\frac {\log n}{\log 2}}-{\frac {n}{2}}=\Theta (n\log n).}
  


  
    
      
        
          log
          
            2
          
        
        â¡
        (
        n
        !
        )
        =
        Î©
        (
        n
        log
        â¡
        n
        )
        .
      
    
    {\displaystyle \log _{2}(n!)=\Omega (n\log n).}
  

This provides the lower-bound part of the claim. A better bound can be given via Stirling's approximation.
An identical upper bound follows from the existence of the algorithms that attain this bound in the worst case, like heapsort and mergesort.
The above argument provides an absolute, rather than only asymptotic lower bound on the number of comparisons, namely 
  
    
      
        
          â
          
            
              log
              
                2
              
            
            â¡
            (
            n
            !
            )
          
          â
        
      
    
    {\displaystyle \left\lceil \log _{2}(n!)\right\rceil }
  
 comparisons. This lower bound is fairly good (it can be approached within a linear tolerance by a simple merge sort), but it is known to be inexact. For example, 
  
    
      
        
          â
          
            
              log
              
                2
              
            
            â¡
            (
            13
            !
            )
          
          â
        
        =
        33
      
    
    {\displaystyle \left\lceil \log _{2}(13!)\right\rceil =33}
  
, but the minimal number of comparisons to sort 13 elements has been proved to be 34.
Determining the exact number of comparisons needed to sort a given number of entries is a computationally hard problem even for small n, and no simple formula for the solution is known. For some of the few concrete values that have been computed, see OEIS:Â A036604.

Lower bound for the average number of comparisons[edit]
A similar bound applies to the average number of comparisons. Assuming that

all keys are distinct, i.e. every comparison will give either a>b or a<b, and
the input is a random permutation, chosen uniformly from the set of all possible permutations of n elements,
it is impossible to determine which order the input is in with fewer than log2(n!) comparisons on average.
This can be most easily seen using concepts from information theory. The Shannon entropy of such a random permutation is log2(n!) bits. Since a comparison can give only two results, the maximum amount of information it provides is 1 bit. Therefore, after k comparisons the remaining entropy of the permutation, given the results of those comparisons, is at least log2(n!)Â âÂ k bits on average. To perform the sort, complete information is needed, so the remaining entropy must be 0. It follows that k must be at least log2(n!) on average. 
The lower bound derived via information theory is phrased as 'information-theoretic lower bound'. Information-theoretic lower bound is correct but is not necessarily the strongest lower bound. And in some cases, the information-theoretic lower bound of a problem may even be far from the true lower bound. For example, the information-theoretic lower bound of selection is 
  
    
      
        
          â
          
            
              log
              
                2
              
            
            â¡
            (
            n
            )
          
          â
        
      
    
    {\displaystyle \left\lceil \log _{2}(n)\right\rceil }
  
 whereas 
  
    
      
        n
        â
        1
      
    
    {\displaystyle n-1}
  
 comparisons are needed by an adversarial argument. The interplay between information-theoretic lower bound and the true lower bound are much like a real-valued function lower-bounding an integer function. However, this is not exactly correct when the average case is considered.
To unearth what happens while analyzing the average case, the key is that what does 'average' refer to? Averaging over what? With some knowledge of information theory, the information-theoretic lower bound averages over the set of all permutations as a whole. But any computer algorithms (under what are believed currently) must treat each permutation as an individual instance of the problem. Hence, the average lower bound we're searching for is averaged over all individual cases.
To search for the lower bound relating to the non-achievability of computers, we adopt the Decision tree model. Let's rephrase a bit of what our objective is. In the Decision tree model, the lower bound to be shown is the lower bound of the average length of root-to-leaf paths of an 
  
    
      
        n
        !
      
    
    {\displaystyle n!}
  
-leaf binary tree (in which each leaf corresponds to a permutation). It would be convinced to say that a balanced full binary tree achieves the minimum of the average length. With some careful calculations, for a balanced full binary tree with 
  
    
      
        n
        !
      
    
    {\displaystyle n!}
  
 leaves, the average length of root-to-leaf paths is given by 


  
    
      
        
          
            
              (
              2
              n
              !
              â
              
                2
                
                  â
                  
                    log
                    
                      2
                    
                  
                  â¡
                  n
                  !
                  â
                  +
                  1
                
              
              )
              â
              â
              
                log
                
                  2
                
              
              â¡
              n
              !
              â
              +
              (
              
                2
                
                  â
                  
                    log
                    
                      2
                    
                  
                  â¡
                  n
                  !
                  â
                  +
                  1
                
              
              â
              n
              !
              )
              â
              â
              
                log
                
                  2
                
              
              â¡
              n
              !
              â
            
            
              n
              !
            
          
        
      
    
    {\displaystyle {\frac {(2n!-2^{\lfloor \log _{2}n!\rfloor +1})\cdot \lceil \log _{2}n!\rceil +(2^{\lfloor \log _{2}n!\rfloor +1}-n!)\cdot \lfloor \log _{2}n!\rfloor }{n!}}}
  

For example, for nÂ =Â 3, the information-theoretic lower bound for the average case is approximately 2.58, while the average lower bound derived via Decision tree model is 8/3, approximately 2.67.
In the case that multiple items may have the same key, there is no obvious statistical interpretation for the term "average case", so an argument like the above cannot be applied without making specific assumptions about the distribution of keys.

Sorting a pre-sorted list[edit]
.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}Main article: Adaptive sort
If a list is already close to sorted, according to some measure of sortedness, the number of comparisons required to sort it can be smaller. An adaptive sort takes advantage of this "presortedness" and runs more quickly on nearly-sorted inputs, often while still maintaining an 
  
    
      
        O
        (
        n
        log
        â¡
        n
        )
      
    
    {\displaystyle O(n\log n)}
  
 worst case time bound. An example is adaptive heap sort, a sorting algorithm based on Cartesian trees. It takes time 
  
    
      
        O
        (
        n
        log
        â¡
        k
        )
      
    
    {\displaystyle O(n\log k)}
  
, where 
  
    
      
        k
      
    
    {\displaystyle k}
  
 is the average, over all values 
  
    
      
        x
      
    
    {\displaystyle x}
  
 in the sequence, of the number of times the sequence jumps from below 
  
    
      
        x
      
    
    {\displaystyle x}
  
 to above 
  
    
      
        x
      
    
    {\displaystyle x}
  
 or vice versa.[11]

Notes[edit]

^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2009) [1990]. Introduction to Algorithms (3rdÂ ed.). MIT Press and McGraw-Hill. pp.Â 191â193. ISBNÂ 0-262-03384-4.

^ Mark Wells, Applications of a language for computing in combinatorics, Information Processing 65 (Proceedings of the 1965 IFIP Congress), 497â498, 1966.

^ Mark Wells, Elements of Combinatorial Computing, Pergamon Press, Oxford, 1971.

^ Takumi Kasai, Shusaku Sawato, Shigeki Iwata, Thirty four comparisons are required to sort 13 items, LNCS 792, 260-269, 1994.

^ Marcin Peczarski, Sorting 13 elements requires 34 comparisons, LNCS 2461, 785â794, 2002.

^ Jump up to: a b c Marcin Peczarski, New results in minimum-comparison sorting, Algorithmica 40 (2), 133â145, 2004.

^ Marcin Peczarski, Computer assisted research of posets, PhD thesis, University of Warsaw, 2006.

^ Peczarski, Marcin (2007). "The Ford-Johnson algorithm is still unbeaten for less than 47 elements". Inf. Process. Lett. 101 (3): 126â128. doi:10.1016/j.ipl.2006.09.001.

^ Jump up to: a b Cheng, Weiyi; Liu, Xiaoguang; Wang, Gang; Liu, Jing (October 2007). "æå°æ¯è¾æåºé®é¢ä¸­Sï¼15ï¼åSï¼19ï¼çè§£å³" [The results of S(15) and S(19) to minimum-comparison sorting problem]. Journal of Frontiers of Computer Science and Technology (in Chinese). 1 (3): 305â313.

^ Peczarski, Marcin (3 August 2011). "Towards Optimal Sorting of 16 Elements". Acta Universitatis Sapientiae. 4 (2): 215â224. arXiv:1108.0866. Bibcode:2011arXiv1108.0866P.

^ Levcopoulos, Christos; Petersson, Ola (1989), "Heapsort - Adapted for Presorted Files", WADS '89: Proceedings of the Workshop on Algorithms and Data Structures, Lecture Notes in Computer Science, vol.Â 382, London, UK: Springer-Verlag, pp.Â 499â509, doi:10.1007/3-540-51542-9_41.


References[edit]
Donald Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching, Second Edition. Addison-Wesley, 1997. ISBNÂ 0-201-89685-0. Section 5.3.1: Minimum-Comparison Sorting, pp.Â 180â197.
.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}hide.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vteSorting algorithmsTheory
Computational complexity theory
Big O notation
Total order
Lists
Inplacement
Stability
Comparison sort
Adaptive sort
Sorting network
Integer sorting
X + Y sorting
Transdichotomous model
Quantum sort
Exchange sorts
Bubble sort
Cocktail shaker sort
Oddâeven sort
Comb sort
Gnome sort
Proportion extend sort
Quicksort
Slowsort
Stooge sort
Bogosort
Selection sorts
Selection sort
Heapsort
Smoothsort
Cartesian tree sort
Tournament sort
Cycle sort
Weak-heap sort
Insertion sorts
Insertion sort
Shellsort
Splaysort
Tree sort
Library sort
Patience sorting
Merge sorts
Merge sort
Cascade merge sort
Oscillating merge sort
Polyphase merge sort
Distribution sorts
American flag sort
Bead sort
Bucket sort
Burstsort
Counting sort
Interpolation sort
Pigeonhole sort
Proxmap sort
Radix sort
Flashsort
Concurrent sorts
Bitonic sorter
Batcher oddâeven mergesort
Pairwise sorting network
Samplesort
Hybrid sorts
Block merge sort
Kirkpatrick-Reisch sort
Timsort
Introsort
Spreadsort
Merge-insertion sort
Other
Topological sorting
Pre-topological order
Pancake sorting
Spaghetti sort





<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=Comparison_sort&oldid=1061274240"
		Categories: Sorting algorithmsHidden categories: CS1 Chinese-language sources (zh)Articles with short descriptionShort description matches Wikidata
	
