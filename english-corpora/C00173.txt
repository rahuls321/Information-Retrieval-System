
Title:
Redâblack tree
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Self-balancing binary search tree data structure
.mw-parser-output .infobox-subbox{padding:0;border:none;margin:-3px;width:auto;min-width:100%;font-size:100%;clear:none;float:none;background-color:transparent}.mw-parser-output .infobox-3cols-child{margin:auto}.mw-parser-output .infobox .navbar{font-size:100%}body.skin-minerva .mw-parser-output .infobox-header,body.skin-minerva .mw-parser-output .infobox-subheader,body.skin-minerva .mw-parser-output .infobox-above,body.skin-minerva .mw-parser-output .infobox-title,body.skin-minerva .mw-parser-output .infobox-image,body.skin-minerva .mw-parser-output .infobox-full-data,body.skin-minerva .mw-parser-output .infobox-below{text-align:center}Redâblack treeTypeTreeInvented1972Invented byRudolf BayerComplexities in big O notationSpace complexitySpace


  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
  

Time complexityFunction

Amortized
Worst CaseSearch


  
    
      
        O
        (
        log
        â¡
        n
        )
      
    
    {\displaystyle O(\log n)}
  
[1]

  
    
      
        O
        (
        log
        â¡
        n
        )
      
    
    {\displaystyle O(\log n)}
  
[1]Insert


  
    
      
        O
        (
        1
        )
      
    
    {\displaystyle O(1)}
  
[2]

  
    
      
        O
        (
        log
        â¡
        n
        )
      
    
    {\displaystyle O(\log n)}
  
[1]Delete


  
    
      
        O
        (
        1
        )
      
    
    {\displaystyle O(1)}
  
[2]

  
    
      
        O
        (
        log
        â¡
        n
        )
      
    
    {\displaystyle O(\log n)}
  
[1]
In computer science, a redâblack tree is a kind of self-balancing binary search tree. Each node stores an extra bit representing "color" ("red" or "black"), used to ensure that the tree remains balanced during insertions and deletions.[3]
When the tree is modified, the new tree is rearranged and "repainted" to restore the coloring properties that constrain how unbalanced the tree can become in the worst case. The properties are designed such that this rearranging and recoloring can be performed efficiently.
The re-balancing is not perfect, but guarantees searching in 
  
    
      
        O
        (
        log
        â¡
        n
        )
      
    
    {\displaystyle O(\log n)}
  
 time, where 
  
    
      
        n
      
    
    {\displaystyle n}
  
 is the number of entries. The insert and delete operations, along with the tree rearrangement and recoloring, are also performed in 
  
    
      
        O
        (
        log
        â¡
        n
        )
      
    
    {\displaystyle O(\log n)}
  
 time.[4]
Tracking the color of each node requires only one bit of information per node because there are only two colors. The tree does not contain any other data specific to its being a redâblack tree, so its memory footprint is almost identical to that of a classic (uncolored) binary search tree. In many cases, the additional bit of information can be stored at no additional memory cost.

.mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}Contents

1 History
2 Terminology
3 Properties
4 Analogy to B-trees of order 4
5 Applications and related data structures
6 Operations

6.1 Notes to the sample code and diagrams of insertion and removal
6.2 Insertion

6.2.1 Notes to the insert diagrams
6.2.2 Insert case 1
6.2.3 Insert case 2
6.2.4 Insert case 3
6.2.5 Insert case 4
6.2.6 Insert case 5
6.2.7 Insert case 6


6.3 Removal: simple cases
6.4 Removal of a black non-root leaf

6.4.1 Notes to the delete diagrams
6.4.2 Delete case 1
6.4.3 Delete case 2
6.4.4 Delete case 3
6.4.5 Delete case 4
6.4.6 Delete case 5
6.4.7 Delete case 6




7 Proof of bounds
8 Set operations and bulk operations
9 Parallel algorithms

9.1 Parallel bulk operations

9.1.1 Join-based

9.1.1.1 Execution time
9.1.1.2 Work


9.1.2 Pipelining

9.1.2.1 Execution time
9.1.2.2 Work






10 Popular culture
11 See also
12 References and notes
13 Further reading
14 External links



History[edit]
In 1972, Rudolf Bayer[5] invented a data structure that was a special order-4 case of a B-tree. These trees maintained all paths from root to leaf with the same number of nodes, creating perfectly balanced trees. However, they were not binary search trees. Bayer called them a "symmetric binary B-tree" in his paper and later they became popular as 2â3â4 trees or just 2â4 trees.[6]
In a 1978 paper, "A Dichromatic Framework for Balanced Trees",[7] Leonidas J. Guibas and Robert Sedgewick derived the redâblack tree from the symmetric binary B-tree.[8] The color "red" was chosen because it was the best-looking color produced by the color laser printer available to the authors while working at Xerox PARC.[9] Another response from Guibas states that it was because of the red and black pens available to them to draw the trees.[10]
In 1993, Arne Andersson introduced the idea of a right leaning tree to simplify insert and delete operations.[11]
In 1999, Chris Okasaki showed how to make the insert operation purely functional. Its balance function needed to take care of only 4 unbalanced cases and one default balanced case.[12]
The original algorithm used 8 unbalanced cases, but Cormen et al. (2001) reduced that to 6 unbalanced cases.[3] Sedgewick showed that the insert operation can be implemented in just 46 lines of Java code.[13][14]
In 2008, Sedgewick proposed the left-leaning redâblack tree, leveraging Anderssonâs idea that simplified the insert and delete operations. Sedgewick originally allowed nodes whose two children are red, making his trees more like 2â3â4 trees, but later this restriction was added, making new trees more like 2â3 trees. Sedgewick implemented the insert algorithm in just 33 lines, significantly shortening his original 46 lines of code.[15][16]

Terminology[edit]
.mw-parser-output .tmulti .thumbinner{display:flex;flex-direction:column}.mw-parser-output .tmulti .trow{display:flex;flex-direction:row;clear:left;flex-wrap:wrap;width:100%;box-sizing:border-box}.mw-parser-output .tmulti .tsingle{margin:1px;float:left}.mw-parser-output .tmulti .theader{clear:both;font-weight:bold;text-align:center;align-self:center;background-color:transparent;width:100%}.mw-parser-output .tmulti .thumbcaption{background-color:transparent}.mw-parser-output .tmulti .text-align-left{text-align:left}.mw-parser-output .tmulti .text-align-right{text-align:right}.mw-parser-output .tmulti .text-align-center{text-align:center}@media all and (max-width:720px){.mw-parser-output .tmulti .thumbinner{width:100%!important;box-sizing:border-box;max-width:none!important;align-items:center}.mw-parser-output .tmulti .trow{justify-content:center}.mw-parser-output .tmulti .tsingle{float:none!important;max-width:100%!important;box-sizing:border-box;text-align:center}.mw-parser-output .tmulti .tsingle .thumbcaption{text-align:left}.mw-parser-output .tmulti .trow>.thumbcaption{text-align:center}}Example of a redâblack treeFigure 1: ... with explicit NIL leavesFigure 2: ... with implicit left and right docking points
A redâblack tree is a special type of binary search tree, used in computer science to organise pieces of comparable data, such as text fragments or numbers (as e.g. the numbers in figures 1 and 2). The nodes carrying keys and/or data are frequently called "internal nodes", but in order to make this very specific they are also called non-NIL nodes in this article.
The leaf nodes of redâblack trees (âNILâ in figure 1) do not contain keys or data. These "leaves" need not be explicit individuals in computer memory: a NULL pointer can âas in all binary tree data structuresâ encode the fact that there is no child node at this position in the (parent) node. Nevertheless, by their position in the tree, these objects are in relation to other nodes that is relevant to the RB-structure, it may have parent, sibling (i.e., the other child of the parent), uncle, even nephew node; and may be childâbut never parent of another node.
It is not really necessary to attribute a "color" to these end-of-path objects, because the condition "is NIL or BLACK" is implied by the condition "is NIL" (see also this remark).
Figure 2 shows the conceptually same redâblack tree without these NIL leaves. In order to arrive at the same notion of a path, one has to notice that e.g. 3 paths run through the node 1, namely a path through 1left plus 2 additional paths through 1right, namely the paths through 6left and 6right. This way, these ends of the paths are also docking points for new nodes to be inserted, fully equivalent to the NIL leaves of figure 1.
On the other hand, in order to save a marginal amount of execution time, these (possibly many) NIL leaves may be implemented as pointers to one unique (and black) sentinel node (instead of pointers of value NULL).
As a conclusion, the fact that a child does not exist (is not a true node, does not contain data) can in all occurrences be specified by the very same NULL pointer or as the very same pointer to a sentinel node. Throughout this article, either choice is called NIL node and has the constant value NIL.
The black depth of a node is defined as the number of black nodes from the root to that node (i.e. the number of black ancestors). The black height of a redâblack tree is the number of black nodes in any path from the root to the leaves, which, by requirement 4, is constant (alternatively, it could be defined as the black depth of any leaf node).[17]:â154â165â
The black height of a node is the black height of the subtree rooted by it. In this article, the black height of a NIL node shall be set to 0, because its subtree is empty as suggested by figure 2, and its tree height is also 0.

Properties[edit]
In addition to the requirements imposed on a binary search tree the following must be satisfied by a redâblack tree:[18]

Each node is either red or black.
All NIL nodes (figure 1) are considered black.
A red node does not have a red child.
Every path from a given node to any of its descendant NIL nodes goes through the same number of black nodes.
Some authors, e.g. Cormen & al.,[18] claim "the root is black" as fifth requirement; but not Mehlhorn & Sanders[17] or Sedgewick & Wayne.[16]:â432â447â Since the root can always be changed from red to black, this rule has little effect on analysis.
This article also omits it, because it slightly disturbs the recursive algorithms and proofs.
As an example, every perfect binary tree that consists only of black nodes is a redâblack tree.
The read-only operations, such as search or tree traversal, do not affect any of the requirements. On the other hand, the modifying operations insert and delete easily maintain requirements 1 and 2, but with respect to the other requirements some extra effort has to be taken, in order to avoid the introduction of a violation of requirement 3, called a red-violation, or of requirement 4, called a black-violation.
The requirements enforce a critical property of redâblack trees: the path from the root to the farthest leaf is no more than twice as long as the path from the root to the nearest leaf. The result is that the tree is height-balanced. Since operations such as inserting, deleting, and finding values require worst-case time proportional to the height 
  
    
      
        h
      
    
    {\displaystyle h}
  
 of the tree, this upper bound on the height allows redâblack trees to be efficient in the worst case, namely logarithmic in the number 
  
    
      
        n
      
    
    {\displaystyle n}
  
 of entries, i.e. 
  
    
      
        h
        â
        O
        (
        log
        â¡
        n
        )
      
    
    {\displaystyle h\in O(\log n)}
  
, which is not the case for ordinary binary search trees. For a mathematical proof see section Proof of bounds.
Redâblack trees, like all binary search trees, allow quite efficient sequential access (e.g. in-order traversal, that is: in the order LeftâRootâRight) of their elements. But they support also asymptotically optimal direct access via a traversal from root to leaf, resulting in 
  
    
      
        O
        (
        log
        â¡
        n
        )
      
    
    {\displaystyle O(\log n)}
  
 search time.

Analogy to B-trees of order 4[edit]
  Figure 3: The same redâblack tree as in the example above, now as a B-tree
A redâblack tree is similar in structure to a B-tree of order[19] 4, where each node can contain between 1 and 3 values and (accordingly) between 2 and 4 child pointers. In such a B-tree, each node will contain only one value matching the value in a black node of the redâblack tree, with an optional value before and/or after it in the same node, both matching an equivalent red node of the redâblack tree.
One way to see this equivalence is to "move up" the red nodes in a graphical representation of the redâblack tree, so that they align horizontally with their parent black node, by creating together a horizontal cluster. In the B-tree, or in the modified graphical representation of the redâblack tree, all leaf nodes are at the same depth.
The redâblack tree is then structurally equivalent to a B-tree of order 4, with a minimum fill factor of 33% of values per cluster with a maximum capacity of 3 values.
This B-tree type is still more general than a redâblack tree though, as it allows ambiguity in a redâblack tree conversionâmultiple redâblack trees can be produced from an equivalent B-tree of order 4 (see figure 3). If a B-tree cluster contains only 1 value, it is the minimum, black, and has two child pointers. If a cluster contains 3 values, then the central value will be black and each value stored on its sides will be red. If the cluster contains two values, however, either one can become the black node in the redâblack tree (and the other one will be red).
So the order-4 B-tree does not maintain which of the values contained in each cluster is the root black tree for the whole cluster and the parent of the other values in the same cluster. Despite this, the operations on redâblack trees are more economical in time because you donât have to maintain the vector of values.[20] It may be costly if values are stored directly in each node rather than being stored by reference. B-tree nodes, however, are more economical in space because you donât need to store the color attribute for each node. Instead, you have to know which slot in the cluster vector is used. If values are stored by reference, e.g. objects, null references can be used and so the cluster can be represented by a vector containing 3 slots for value pointers plus 4 slots for child references in the tree. In that case, the B-tree can be more compact in memory, improving data locality.
The same analogy can be made with B-trees with larger orders that can be structurally equivalent to a colored binary tree: you just need more colors. Suppose that you add blue, then the blueâredâblack tree defined like redâblack trees but with the additional constraint that no two successive nodes in the hierarchy will be blue and all blue nodes will be children of a red node, then it becomes equivalent to a B-tree whose clusters will have at most 7 values in the following colors: blue, red, blue, black, blue, red, blue (For each cluster, there will be at most 1 black node, 2 red nodes, and 4 blue nodes).
For moderate volumes of values, insertions and deletions in a colored binary tree are faster compared to B-trees because colored trees donât attempt to maximise the fill factor of each horizontal cluster of nodes (only the minimum fill factor is guaranteed in colored binary trees, limiting the number of splits or junctions of clusters). B-trees will be faster for performing rotations (because rotations will frequently occur within the same cluster rather than with multiple separate nodes in a colored binary tree). For storing large volumes, however, B-trees will be much faster as they will be more compact by grouping several children in the same cluster where they can be accessed locally.
All optimizations possible in B-trees to increase the average fill factors of clusters are possible in the equivalent multicolored binary tree. Notably, maximizing the average fill factor in a structurally equivalent B-tree is the same as reducing the total height of the multicolored tree, by increasing the number of non-black nodes. The worst case occurs when all nodes in a colored binary tree are black, the best case occurs when only a third of them are black (and the other two thirds are red nodes).

Applications and related data structures[edit]
Redâblack trees offer worst-case guarantees for insertion time, deletion time, and search time. Not only does this make them valuable in time-sensitive applications such as real-time applications, but it makes them valuable building blocks in other data structures that provide worst-case guarantees; for example, many data structures used in computational geometry can be based on redâblack trees, and the Completely Fair Scheduler used in current Linux kernels and epoll system call implementation[21] uses redâblack trees.
The AVL tree is another structure supporting 
  
    
      
        O
        (
        log
        â¡
        n
        )
      
    
    {\displaystyle O(\log n)}
  
 search, insertion, and removal. AVL trees can be colored redâblack, thus are a subset of RB trees. Worst-case height is 0.720 times the worst-case height of RB trees, so AVL trees are more rigidly balanced. The performance measurements of Ben Pfaff with realistic test cases in 79 runs find AVL to RB ratios between 0.677 and 1.077, median at 0.947, and geometric mean 0.910.[22] WAVL trees have a performance in between those two.
Redâblack trees are also particularly valuable in functional programming, where they are one of the most common persistent data structures, used to construct associative arrays and sets that can retain previous versions after mutations. The persistent version of redâblack trees requires 
  
    
      
        O
        (
        log
        â¡
        n
        )
      
    
    {\displaystyle O(\log n)}
  
 space for each insertion or deletion, in addition to time.
For every 2â4 tree, there are corresponding redâblack trees with data elements in the same order. The insertion and deletion operations on 2â4 trees are also equivalent to color-flipping and rotations in redâblack trees. This makes 2â4 trees an important tool for understanding the logic behind redâblack trees, and this is why many introductory algorithm texts introduce 2â4 trees just before redâblack trees, even though 2â4 trees are not often used in practice.
In 2008, Sedgewick introduced a simpler version of the redâblack tree called the left-leaning redâblack tree[23] by eliminating a previously unspecified degree of freedom in the implementation. The LLRB maintains an additional invariant that all red links must lean left except during inserts and deletes. Redâblack trees can be made isometric to either 2â3 trees,[24] or 2â4 trees,[23] for any sequence of operations. The 2â4 tree isometry was described in 1978 by Sedgewick.[7] With 2â4 trees, the isometry is resolved by a "color flip," corresponding to a split, in which the red color of two children nodes leaves the children and moves to the parent node.
The original description of the tango tree, a type of tree optimised for fast searches, specifically uses redâblack trees as part of its data structure.[25]
As of Java 8, the HashMap has been modified such that instead of using a LinkedList to store different elements with colliding hashcodes, a redâblack tree is used. This results in the improvement of time complexity of searching such an element from 
  
    
      
        O
        (
        m
        )
      
    
    {\displaystyle O(m)}
  
 to 
  
    
      
        O
        (
        log
        â¡
        m
        )
      
    
    {\displaystyle O(\log m)}
  
 where 
  
    
      
        m
      
    
    {\displaystyle m}
  
 is the number of elements with colliding hashcodes.[26]

Operations[edit]
The read-only operations, such as search or tree traversal, on a redâblack tree require no modification from those used for binary search trees, because every redâblack tree is a special case of a simple binary search tree. However, the immediate result of an insertion or removal may violate the properties of a redâblack tree, the restoration of which is called rebalancing so that redâblack trees become self-balancing.
It requires in the worst case a small number, 
  
    
      
        O
        (
        log
        â¡
        n
        )
      
    
    {\displaystyle O(\log n)}
  
 in Big O notation, where 
  
    
      
        n
      
    
    {\displaystyle n}
  
 is the number of objects in the tree, on average or amortized 
  
    
      
        O
        (
        1
        )
      
    
    {\displaystyle O(1)}
  
, a constant number,[27]:â310â [17]:â158â of color changes (which are very quick in practice); and no more than three tree rotations[28] (two for insertion).
If the example implementation below is not suitable, other implementations with explanations may be found in Ben Pfaffâs[29] annotated C library GNU libavl (v2.0.3 as of June 2019).
The details of the insert and removal operations will be demonstrated with example C++ code. The example code uses the type definitions and macros below, as well as the helper function for rotations:

// Basic type definitions:

enum color_t { BLACK, RED };

struct RBnode {     // node of redâblack tree
  RBnode* parent;   // == NULL if root of the tree
  RBnode* child[2]; // == NIL if child is empty
    // Index is:
    //   LEFT Â := 0, if (key < parent->key)
    //   RIGHTÂ := 1, if (key > parent->key)
  enum color_t color;
  int key;
};

#define NIL   NULL // null pointer  or  pointer to sentinel node
#define LEFT  0
#define RIGHT 1
#define left  child[LEFT]
#define right child[RIGHT]

struct RBtree { // redâblack tree
  RBnode* root; // == NIL if tree is empty
};

// Get the child direction (â { LEFT, RIGHT })
//   of the non-root non-NIL  RBnode* N:
#define childDir(N) ( N == (N->parent)->rightÂ ? RIGHTÂ : LEFT )


RBnode* RotateDirRoot(
    RBtree* T,   // redâblack tree
    RBnode* P,   // root of subtree (may be the root of T)
    int dir) {   // dir â { LEFT, RIGHT }
  RBnode* G = P->parent;
  RBnode* S = P->child[1-dir];
  RBnode* C;
  assert(S != NIL); // pointer to true node required
  C = S->child[dir];
  P->child[1-dir] = C; if (C != NIL) C->parent = P;
  S->child[  dir] = P; P->parent = S;
  S->parent = G;
  if (G != NULL)
    G->child[ P == G->right ? RIGHT : LEFT ] = S;
  else
    T->root = S;
  return S; // new root of subtree
}

#define RotateDir(N,dir) RotateDirRoot(T,N,dir)
#define RotateLeft(N)    RotateDirRoot(T,N,LEFT)
#define RotateRight(N)   RotateDirRoot(T,N,RIGHT)



Notes to the sample code and diagrams of insertion and removal[edit]
The proposal breaks down both, insertion and removal (not mentioning some very simple cases), into six constellations of nodes, edges and colors, which are called cases. The code repairing (rebalancing) a case sometimes uses code of a subsequent case. The proposal contains for both, insertion and removal, exactly one case that advances one black level closer to the root and loops, the other five cases rebalance the tree of their own. The more complicated cases are pictured in a diagram.

The variable N denotes the current node, which is labeled N in the diagrams.
 symbolises a red node and  a (non-NIL) black node (of black height â¥ 1), a (non-NIL) node  may be red or black, but the same color throughout the same diagram. True NIL nodes are not represented in the diagrams.
A diagram contains three columns and two to four actions.
The left column shows the first iteration, the right column the higher iterations, the middle column shows the segmentation of a case into its different actions.[30]
The action "entry" shows the constellation of nodes with their colors which defines a case and mostly violates some of the requirements.A blue border rings the current node N and the other nodes are labeled according to their relation to N.
If a rotation is considered useful, this is pictured in the next action, which is labeled "rotation".
If some recoloring is considered useful, this is pictured in the next action, which is labeled "color".[31]
If there is still some need to repair, the constellation satisfies the loop invariant of insert or delete with the newly assigned current node N, which then again carries a blue ring. Possibly some other nodes are also newly assigned relative to N. This action is labeled "reassign".The subsequent actions reuse code of the other cases that may also be an iteration one black level closer to the root.
A possibly numbered triangle with a black circle atop  represents a redâblack subtree (connected to its parent according to requirement 3) with a black height equal to the iteration level minus one, i.e. zero in the first iteration. Its root may be red or black.A possibly numbered triangle  represents a redâblack subtree with a black height one less, i.e. its parent has black height zero in the second iteration.


Remark
For simplicity, the sample code uses the disjunction:
U == NIL || U->color == BLACK // considered black
and the conjunction:
UÂ != NIL && U->color == RED Â  // not considered black
Thereby, it has to be kept in mind that both statements are not evaluated in total, if U == NIL. Then in both cases U->color is not touched (see Short-circuit evaluation).(The comment considered black is in accordance with requirement 2.)
The related if-statements have to occur far less frequently if the proposal[30] is realised.
Insertion[edit]
Insertion begins by placing the new (non-NIL) node, say N, at the position in the binary search tree of a NIL node whose in-order predecessorâs key compares less than the new nodeâs key, which in turn compares less than the key of its in-order successor.
(Frequently, this positioning is the result of a search within the tree immediately preceding the insert operation and consists of a node P together with a direction dir with P->child[dir] == NIL.)
The newly inserted node is temporarily colored red so that all paths contain the same number of black nodes as before.
But if its parent, say P, is also red then this action introduces a red-violation.

void RBinsert1(
  RBtree* T,         // -> redâblack tree
  struct RBnode* N,  // -> node to be inserted
  struct RBnode* P,  // -> parent node of N ( may be NULL )
  byte dir)          // side ( LEFT or RIGHT ) of P where to insert N
{
  struct RBnode* G;  // -> parent node of P
  struct RBnode* U;  // -> uncle of N

  N->color = RED;
  N->left  = NIL;
  N->right = NIL;
  N->parent = P;
  if (P == NULL) {   // There is no parent
    T->root = N;     // N is the new root of the tree T.
    return; // insertion complete
  }
  P->child[dir] = N; // insert N as dir-child of P
  // start of the (do while)-loop:
  do {

The rebalancing loop has the following invariant:

The current node N is  (red) at the beginning of each iteration.
Requirement 3 is satisfied for all pairs nodeâparent with the possible exception NâP when P is also red (a red-violation at N).
All other properties (including requirement 4) are satisfied throughout the tree.
Notes to the insert diagrams[edit]



before
caseâ
rota-tion
assig-nment
after
ânext
Îh


P
G
U
x
P
G
U
x


â



I3



â







I1



â




â


I4




â







I2

N:=G
?

?
2





i
I5
Pâ¶N
N:=P



o
I6
0





o
I6
Pâ·G





â



Insertion: This synopsis shows in its before column, that allÂ  Â  Â  Â  Â  Â  Â  Â  possible cases[32] of constellations are covered.

In the diagrams, P is used for Nâs parent, G for the grandparent, and U will denote Nâs uncle.
The diagrams show the parent node P as the left child of its parent G even though it is possible for P to be on either side. The sample code covers both possibilities by means of the side variable dir.
N is the insertion node, but as the operation proceeds also other nodes may become current (see case I2).
The diagrams show the cases where P is red as well, the red-violation.
The column x indicates the change in child direction, i.e. o (for "outer") means that P and N are both left or both right children, whereas i (for "inner") means that the child direction changes from Pâs to Nâs.
The column group before defines the case, whose name is given in the column case. Thereby possible values in cells left empty are ignored. So in case I2 the sample code covers both possibilities of child directions of N, although the corresponding diagram shows only one.
The rows in the synopsis are ordered such that the coverage of all possible RB cases is easily comprehensible.
The column rotation indicates whether a rotation contributes to the rebalancing.
The column assignment shows an assignment of N before entering a subsequent step. This possibly induces a reassignment of the other nodes P, G, U as well.
If something has been changed by the case, this is shown in the column group after.
An arrow â in column next signifies that the rebalancing is complete with this step. If the column after determines exactly one case, this case is given as the subsequent one, otherwise there are question marks.
The loop is contained in the sections "Insert case 1" and "Insert case 2", where in case I2 the problem of rebalancing is escalated 
  
    
      
        Î
        h
        =
        2
      
    
    {\displaystyle \Delta h=2}
  
 levels higher in the tree, in that the grandfather G becomes the new current node N. So it takes maximally 
  
    
      
        
          
            
              h
              2
            
          
        
      
    
    {\displaystyle {\tfrac {h}{2}}}
  
 iterations to repair the tree (where 
  
    
      
        h
      
    
    {\displaystyle h}
  
 is the height of the tree). Because the probability of escalation decreases exponentially with each iteration the total rebalancing cost is constant on average, indeed amortized constant.
From the body of the loop, case I1 exits by itself and there are exiting branches to cases I4, I6, I5 + I6, and I3.
Rotations occur in cases I6 and I5 + I6 â outside the loop. Therefore, at most two rotations occur in total.
Insert case 1[edit]
The current nodeâs parent P is black, so requirement 3 holds. Requirement 4 holds as well according to the loop invariant.

    if (P->color == BLACK) {
      // Case_I1 (P black):
      return; // insertion complete
    }
    // From now on P is red.
    if ((G = P->parent) == NULL) 
      goto Case_I4; // P red and root
    // else: P red and G!=NULL.
    dir = childDir(P); // the side of parent G on which node P is located
    U = G->child[1-dir]; // uncle
    if (U == NIL || U->color == BLACK) // considered black
      goto Case_I56; // P red && U black

first iterationhigher iterationInsert case 2
Insert case 2[edit]
If both the parent P and the uncle U are red, then both of them can be repainted black and the grandparent G becomes red for maintaining requirement 4. Since any path through the parent or uncle must pass through the grandparent, the number of black nodes on these paths has not changed. However, the grandparent G may now violate requirement 3, if it has a red parent. After relabeling G to N the loop invariant is fulfilled so that the rebalancing can be iterated on one black level (= 2 tree levels) higher.

    // Case_I2 (P+U red):
    P->color = BLACK;
    U->color = BLACK;
    G->color = RED;
    N = G; // new current node
    // iterate 1 black level higher
    //   (= 2 tree levels)
  } while ((P = N->parent) != NULL);
  // end of the (do while)-loop

Insert case 3[edit]
Insert case 2 has been executed for 
  
    
      
        
          
            
              
                h
                â
                1
              
              2
            
          
        
      
    
    {\displaystyle {\tfrac {h-1}{2}}}
  
 times and the total height of the tree has increased by 1, now being 
  
    
      
        h
        Â 
        .
      
    
    {\displaystyle h~.}
  

The current node N is the (red) root of the tree, and all RB-properties are satisfied.

  // Leaving the (do while)-loop (after having fallen through from Case_I2).
  // Case_I3: N is the root and red.
  return; // insertion complete

Insert case 4[edit]
The parent P is red and the root.
Because N is also red, requirement 3 is violated. But after switching Pâs color the tree is in RB-shape.
The black height of the tree increases by 1.

Case_I4: // P is the root and red:
  P->color = BLACK;
  return; // insertion complete

first iterationhigher iterationInsert case 5
Insert case 5[edit]
The parent P is red but the uncle U is black. The ultimate goal is to rotate the parent node P to the grandparent position, but this will not work if N is an "inner" grandchild of G (i.e., if N is the left child of the right child of G or the right child of the left child of G). A dir-rotation at P switches the roles of the current node N and its parent P. The rotation adds paths through N (those in the subtree labeled 2, see diagram) and removes paths through P (those in the subtree labeled 4). But both P and N are red, so requirement 4 is preserved. Requirement 3 is restored in case 6.

Case_I56: // P red && U black:
  if (N == P->child[1-dir])
  { // Case_I5 (P red && U black && N inner grandchild of G):
    RotateDir(P,dir); // P is never the root
    N = P; // new current node
    P = G->child[dir]; // new parent of N
    // fall through to Case_I6
  }

first iterationhigher iterationInsert case 6
Insert case 6[edit]
The current node N is now certain to be an "outer" grandchild of G (left of left child or right of right child). Now (1-dir)-rotate at G, putting P in place of G and making P the parent of N and G. G is black and its former child P is red, since requirement 3 was violated. After switching the colors of P and G the resulting tree satisfies requirement 3. Requirement 4 also remains satisfied, since all paths that went through the black G now go through the black P.

  // Case_I6 (P red && U black && N outer grandchild of G):
  RotateDirRoot(T,G,1-dir); // G may be the root
  P->color = BLACK;
  G->color = RED;
  return; // insertion complete
} // end of RBinsert1

Because the algorithm transforms the input without using an auxiliary data structure and using only a small amount of extra storage space for auxiliary variables it is in-place.

Removal: simple cases[edit]
The label N denotes the current node that at entry is the node to be deleted.
If N is the root that does not have a non-NIL child, it is replaced by a NIL node, after which the tree is emptyâand in RB-shape.
If N has two non-NIL children, an additional navigation to either the maximum element in its left subtree (which is the in-order predecessor) or the minimum element in its right subtree (which is the in-order successor) finds a node with no other node in between (as shown here). This "replacement node", say R, has â as the maximal or minimal element of a subtree â at most one non-NIL child. In order to keep the software completely independent of the node structure as defined by the user, all redâblack tree data related with N and R, i.e. the color of and the pointers to and from the two nodes, are exchanged. (The modified redâblack tree is the same as before with the exception of the reversed order between N and R, an issue which immediately is resolved by the removal of N.) Now N has at most one non-NIL child. 
If N has exactly one non-NIL child, it must be a red child, because if it were a black one then requirement 4 would force a second black non-NIL child.
If N is a red node, it cannot have a non-NIL child, because this would have to be black by requirement 3. Furthermore, it cannot have exactly one black child as argued just above. As a consequence, the red node N is without any child and can simply be removed.
If N is a black node, it may have a red child or no non-NIL child at all.
If N has a red child, it is simply replaced with this child after painting the latter black.

Removal of a black non-root leaf[edit]
The complex case is when N is not the root, colored black and has only NIL children (â no proper child).
In the first iteration, N is replaced by NIL.

void RBdelete2(
  RBtree* T,         // -> redâblack tree
  struct RBnode* N)  // -> node to be deleted
 {
  struct RBnode* P = N->parent;  // -> parent node of N
  byte dir;          // side of P on which N is located (â { LEFT, RIGHT })
  struct RBnode* S;  // -> sibling of N
  struct RBnode* C;  // -> close   nephew
  struct RBnode* D;  // -> distant nephew

  // PÂ != NULL, since N is not the root.
  dir = childDir(N); // side of parent P on which the node N is located
  // Replace N at its parent P by NIL:
  P->child[dir] = NIL;
  goto Start_D;      // jump into the loop

  // start of the (do while)-loop:
  do {
    dir = childDir(N);   // side of parent P on which node N is located
Start_D:
    S = P->child[1-dir]; // sibling of N (has black height >= 1)
    D = S->child[1-dir]; // distant nephew
    C = S->child[  dir]; // close   nephew
    if (S->color == RED)
      goto Case_D3;                  // S red ===> P+C+D black
    // S is black:
    if (D != NIL && D->color == RED) // not considered black
      goto Case_D6;                  // D red && S black
    if (C != NIL && C->color == RED) // not considered black
      goto Case_D5;                  // C red && S+D black
    // Here both nephews are == NIL (first iteration) or black (later).
    if (P->color == RED)
      goto Case_D4;                  // P red && C+S+D black

The rebalancing loop has the following invariant:

At the beginning of each iteration the black height of N equals the iteration number minus one, which means that in the first iteration it is zero and that N is a true black node  in higher iterations.
The number of black nodes on the paths through N is one less than before the deletion, whereas it is unchanged on all other paths, so that there is a black-violation at P if other paths exist.
All other properties (including requirement 3) are satisfied throughout the tree.
Notes to the delete diagrams[edit]



before
caseâ
rota-tion
assig-nment
after
ânext
Îh


P
C
S
D
P
C
S
D


â



D2



â







D3
Pâ¶S
N:=N

?

?
?
0






D1

N:=P
?

?
1






D4






â







D5
Câ·S
N:=N




D6
0






D6
Pâ¶S





â



Deletion: This synopsis shows in its before column, that allÂ  Â  Â  Â  Â  Â  Â  Â  possible cases[32] of color constellations are covered.

In the diagrams below, P is used for Nâs parent, S for the sibling of N, C (meaning close nephew) for Sâs child in the same direction as N, and D (meaning distant nephew) for Sâs other child (S cannot be a NIL node in the first iteration, because it has to have black height one, which was the black height of N before its deletion, but C and D may be NIL nodes).
The diagrams show the current node N as the left child of its parent P even though it is possible for N to be on either side. The code samples cover both possibilities by means of the side variable dir.
At the beginning (in the first iteration) of removal, N is the NIL node replacing the node to be deleted. Because its location in parentâs node is the only thing of importance, it is symbolised by  (meaning: the current node N is a NIL node and left child) in the left column of the delete diagrams. As the operation proceeds also proper nodes (of black height â¥ 1) may become current (see e.g. case D1).
By counting the black bullets ( and ) in a delete diagram it can be observed that the paths through N have one bullet less than the other paths. This means a black-violation at Pâif it exists.
The color constellation in column group before defines the case, whose name is given in the column case. Thereby possible values in cells left empty are ignored.
The rows in the synopsis are ordered such that the coverage of all possible RB cases is easily comprehensible.
The column rotation indicates whether a rotation contributes to the rebalancing.
The column assignment shows an assignment of N before entering a subsequent step. This possibly induces a reassignment of the other nodes P, C, S, D as well.
If something has been changed by the case, this is shown in the column group after.
An arrow â in column next signifies that the rebalancing is complete with this step. If the column after determines exactly one case, this case is given as the subsequent one, otherwise there are question marks.
The loop is contained in the sections from Start_D through "Delete case 1", where the problem of rebalancing is escalated 
  
    
      
        Î
        h
        =
        1
      
    
    {\displaystyle \Delta h=1}
  
 level higher in the tree in that the parent P becomes the new current node N. So it takes maximally 
  
    
      
        h
      
    
    {\displaystyle h}
  
 iterations to repair the tree (where 
  
    
      
        h
      
    
    {\displaystyle h}
  
 is the height of the tree). Because the probability of escalation decreases exponentially with each iteration the total rebalancing cost is constant on average, indeed amortized constant. (Just as an aside: Mehlhorn & Sanders point out: "AVL trees do not support constant amortized update costs."[17]:â165,â158â This is true for the rebalancing after a deletion, but not AVL insertion.[33])
Out of the body of the loop there are exiting branches to the cases D3, D6, D5 + D6, D4, and D2; section "Delete case 3" of its own has three different exiting branches to the cases D6, D5 and D4.
Rotations occur in cases D6 and D5 + D6 and D3 + D5 + D6 â all outside the loop. Therefore, at most three rotations occur in total.
â Explanation of symbolsfirst iterationhigher iterationDelete case 1
Delete case 1[edit]
P, S, and Sâs children are black. After painting S red all paths passing through S, which are precisely those paths not passing through N, have one less black node. Now all paths in the subtree rooted by P have the same number of black nodes, but one fewer than the paths that do not pass through P, so requirement 4 may still be violated. After relabeling P to N the loop invariant is fulfilled so that the rebalancing can be iterated on one black level (= 1 tree level) higher.

    // Case_D1 (P+C+S+D black):
    S->color = RED;
    N = P; // new current node (maybe the root)
    // iterate 1 black level
    //   (= 1 tree level) higher
  } while ((P = N->parent) != NULL);
  // end of the (do while)-loop

Delete case 2[edit]
The current node N is the new root. One black node has been removed from every path, so the RB-properties are preserved.
The black height of the tree decreases by 1.

  // Case_D2 (P == NULL):
  return; // deletion complete


first iterationhigher iterationDelete case 3
Delete case 3[edit]
The sibling S is red, so P and the nephews C and D have to be black. A dir-rotation at P turns S into Nâs grandparent.
Then after reversing the colors of P and S, the path through N is still short one black node. But N has a red parent P and a black sibling S, so the transformations in cases D4, D5, or D6 are able to restore the RB-shape.

Case_D3: // S red && P+C+D black:
  RotateDirRoot(T,P,dir); // P may be the root
  P->color = RED;
  S->color = BLACK;
  S = C; //Â != NIL
  // now: P red && S black
  D = S->child[1-dir]; // distant nephew
  if (D != NIL && D->color == RED)
    goto Case_D6;      // D red && S black
  C = S->child[  dir]; // close   nephew
  if (C != NIL && C->color == RED)
    goto Case_D5;      // C red && S+D black
  // Otherwise C+D considered black.
  // fall through to Case_D4


first iterationhigher iterationDelete case 4
Delete case 4[edit]
The sibling S and Sâs children are black, but P is red. Exchanging the colors of S and P does not affect the number of black nodes on paths going through S, but it does add one to the number of black nodes on paths going through N, making up for the deleted black node on those paths.

Case_D4: // P red && S+C+D black:
  S->color = RED;
  P->color = BLACK;
  return; // deletion complete


first iterationhigher iterationDelete case 5
Delete case 5[edit]
The sibling S is black, Sâs close child C is red, and Sâs distant child D is black. After a (1-dir)-rotation at S the nephew C becomes Sâs parent and Nâs new sibling. The colors of S and C are exchanged.
All paths still have the same number of black nodes, but now N has a black sibling whose distant child is red, so the constellation is fit for case D6. Neither N nor its parent P are affected by this transformation, and P may be red or black ( in the diagram).

Case_D5: // C red && S+D black:
  RotateDir(S,1-dir); // S is never the root
  S->color = RED;
  C->color = BLACK;
  D = S;
  S = C;
  // now: D red && S black
  // fall through to Case_D6


first iterationhigher iterationDelete case 6
Delete case 6[edit]
The sibling S is black, Sâs distant child D is red. After a dir-rotation at P the sibling S becomes the parent of P and Sâs distant child D. The colors of P and S are exchanged, and D is made black. The subtree still has the same color at its root, namely either red or black ( in the diagram), which refers to the same color both before and after the transformation. This way requirement 3 is preserved. The paths in the subtree not passing through N (i.o.w. passing through D and node 3 in the diagram) pass through the same number of black nodes as before, but N now has one additional black ancestor: either P has become black, or it was black and S was added as a black grandparent. Thus, the paths passing through N pass through one additional black node, so that requirement 4 is restored and the total tree is in RB-shape.

Case_D6: // D red && S black:
  RotateDirRoot(T,P,dir); // P may be the root
  S->color = P->color;
  P->color = BLACK;
  D->color = BLACK;
  return; // deletion complete
} // end of RBdelete2

Because the algorithm transforms the input without using an auxiliary data structure and using only a small amount of extra storage space for auxiliary variables it is in-place.

Proof of bounds[edit]
  Figure 4: Redâblack trees RBh of heights h=1,2,3,4,5,each with minimal number 1,2,4,6 resp. 10 of nodes.
For 
  
    
      
        h
        â
        
          N
        
      
    
    {\displaystyle h\in \mathbb {N} }
  
 there is a redâblack tree of height 
  
    
      
        h
      
    
    {\displaystyle h}
  
 with


  
    
      
        
          m
          
            h
          
        
        :=
        
          2
          
            â
            (
            h
            +
            1
            )
            
              /
            
            2
            â
          
        
        +
        
          2
          
            â
            h
            
              /
            
            2
            â
          
        
        â
        2
      
    
    {\displaystyle m_{h}:=2^{\lfloor (h+1)/2\rfloor }+2^{\lfloor h/2\rfloor }-2}
  
 Â  nodes
and there is no redâblack tree of this height with fewer nodesâtherefore it is minimal.Its black height is â
  
    
      
        â
        h
        
          /
        
        2
        â
        .
      
    
    {\displaystyle \lfloor h/2\rfloor .}
  


Proof
For a redâblack tree of a certain height to have minimal number of nodes, it has to have exactly one longest path with maximal number of red nodes, in order to achieve a maximal tree height with a minimal black height. Besides this path all other nodes have to be black.[16]:â444 Proof sketchâ If a node is taken off this tree it either loses height or some RB property.
The RB tree of height 
  
    
      
        h
        =
        1
      
    
    {\displaystyle h=1}
  
 with red root is minimal. This is in agreement with


  
    
      
        
          m
          
            1
          
        
        =
        
          2
          
            â
            (
            1
            +
            1
            )
            
              /
            
            2
            â
          
        
        
        +
        
        
          2
          
            â
            1
            
              /
            
            2
            â
          
        
        
        
        â
        
        
        2
        =
        
          2
          
            1
          
        
        
        +
        
        
          2
          
            0
          
        
        
        
        â
        
        
        2
        =
        1
        Â 
        .
      
    
    {\displaystyle m_{1}=2^{\lfloor (1+1)/2\rfloor }\!+\!2^{\lfloor 1/2\rfloor }\!\!-\!\!2=2^{1}\!+\!2^{0}\!\!-\!\!2=1~.}
  

A minimal RB tree (RBh in figure 4) of height 
  
    
      
        h
        >
        1
      
    
    {\displaystyle h>1}
  
 has a root whose two child subtrees are of different height. The higher child subtree is also a minimal RB tree, RBhâ1, containing also a longest path that defines its height 
  
    
      
        h
        
        
        â
        
        
        1
      
    
    {\displaystyle h\!\!-\!\!1}
  
; it has 
  
    
      
        
          m
          
            h
            â
            1
          
        
      
    
    {\displaystyle m_{h-1}}
  
 nodes and the black height 
  
    
      
        â
        (
        h
        
        
        â
        
        
        1
        )
        
          /
        
        2
        â
        =:
        s
        .
      
    
    {\displaystyle \lfloor (h\!\!-\!\!1)/2\rfloor =:s.}
  
 The other subtree is a perfect binary tree of (black) height 
  
    
      
        s
      
    
    {\displaystyle s}
  
 having 
  
    
      
        
          2
          
            s
          
        
        
        
        â
        
        
        1
        =
        
          2
          
            â
            (
            h
            â
            1
            )
            
              /
            
            2
            â
          
        
        
        
        â
        
        
        1
      
    
    {\displaystyle 2^{s}\!\!-\!\!1=2^{\lfloor (h-1)/2\rfloor }\!\!-\!\!1}
  
 black nodesâand no red node. Then the number of nodes is by induction






  
    
      
        
          m
          
            h
          
        
      
    
    {\displaystyle m_{h}}
  


  
    
      
        =
      
    
    {\displaystyle =}
  


  
    
      
        (
        
          m
          
            h
            â
            1
          
        
        )
      
    
    {\displaystyle (m_{h-1})}
  


  
    
      
        +
      
    
    {\displaystyle +}
  


  
    
      
        (
        1
        )
      
    
    {\displaystyle (1)}
  


  
    
      
        +
      
    
    {\displaystyle +}
  


  
    
      
        (
        
          2
          
            â
            (
            h
            â
            1
            )
            
              /
            
            2
            â
          
        
        â
        1
        )
      
    
    {\displaystyle (2^{\lfloor (h-1)/2\rfloor }-1)}
  






(higher subtree)

(root)

(second subtree)


resulting in




  
    
      
        
          m
          
            h
          
        
      
    
    {\displaystyle m_{h}}
  


  
    
      
        =
      
    
    {\displaystyle =}
  


  
    
      
        (
        
          2
          
            â
            h
            
              /
            
            2
            â
          
        
        +
        
          2
          
            â
            (
            h
            â
            1
            )
            
              /
            
            2
            â
          
        
        â
        2
        )
      
    
    {\displaystyle (2^{\lfloor h/2\rfloor }+2^{\lfloor (h-1)/2\rfloor }-2)}
  


  
    
      
        +
      
    
    {\displaystyle +}
  


  
    
      
        
          2
          
            â
            (
            h
            â
            1
            )
            
              /
            
            2
            â
          
        
      
    
    {\displaystyle 2^{\lfloor (h-1)/2\rfloor }}
  






  
    
      
        =
      
    
    {\displaystyle =}
  


  
    
      
        
          2
          
            â
            h
            
              /
            
            2
            â
          
        
        +
        
          2
          
            â
            (
            h
            +
            1
            )
            
              /
            
            2
            â
          
        
        â
        2
      
    
    {\displaystyle 2^{\lfloor h/2\rfloor }+2^{\lfloor (h+1)/2\rfloor }-2}
  
.Â Â â 

The graph of the function 
  
    
      
        
          m
          
            h
          
        
      
    
    {\displaystyle m_{h}}
  
 is convex and piecewise linear with breakpoints at 
  
    
      
        (
        h
        =
        2
        k
        
        
          |
        
        
        
          m
          
            2
            k
          
        
        =
        2
        â
        
          2
          
            k
          
        
        
        â
        
        2
        )
      
    
    {\displaystyle (h=2k\;|\;m_{2k}=2\cdot 2^{k}\!-\!2)}
  
 where 
  
    
      
        k
        â
        
          N
        
        .
      
    
    {\displaystyle k\in \mathbb {N} .}
  
 The function has been tabulated as 
  
    
      
        
          m
          
            h
          
        
        =
      
    
    {\displaystyle m_{h}=}
  
 A027383(hâ1) for 
  
    
      
        h
        â¥
        1
      
    
    {\displaystyle h\geq 1}
  
 (sequence A027383 in the OEIS).

Solving the function for 
  
    
      
        h
      
    
    {\displaystyle h}
  

The inequality 
  
    
      
        9
        >
        8
        =
        
          2
          
            3
          
        
      
    
    {\displaystyle 9>8=2^{3}}
  
 leads to 
  
    
      
        3
        >
        
          2
          
            3
            
              /
            
            2
          
        
      
    
    {\displaystyle 3>2^{3/2}}
  
 that, for odd 
  
    
      
        h
      
    
    {\displaystyle h}
  
, leads to


  
    
      
        
          m
          
            h
          
        
        =
        
          2
          
            (
            h
            +
            1
            )
            
              /
            
            2
          
        
        +
        
          2
          
            (
            h
            â
            1
            )
            
              /
            
            2
          
        
        â
        2
        =
        3
        â
        
          2
          
            (
            h
            â
            1
            )
            
              /
            
            2
          
        
        â
        2
        =
        
          
            (
          
        
        3
        â
        
          2
          
            â
            3
            
              /
            
            2
          
        
        
          
            )
          
        
        â
        
          2
          
            (
            h
            +
            2
            )
            
              /
            
            2
          
        
        â
        2
        >
        2
        â
        
          2
          
            h
            
              /
            
            2
          
        
        â
        2
        ,
      
    
    {\displaystyle m_{h}=2^{(h+1)/2}+2^{(h-1)/2}-2=3\cdot 2^{(h-1)/2}-2={\bigl (}3\cdot 2^{-3/2}{\bigr )}\cdot 2^{(h+2)/2}-2>2\cdot 2^{h/2}-2,}
  
,
which yields


  
    
      
        
          m
          
            h
          
        
        â¥
        2
        â
        
          2
          
            h
            
              /
            
            2
          
        
        â
        2
      
    
    {\displaystyle m_{h}\geq 2\cdot 2^{h/2}-2}
  

for all 
  
    
      
        h
      
    
    {\displaystyle h}
  
 (odd and even). So 
  
    
      
        h
      
    
    {\displaystyle h}
  
 is in the interval






  
    
      
        
          log
          
            2
          
        
        â¡
        (
        n
        +
        1
        )
      
    
    {\displaystyle \log _{2}(n+1)}
  


  
    
      
        â¤
        h
        â¤
      
    
    {\displaystyle \leq h\leq }
  


  
    
      
        2
        
          log
          
            2
          
        
        â¡
        (
        n
        +
        2
        )
        â
        2
        =
        2
        
          log
          
            2
          
        
        â¡
        (
        n
        
          /
        
        2
        +
        1
        )
      
    
    {\displaystyle 2\log _{2}(n+2)-2=2\log _{2}(n/2+1)}
  


  
    
      
        
          
            [
          
        
        <
        2
        
          log
          
            2
          
        
        â¡
        (
        n
        +
        1
        )
        
        
          
            ]
          
        
      
    
    {\displaystyle {\bigl [}<2\log _{2}(n+1)\;{\bigr ]}}
  




(perfect binary tree)

(minimal redâblack tree)

with 
  
    
      
        n
      
    
    {\displaystyle n}
  
 being the number of nodes.[34]

Conclusion
A redâblack tree with 
  
    
      
        n
      
    
    {\displaystyle n}
  
 nodes (keys) has a height 
  
    
      
        h
        â
        O
        (
        log
        â¡
        n
        )
        .
      
    
    {\displaystyle h\in O(\log n).}
  


Set operations and bulk operations[edit]
In addition to the single-element insert, delete and lookup operations, several set operations have been defined on redâblack trees: union, intersection and set difference. Then fast bulk operations on insertions or deletions can be implemented based on these set functions. These set operations rely on two helper operations, Split and Join. With the new operations, the implementation of redâblack trees can be more efficient and highly-parallelizable.[35] This implementation allows a red root.

Join: The function Join is on two redâblack trees t1 and t2 and a key k, where t1 < k < t2, i.e. all keys in t1 are less than k, and all keys in t2 are greater than k. It returns a tree containing all elements in t1, t2 as well as k.
If the two trees have the same black height, Join simply creates a new node with left subtree t1, root k and right subtree t2. If both t1 and t2 have black root, set k to be red. Otherwise k is set black.
If the black heights are unequal, suppose that t1 has larger black height than t2 (the other case is symmetric). Join follows the right spine of t1 until a black node c, which is balanced with t2. At this point a new node with left child c, root k (set to be red) and right child t2 is created to replace c. The new node may invalidate the redâblack invariant because at most three red nodes can appear in a row. This can be fixed with a double rotation. If double red issue propagates to the root, the root is then set to be black, restoring the properties. The cost of this function is the difference of the black heights between the two input trees.
Split: To split a redâblack tree into two smaller trees, those smaller than key x, and those larger than key x, first draw a path from the root by inserting x into the redâblack tree. After this insertion, all values less than x will be found on the left of the path, and all values greater than x will be found on the right. By applying Join, all the subtrees on the left side are merged bottom-up using keys on the path as intermediate nodes from bottom to top to form the left tree, and the right part is symmetric.
For some applications, Split also returns a boolean value denoting if x appears in the tree. The cost of Split is 
  
    
      
        O
        (
        log
        â¡
        n
        )
        ,
      
    
    {\displaystyle O(\log n),}
  
 order of the height of the tree. This algorithm actually has nothing to do with any special properties of a redâblack tree, and may be used on any tree with a join operation, such as an AVL tree.
The join algorithm is as follows:

function joinRightRB(TL, k, TR)
    if r(TL)=âr(TL)/2âÃ2:
        return Node(TL,â¨k,redâ©,TR)
    else 
        (L',â¨k',c'â©,R')=expose(TL)
        T'=Node(L',â¨k',c'â©,joinRightRB(R',k,TR)
        if (c'=black) and (T'.right.color=T'.right.right.color=red):
            T'.right.right.color=black;
            return rotateLeft(T')
        else return T'

function joinLeftRB(TL, k, TR)
  /* symmetric to joinRightRB */

function join(TL, k, TR)
    if âr(TL)/2â>âr(TR)/2âÃ2:
        T'=joinRightRB(TL,k,TR)
        if (T'.color=red) and (T'.right.color=red):
            T'.color=black
        return T'
    else if âr(TR)/2â>âr(TL)/2âÃ2
        /* symmetric */
    else if (TL.color=black) and (TR.color=black)
        Node(TL,â¨k,redâ©,TR)
    else
        Node(TL,â¨k,blackâ©,TR)

Here 
  
    
      
        r
        (
        v
        )
      
    
    {\displaystyle r(v)}
  
 of a node 
  
    
      
        v
      
    
    {\displaystyle v}
  
 means twice the black height of a black node, and the twice the black height of a red node. expose(v)=(l,â¨k,câ©,r) means to extract a tree node 
  
    
      
        v
      
    
    {\displaystyle v}
  
âs left child 
  
    
      
        l
      
    
    {\displaystyle l}
  
, the key of the node 
  
    
      
        k
      
    
    {\displaystyle k}
  
, the color of the node 
  
    
      
        c
      
    
    {\displaystyle c}
  
 and the right child 
  
    
      
        r
      
    
    {\displaystyle r}
  
. Node(l,â¨k,câ©,r) means to create a node of left child 
  
    
      
        l
      
    
    {\displaystyle l}
  
, key 
  
    
      
        k
      
    
    {\displaystyle k}
  
, color 
  
    
      
        c
      
    
    {\displaystyle c}
  
 and right child 
  
    
      
        r
      
    
    {\displaystyle r}
  
.
The split algorithm is as follows:

function split(T, k)
    if (T = nil) return (nil, false, nil)
    (L,(m,c),R) = expose(T)
    if (k = m) return (L, true, R)
    if (k < m) 
        (L',b,R') = split(L, k)
        return (L',b,join(R',m,R))
    if (k>m) 
        (L',b,R') = split(R, k)
        return (join(L,m,L'),b,R))

The union of two redâblack trees t1 and t2 representing sets A and B, is a redâblack tree t that represents A âª B. The following recursive function computes this union:

function union(t1, t2):
    if t1 = nil:
        return t2
    if t2 = nil:
        return t1
    t<, t> â split t2 on t1.root
    return join(t1.root, union(left(t1), t<), union(right(t1), t>))

Here, split is presumed to return two trees: one holding the keys less its input key, one holding the greater keys. (The algorithm is non-destructive, but an in-place destructive version exists as well.)
The algorithm for intersection or difference is similar, but requires the Join2 helper routine that is the same as Join but without the middle key. Based on the new functions for union, intersection or difference, either one key or multiple keys can be inserted to or deleted from the redâblack tree. Since Split calls Join but does not deal with the balancing criteria of redâblack trees directly, such an implementation is usually called the "join-based" implementation.
The complexity of each of union, intersection and difference is 
  
    
      
        O
        (
        
          (
          
            m
            log
            â¡
            
              (
              
                
                  
                    n
                    m
                  
                
                +
                1
              
              )
            
          
          )
        
      
    
    {\displaystyle O(\left(m\log \left({n \over m}+1\right)\right)}
  
 for two redâblack trees of sizes 
  
    
      
        m
      
    
    {\displaystyle m}
  
 and 
  
    
      
        n
        (
        â¥
        m
        )
      
    
    {\displaystyle n(\geq m)}
  
. This complexity is optimal in terms of the number of comparisons. More importantly, since the recursive calls to union, intersection or difference are independent of each other, they can be executed in parallel with a parallel depth 
  
    
      
        O
        (
        log
        â¡
        m
        log
        â¡
        n
        )
      
    
    {\displaystyle O(\log m\log n)}
  
.[35] When 
  
    
      
        m
        =
        1
      
    
    {\displaystyle m=1}
  
, the join-based implementation has the same computational directed acyclic graph (DAG) as single-element insertion and deletion if the root of the larger tree is used to split the smaller tree.

Parallel algorithms[edit]
Parallel algorithms for constructing redâblack trees from sorted lists of items can run in constant time or 
  
    
      
        O
        (
        log
        â¡
        log
        â¡
        n
        )
      
    
    {\displaystyle O(\log \log n)}
  
 time, depending on the computer model, if the number of processors available is asymptotically proportional to the number 
  
    
      
        n
      
    
    {\displaystyle n}
  
 of items where 
  
    
      
        n
        â
        â
      
    
    {\displaystyle n\to \infty }
  
. Fast search, insertion, and deletion parallel algorithms are also known.[36]
The join-based algorithms for redâblack trees are parallel for bulk operations, including union, intersection, construction, filter, map-reduce, and so on.

Parallel bulk operations[edit]
Basic operations like insertion, removal or update can be parallelised by defining operations that process bulks of multiple elements. It is also possible to process bulks with several basic operations, for example bulks may contain elements to insert and also elements to remove from the tree.
The algorithms for bulk operations arenât just applicable to the redâblack tree, but can be adapted to other sorted sequence data structures as well, like the 2â3 tree, 2â3â4 tree and (a,b)-tree. In the following different algorithms for bulk insert will be explained, but the same algorithms can also be applied to removal and update. Bulk insert is an operation that inserts each element of a sequence 
  
    
      
        I
      
    
    {\displaystyle I}
  
 into a tree 
  
    
      
        T
      
    
    {\displaystyle T}
  
.

Join-based[edit]
This approach can be applied to every sorted sequence data structure that supports efficient join- and split-operations.[37]
The general idea is to split 
  
    
      
        I
      
    
    {\displaystyle I}
  
 and 
  
    
      
        T
      
    
    {\displaystyle T}
  
 in multiple parts and perform the insertions on these parts in parallel.

First the bulk 
  
    
      
        I
      
    
    {\displaystyle I}
  
 of elements to insert has to be sorted.
After that, the algorithm splits 
  
    
      
        I
      
    
    {\displaystyle I}
  
 into 
  
    
      
        k
        â
        
          
            N
          
          
            +
          
        
      
    
    {\displaystyle k\in \mathbb {N} ^{+}}
  
 parts 
  
    
      
        â¨
        
          I
          
            1
          
        
        ,
        â¯
        ,
        
          I
          
            k
          
        
        â©
      
    
    {\displaystyle \langle I_{1},\cdots ,I_{k}\rangle }
  
 of about equal sizes.
Next the tree 
  
    
      
        T
      
    
    {\displaystyle T}
  
 has to be split into 
  
    
      
        k
      
    
    {\displaystyle k}
  
 parts 
  
    
      
        â¨
        
          T
          
            1
          
        
        ,
        â¯
        ,
        
          T
          
            k
          
        
        â©
      
    
    {\displaystyle \langle T_{1},\cdots ,T_{k}\rangle }
  
 in a way, so that for every 
  
    
      
        j
        â
        
          
            N
          
          
            +
          
        
        
          |
        
        
        1
        â¤
        j
        <
        k
      
    
    {\displaystyle j\in \mathbb {N} ^{+}|\,1\leq j<k}
  
 following constraints hold:

  
    
      
        
          last
        
        (
        
          I
          
            j
          
        
        )
        <
        
          first
        
        (
        
          T
          
            j
            +
            1
          
        
        )
      
    
    {\displaystyle {\text{last}}(I_{j})<{\text{first}}(T_{j+1})}
  


  
    
      
        
          last
        
        (
        
          T
          
            j
          
        
        )
        <
        
          first
        
        (
        
          I
          
            j
            +
            1
          
        
        )
      
    
    {\displaystyle {\text{last}}(T_{j})<{\text{first}}(I_{j+1})}
  

Now the algorithm inserts each element of 
  
    
      
        
          I
          
            j
          
        
      
    
    {\displaystyle I_{j}}
  
 into 
  
    
      
        
          T
          
            j
          
        
      
    
    {\displaystyle T_{j}}
  
 sequentially. This step has to be performed for every 
  
    
      
        j
      
    
    {\displaystyle j}
  
, which can be done by up to 
  
    
      
        k
      
    
    {\displaystyle k}
  
 processors in parallel.
Finally, the resulting trees will be joined to form the final result of the entire operation.
Note that in Step 3 the constraints for splitting 
  
    
      
        I
      
    
    {\displaystyle I}
  
 assure that in Step 5 the trees can be joined again and the resulting sequence is sorted.


		
			
			
initial tree

			
		
		
			
			
split I and T

			
		
		
			
			
insert into the split T

			
		
		
			
			
join T

			
		

The pseudo code shows a simple divide-and-conquer implementation of the join-based algorithm for bulk-insert.
Both recursive calls can be executed in parallel.
The join operation used here differs from the version explained in this article, instead join2 is used, which misses the second parameter k.

bulkInsert(T, I, k):
    I.sort()
    bulklInsertRec(T, I, k)

bulkInsertRec(T, I, k):
    if k = 1:
        forall e in I: T.insert(e)
    else
        mÂ := âsize(I) / 2â
        (T1, _, T2)Â := split(T, I[m])
        bulkInsertRec(T1, I[0 .. m], âk / 2â)
            || bulkInsertRec(T2, I[m + 1 .. size(I) - 1], âk / 2â)
        T â join2(T1, T2)

Execution time[edit]
Sorting 
  
    
      
        I
      
    
    {\displaystyle I}
  
 is not considered in this analysis.




#recursion levels

  
    
      
        â
        O
        (
        log
        â¡
        k
        )
      
    
    {\displaystyle \in O(\log k)}
  



T(split) + T(join)

  
    
      
        â
        O
        (
        log
        â¡
        
          |
        
        T
        
          |
        
        )
      
    
    {\displaystyle \in O(\log |T|)}
  



insertions per thread

  
    
      
        â
        O
        
          (
          
            
              
                
                  |
                
                I
                
                  |
                
              
              k
            
          
          )
        
      
    
    {\displaystyle \in O\left({\frac {|I|}{k}}\right)}
  



T(insert)

  
    
      
        â
        O
        (
        log
        â¡
        
          |
        
        T
        
          |
        
        )
      
    
    {\displaystyle \in O(\log |T|)}
  



T(bulkInsert) with 
  
    
      
        k
      
    
    {\displaystyle k}
  
 = #processors

  
    
      
        â
        O
        
          (
          
            log
            â¡
            k
            log
            â¡
            
              |
            
            T
            
              |
            
            +
            
              
                
                  
                    |
                  
                  I
                  
                    |
                  
                
                k
              
            
            log
            â¡
            
              |
            
            T
            
              |
            
          
          )
        
      
    
    {\displaystyle \in O\left(\log k\log |T|+{\frac {|I|}{k}}\log |T|\right)}
  


This can be improved by using parallel algorithms for splitting and joining.
In this case the execution time is 
  
    
      
        â
        O
        
          (
          
            log
            â¡
            
              |
            
            T
            
              |
            
            +
            
              
                
                  
                    |
                  
                  I
                  
                    |
                  
                
                k
              
            
            log
            â¡
            
              |
            
            T
            
              |
            
          
          )
        
      
    
    {\displaystyle \in O\left(\log |T|+{\frac {|I|}{k}}\log |T|\right)}
  
.[38]

Work[edit]



#splits, #joins

  
    
      
        â
        O
        (
        k
        )
      
    
    {\displaystyle \in O(k)}
  



W(split) + W(join)

  
    
      
        â
        O
        (
        log
        â¡
        
          |
        
        T
        
          |
        
        )
      
    
    {\displaystyle \in O(\log |T|)}
  



#insertions

  
    
      
        â
        O
        (
        
          |
        
        I
        
          |
        
        )
      
    
    {\displaystyle \in O(|I|)}
  



W(insert)

  
    
      
        â
        O
        (
        log
        â¡
        
          |
        
        T
        
          |
        
        )
      
    
    {\displaystyle \in O(\log |T|)}
  



W(bulkInsert)

  
    
      
        â
        O
        (
        k
        log
        â¡
        
          |
        
        T
        
          |
        
        +
        
          |
        
        I
        
          |
        
        log
        â¡
        
          |
        
        T
        
          |
        
        )
      
    
    {\displaystyle \in O(k\log |T|+|I|\log |T|)}
  


Pipelining[edit]
Another method of parallelizing bulk operations is to use a pipelining approach.[39]
This can be done by breaking the task of processing a basic operation up into a sequence of subtasks.
For multiple basic operations the subtasks can be processed in parallel by assigning each subtask to a separate processor.

First the bulk 
  
    
      
        I
      
    
    {\displaystyle I}
  
 of elements to insert has to be sorted.
For each element in 
  
    
      
        I
      
    
    {\displaystyle I}
  
 the algorithm locates the according insertion position in 
  
    
      
        T
      
    
    {\displaystyle T}
  
. This can be done in parallel for each element 
  
    
      
        â
        I
      
    
    {\displaystyle \in I}
  
 since 
  
    
      
        T
      
    
    {\displaystyle T}
  
 wonât be mutated in this process. Now 
  
    
      
        I
      
    
    {\displaystyle I}
  
 has to be divided into subsequences 
  
    
      
        S
      
    
    {\displaystyle S}
  
 according to the insertion position of each element. For example 
  
    
      
        
          s
          
            n
            ,
            
              
                l
                e
                f
                t
              
            
          
        
      
    
    {\displaystyle s_{n,{\mathit {left}}}}
  
 is the subsequence of 
  
    
      
        I
      
    
    {\displaystyle I}
  
 that contains the elements whose insertion position would be to the left of node 
  
    
      
        n
      
    
    {\displaystyle n}
  
.
The middle element 
  
    
      
        
          m
          
            n
            ,
            
              
                d
                i
                r
              
            
          
        
      
    
    {\displaystyle m_{n,{\mathit {dir}}}}
  
 of every subsequence 
  
    
      
        
          s
          
            n
            ,
            
              
                d
                i
                r
              
            
          
        
      
    
    {\displaystyle s_{n,{\mathit {dir}}}}
  
 will be inserted into 
  
    
      
        T
      
    
    {\displaystyle T}
  
 as a new node 
  
    
      
        
          n
          â²
        
      
    
    {\displaystyle n'}
  
. This can be done in parallel for each 
  
    
      
        
          m
          
            n
            ,
            
              
                d
                i
                r
              
            
          
        
      
    
    {\displaystyle m_{n,{\mathit {dir}}}}
  
 since by definition the insertion position of each 
  
    
      
        
          m
          
            n
            ,
            
              
                d
                i
                r
              
            
          
        
      
    
    {\displaystyle m_{n,{\mathit {dir}}}}
  
 is unique. If 
  
    
      
        
          s
          
            n
            ,
            
              
                d
                i
                r
              
            
          
        
      
    
    {\displaystyle s_{n,{\mathit {dir}}}}
  
 contains elements to the left or to the right of 
  
    
      
        
          m
          
            n
            ,
            
              
                d
                i
                r
              
            
          
        
      
    
    {\displaystyle m_{n,{\mathit {dir}}}}
  
, those will be contained in a new set of subsequences 
  
    
      
        S
      
    
    {\displaystyle S}
  
 as 
  
    
      
        
          s
          
            
              n
              â²
            
            ,
            
              
                l
                e
                f
                t
              
            
          
        
      
    
    {\displaystyle s_{n',{\mathit {left}}}}
  
 or 
  
    
      
        
          s
          
            
              n
              â²
            
            ,
            
              
                r
                i
                g
                h
                t
              
            
          
        
      
    
    {\displaystyle s_{n',{\mathit {right}}}}
  
.
Now 
  
    
      
        T
      
    
    {\displaystyle T}
  
 possibly contains up to two consecutive red nodes at the end of the paths form the root to the leaves, which needs to be repaired. Note that, while repairing, the insertion position of elements 
  
    
      
        â
        S
      
    
    {\displaystyle \in S}
  
 have to be updated, if the corresponding nodes are affected by rotations.If two nodes have different nearest black ancestors, they can be repaired in parallel. Since at most four nodes can have the same nearest black ancestor, the nodes at the lowest level can be repaired in a constant number of parallel steps.This step will be applied successively to the black levels above until 
  
    
      
        T
      
    
    {\displaystyle T}
  
 is fully repaired.
The steps 3 to 5 will be repeated on the new subsequences until 
  
    
      
        S
      
    
    {\displaystyle S}
  
 is empty. At this point every element 
  
    
      
        â
        I
      
    
    {\displaystyle \in I}
  
 has been inserted. Each application of these steps is called a stage. Since the length of the subsequences in 
  
    
      
        S
      
    
    {\displaystyle S}
  
 is 
  
    
      
        â
        O
        (
        
          |
        
        I
        
          |
        
        )
      
    
    {\displaystyle \in O(|I|)}
  
 and in every stage the subsequences are being cut in half, the number of stages is 
  
    
      
        â
        O
        (
        log
        â¡
        
          |
        
        I
        
          |
        
        )
      
    
    {\displaystyle \in O(\log |I|)}
  
.Since all stages move up the black levels of the tree, they can be parallelised in a pipeline. Once a stage has finished processing one black level, the next stage is able to move up and continue at that level.

		
			
			
Initial tree

			
		
		
			
			
Find insert positions

			
		
		
			
			
Stage 1 inserts elements

			
		
		
			
			
Stage 1 begins to repair nodes

			
		
		
			
			
Stage 2 inserts elements

			
		
		
			
			
Stage 2 begins to repair nodes

			
		
		
			
			
Stage 3 inserts elements

			
		
		
			
			
Stage 3 begins to repair nodes

			
		
		
			
			
Stage 3 continues to repair nodes

			
		

Execution time[edit]
Sorting 
  
    
      
        I
      
    
    {\displaystyle I}
  
 is not considered in this analysis.
Also, 
  
    
      
        
          |
        
        I
        
          |
        
      
    
    {\displaystyle |I|}
  
 is assumed to be smaller than 
  
    
      
        
          |
        
        T
        
          |
        
      
    
    {\displaystyle |T|}
  
, otherwise it would be more efficient to construct the resulting tree from scratch.




T(find insert position)

  
    
      
        â
        O
        (
        log
        â¡
        
          |
        
        T
        
          |
        
        )
      
    
    {\displaystyle \in O(\log |T|)}
  



#stages

  
    
      
        â
        O
        (
        log
        â¡
        
          |
        
        I
        
          |
        
        )
      
    
    {\displaystyle \in O(\log |I|)}
  



T(insert) + T(repair)

  
    
      
        â
        O
        (
        log
        â¡
        
          |
        
        T
        
          |
        
        )
      
    
    {\displaystyle \in O(\log |T|)}
  



T(bulkInsert) with 
  
    
      
        
          |
        
        I
        
          |
        
      
    
    {\displaystyle |I|}
  
 ~ #processors

  
    
      
        â
        O
        (
        log
        â¡
        
          |
        
        I
        
          |
        
        +
        2
        â
        log
        â¡
        
          |
        
        T
        
          |
        
        )
      
    
    {\displaystyle \in O(\log |I|+2\cdot \log |T|)}
  

  
    
      
        =
        O
        (
        log
        â¡
        
          |
        
        T
        
          |
        
        )
      
    
    {\displaystyle =O(\log |T|)}
  


Work[edit]



W(find insert positions)

  
    
      
        â
        O
        (
        
          |
        
        I
        
          |
        
        log
        â¡
        
          |
        
        T
        
          |
        
        )
      
    
    {\displaystyle \in O(|I|\log |T|)}
  



#insertions, #repairs

  
    
      
        â
        O
        (
        
          |
        
        I
        
          |
        
        )
      
    
    {\displaystyle \in O(|I|)}
  



W(insert) + W(repair)

  
    
      
        â
        O
        (
        log
        â¡
        
          |
        
        T
        
          |
        
        )
      
    
    {\displaystyle \in O(\log |T|)}
  



W(bulkInsert)

  
    
      
        â
        O
        (
        2
        â
        
          |
        
        I
        
          |
        
        log
        â¡
        
          |
        
        T
        
          |
        
        )
      
    
    {\displaystyle \in O(2\cdot |I|\log |T|)}
  

  
    
      
        =
        O
        (
        
          |
        
        I
        
          |
        
        log
        â¡
        
          |
        
        T
        
          |
        
        )
      
    
    {\displaystyle =O(|I|\log |T|)}
  


Popular culture[edit]
A redâblack tree was referenced correctly in an episode of Missing[40] as noted by Robert Sedgewick in one of his lectures:[41]

Jess: It was the red door again.Pollock: I thought the red door was the storage container.Jess: But it wasnât red anymore, it was black.Antonio: So red turning to black means what?Pollock: Budget deficits, red ink, black ink.Antonio: It could be from a binary search tree. The redâblack tree tracks every simple path from a node to a descendant leaf that has the same number of black nodes.Jess: Does that help you with the ladies?
See also[edit]
List of data structures
Tree data structure
Tree rotation
AA tree, a variation of the redâblack tree
AVL tree
B-tree (2â3 tree, 2â3â4 tree, B+ tree, B*-tree, UB-tree)
Scapegoat tree
Splay tree
T-tree
WAVL tree
References and notes[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ Jump up to: a b c d .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}James Paton. "RedâBlack Trees".

^ Jump up to: a b rebalancing only (no lookup), see Tarjan and Mehlhorn.

^ Jump up to: a b Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001). "RedâBlack Trees". Introduction to Algorithms (secondÂ ed.). MIT Press. pp.Â 273â301. ISBNÂ 978-0-262-03293-3.

^ Morris, John (1998). "RedâBlack Trees". Data Structures and Algorithms.

^ Rudolf Bayer (1972). "Symmetric binary B-Trees: Data structure and maintenance algorithms". Acta Informatica. 1 (4): 290â306. doi:10.1007/BF00289509. S2CIDÂ 28836825.

^ Drozdek, Adam (2001). Data Structures and Algorithms in Java (2Â ed.). Sams Publishing. p.Â 323. ISBNÂ 978-0534376680.

^ Jump up to: a b Leonidas J. Guibas and Robert Sedgewick (1978). "A Dichromatic Framework for Balanced Trees". Proceedings of the 19th Annual Symposium on Foundations of Computer Science. pp.Â 8â21. doi:10.1109/SFCS.1978.3.

^ "Red Black Trees". eternallyconfuzzled.com. Archived from the original on 2007-09-27. Retrieved 2015-09-02.

^ Robert Sedgewick (2012). RedâBlack BSTs. Coursera. A lot of people ask why did we use the name redâblack. Well, we invented this data structure, this way of looking at balanced trees, at Xerox PARC that was the home of the personal computer and many other innovations that we live with today entering[sic] graphic user interfaces, ethernet and object-oriented programmings[sic] and many other things. But one of the things that was invented there was laser printing and we were very excited to have nearby color laser printer that could print things out in color and out of the colors the red looked the best. So, thatâs why we picked the color red to distinguish red links, the types of links, in three nodes. So, thatâs an answer to the question for people that have been asking.

^ "Where does the term "Red/Black Tree" come from?". programmers.stackexchange.com. Retrieved 2015-09-02.

^ Andersson, Arne (1993-08-11). "Balanced search trees made simple".  In Dehne, Frank; Sack, JÃ¶rg-RÃ¼diger; Santoro, Nicola; Whitesides, Sue (eds.). Algorithms and Data Structures (Proceedings). Lecture Notes in Computer Science. Vol.Â 709. Springer-Verlag Berlin Heidelberg. pp.Â 60â71. CiteSeerXÂ 10.1.1.118.6192. doi:10.1007/3-540-57155-8_236. ISBNÂ 978-3-540-57155-1. Archived from the original on 2018-12-08. Alt URL

^ Okasaki, Chris (1999-01-01). "Redâblack trees in a functional setting". Journal of Functional Programming. 9 (4): 471â477. doi:10.1017/S0956796899003494. ISSNÂ 1469-7653. Archived from the original (PS) on 2007-09-26. Retrieved 2007-05-13.

^ Sedgewick, Robert (1983). Algorithms (1stÂ ed.). Addison-Wesley. ISBNÂ 978-0-201-06672-2.

^ Sedgewick, Robert; Wayne, Kevin. "RedBlackBST.java". algs4.cs.princeton.edu. Retrieved 7 April 2018.

^ Sedgewick, Robert (2008). "Left-leaning RedâBlack Trees" (PDF). {{cite journal}}: Cite journal requires |journal= (help)

^ Jump up to: a b c Sedgewick, Robert; Wayne, Kevin (2011). Algorithms (4thÂ ed.). Addison-Wesley Professional. ISBNÂ 978-0-321-57351-3.

^ Jump up to: a b c d Mehlhorn, Kurt; Sanders, Peter (2008). "7. Sorted Sequences" (PDF). Algorithms and Data Structures: The Basic Toolbox. Berlin/Heidelberg: Springer. CiteSeerXÂ 10.1.1.148.2305. doi:10.1007/978-3-540-77978-0. ISBNÂ 978-3-540-77977-3.

^ Jump up to: a b Cormen, Thomas; Leiserson, Charles; Rivest, Ronald; Stein, Clifford (2009). "13. RedâBlack Trees". Introduction to Algorithms (3rdÂ ed.). MIT Press. pp.Â 308â309. ISBNÂ 978-0-262-03384-8.

^ Using Knuthâs definition of order: the maximum number of children

^ Sedgewick, Robert (1998). Algorithms in C++. Addison-Wesley Professional. pp.Â 565â575. ISBNÂ 978-0-201-35088-3.

^ "The Implementation of epoll (1)". September 2014.

^ Pfaff 2004

^ Jump up to: a b http://www.cs.princeton.edu/~rs/talks/LLRB/RedBlack.pdf

^ http://www.cs.princeton.edu/courses/archive/fall08/cos226/lectures/10BalancedTrees-2x2.pdf

^ Demaine, E. D.; Harmon, D.; Iacono, J.; PÄtraÅcu, M. (2007). "Dynamic OptimalityâAlmost" (PDF). SIAM Journal on Computing. 37 (1): 240. doi:10.1137/S0097539705447347. S2CIDÂ 1480961.

^ "How does a HashMap work in JAVA". coding-geek.com.

^ Tarjan, Robert Endre (April 1985). "Amortized Computational Complexity" (PDF). SIAM Journal on Algebraic and Discrete Methods. 6 (2): 306â318. doi:10.1137/0606031.

^ The important thing about these tree rotations is that they preserve the in-order sequence of the treeâs nodes.

^ "Ben Pfaff (2007): Online HTML version of a well-documented collection of binary search tree and balanced tree library routines".

^ Jump up to: a b The left columns contain far less nodes than the right ones, especially for removal. This indicates that some efficiency can be gained by pulling the first iteration out of the rebalancing loops of insertion and deletion, because many of the named nodes are NIL nodes in the first iteration and definitively non-NIL later. (See also this remark.)

^ Rotations have been placed before recoloring for reasons of clarity. But the two commute, so that it is free choice to move the rotation to the tail.

^ Jump up to: a b The same partitioning is found in Ben Pfaff.

^ Dinesh P. Mehta, Sartaj Sahni (Ed.) Handbook of Data Structures and Applications 10.4.2

^ Equality at the upper bound holds for the minimal RB trees RB2k of even height 
  
    
      
        2
        â
        k
      
    
    {\displaystyle 2\cdot k}
  
 with 
  
    
      
        n
        =
        2
        â
        
          2
          
            k
          
        
        â
        2
      
    
    {\displaystyle n=2\cdot 2^{k}-2}
  
 nodes and only for those. So the inequality is marginally more precise than the widespread 
  
    
      
        h
        <
        2
        
          log
          
            2
          
        
        â¡
        (
        n
        +
        1
        )
        ,
      
    
    {\displaystyle h<2\log _{2}(n+1),}
  
 e.g. in Cormen p. 264.Moreover, these trees are binary trees that admit one and only one coloring conforming to the RB requirements 1 to 4. But there are further such trees, e.g. appending a child node to a black leaf always forces it to red. (A minimal RB tree of odd height allows to flip the rootâs color from red to black.)

^ Jump up to: a b Blelloch, Guy E.; Ferizovic, Daniel; Sun, Yihan (2016), "Just Join for Parallel Ordered Sets" (PDF), Symposium on Parallel Algorithms and Architectures, Proc. of 28th ACM Symp. Parallel Algorithms and Architectures (SPAA 2016), ACM, pp.Â 253â264, arXiv:1602.02120, doi:10.1145/2935764.2935768, ISBNÂ 978-1-4503-4210-0, S2CIDÂ 2897793.

^ 
Park, Heejin; Park, Kunsoo (2001). "Parallel algorithms for redâblack trees". Theoretical Computer Science. 262 (1â2): 415â435. doi:10.1016/S0304-3975(00)00287-5. Our parallel algorithm for constructing a redâblack tree from a sorted list of 
  
    
      
        n
      
    
    {\displaystyle n}
  
 items runs in 
  
    
      
        O
        (
        1
        )
      
    
    {\displaystyle O(1)}
  
 time with 
  
    
      
        n
      
    
    {\displaystyle n}
  
 processors on the CRCW PRAM and runs in 
  
    
      
        O
        (
        log
        â¡
        log
        â¡
        n
        )
      
    
    {\displaystyle O(\log \log n)}
  
 time with 
  
    
      
        n
        
          /
        
        log
        â¡
        log
        â¡
        n
      
    
    {\displaystyle n/\log \log n}
  
 processors on the EREW PRAM.

^ Sanders, Peter (2019).  Mehlhorn, Kurt; Dietzfelbinger, Martin; Dementiev, Roman (eds.). Sequential and Parallel Algorithms and Data StructuresÂ : The Basic Toolbox. Springer eBooks. Cham: Springer. pp.Â 252â253. doi:10.1007/978-3-030-25209-0. ISBNÂ 9783030252090. S2CIDÂ 201692657.

^ Akhremtsev, Yaroslav; Sanders, Peter (2016). "Fast Parallel Operations on Search Trees". HiPC 2016, the 23rd IEEE International Conference on High Performance Computing, Data, and Analytics, Hyderabad, India, December, 19-22. IEEE, Piscataway (NJ): 291â300. arXiv:1510.05433. Bibcode:2015arXiv151005433A. ISBNÂ 978-1-5090-5411-4.

^ JÃ¡jÃ¡, Joseph (1992). An introduction to parallel algorithms. Reading, Mass. [u.a.]: Addison-Wesley. pp.Â 65â70. ISBNÂ 0201548569.

^ Missing (Canadian TV series). A, W Network (Canada); Lifetime (United States).

^ Robert Sedgewick (2012). B-Trees. Coursera.  10:37 minutes in. So not only is there some excitement in that dialogue but itâs also technically correct that you donât often find with math in popular culture of computer science. A redâblack tree tracks every simple path from a node to a descendant leaf with the same number of black nodes they got that right.


Further reading[edit]
Mathworld: RedâBlack Tree
San Diego State University: CS 660: RedâBlack tree notes, by Roger Whitney
 Pfaff, Ben (June 2004). "Performance Analysis of BSTs in System Software" (PDF). Stanford University.
External links[edit]
Ben Pfaff: An Introduction to Binary Search Trees and Balanced Trees. Free Software Foundation, Boston 2004, ftp.gnu.org (PDF gzip; 1662Â kB)
Redâblack tree implementation in C
A complete and working implementation in C
OCW MIT Lecture by Prof. Erik Demaine on Red Black Trees -
Binary Search Tree Insertion Visualization on YouTube â Visualization of random and pre-sorted data insertions, in elementary binary search trees, and left-leaning redâblack trees
An intrusive redâblack tree written in C++
Redâblack BSTs in 3.3 Balanced Search Trees
Redâblack BST Demo
A verified functional implementation in Isabelle (proof assistant): Chapter 8 in Functional Algorithms, Verified!
.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}show.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vteTree data structuresSearch trees(dynamic sets/associative arrays)
2â3
2â3â4
AA
(a,b)
AVL
B
B+
B*
Bx
(Optimal)Â Binary search
Dancing
HTree
Interval
Order statistic
(Left-leaning)Â Redâblack
Scapegoat
Splay
T
Treap
UB
Weight-balanced
Heaps
Binary
Binomial
Brodal
Fibonacci
Leftist
Pairing
Skew
van Emde Boas
Weak
Tries
Ctrie
C-trie (compressed ADT)
Hash
Radix
Suffix
Ternary search
X-fast
Y-fast
Spatial data partitioning trees
Ball
BK
BSP
Cartesian
Hilbert R
k-d (implicit k-d)
M
Metric
MVP
Octree
Priority R
Quad
R
R+
R*
Segment
VP
X
Other trees
Cover
Exponential
Fenwick
Finger
Fractal tree index
Fusion
Hash calendar
iDistance
K-ary
Left-child right-sibling
Link/cut
Log-structured merge
Merkle
PQ
Range
SPQR
Top

showvteWell-known data structuresTypes
Collection
Container
Abstract
Associative array
Multimap
Retrieval Data Structure
List
Stack
Queue
Double-ended queue
Priority queue
Double-ended priority queue
Set
Multiset
Disjoint-set
Arrays
Bit array
Circular buffer
Dynamic array
Hash table
Hashed array tree
Sparse matrix
Linked
Association list
Linked list
Skip list
Unrolled linked list
XOR linked list
Trees
B-tree
Binary search tree
AA tree
AVL tree
Redâblack tree
Self-balancing tree
Splay tree
Heap
Binary heap
Binomial heap
Fibonacci heap
R-tree
R* tree
R+ tree
Hilbert R-tree
Trie
Hash tree
Graphs
Binary decision diagram
Directed acyclic graph
Directed acyclic word graph

List of data structures





<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=Redâblack_tree&oldid=1066899057"
		Categories: 1972 in computingBinary treesSearch treesHidden categories: CS1 errors: missing periodicalArticles with short descriptionShort description matches WikidataArticles containing proofsArticles with example C code
	
