
Title:
AI accelerator
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Hardware acceleration unit for artificial intelligence tasks



It has been suggested that this article be merged into Hardware for artificial intelligence. (Discuss) Proposed since October 2021.
An AI accelerator is a class of specialized hardware accelerator[1] or computer system[2][3] designed to accelerate artificial intelligence and machine learning applications, including artificial neural networks and machine vision. Typical applications include algorithms for robotics, internet of things, and other data-intensive or sensor-driven tasks.[4] They are often manycore designs and generally focus on low-precision arithmetic, novel dataflow architectures or in-memory computing capability. As of 2018[update], a typical AI integrated circuit chip contains billions of MOSFET transistors.[5]
A number of vendor-specific terms exist for devices in this category, and it is an emerging technology without a dominant design.

Contents

1 History

1.1 Early attempts
1.2 Heterogeneous computing
1.3 Use of GPU
1.4 Use of FPGAs
1.5 Emergence of dedicated AI accelerator ASICs
1.6 In-memory computing architectures
1.7 In-memory computing with analog resistive memories
1.8 Atomically thin semiconductors
1.9 Integrated photonic tensor core


2 Nomenclature
3 Potential applications
4 See also
5 References
6 External links



History[edit]
Computer systems have frequently complemented the CPU with special-purpose accelerators for specialized tasks, known as coprocessors. Notable application-specific hardware units include video cards for graphics, sound cards, graphics processing units and digital signal processors. As deep learning and artificial intelligence workloads rose in prominence in the 2010s, specialized hardware units were developed or adapted from existing products to accelerate these tasks.

Early attempts[edit]
First attempts like Intel's ETANN 80170NX[6] incorporated analog circuits to compute neural functions. Another example for chips of this category is ANNA, a neural net CMOS accelerator developed by Yann LeCun.[7] Later all-digital chips like Nestor/Intel Ni1000 followed. As early as 1993, digital signal processors were used as neural network accelerators e.g. to accelerate optical character recognition software.[8] In the 1990s, there were also attempts to create parallel high-throughput systems for workstations aimed at various applications, including neural network simulations.[9][10][11] FPGA-based accelerators were also first explored in the 1990s for both inference[12] and training.[13] Smartphones began incorporating AI accelerators starting with the Qualcomm Snapdragon 820 in 2015.[14][15]

Heterogeneous computing[edit]
Heterogeneous computing refers to incorporating a number of specialized processors in a single system, or even a single chip, each optimized for a specific type of task. Architectures such as the Cell microprocessor[16] have features significantly overlapping with AI accelerators including: support for packed low precision arithmetic, dataflow architecture, and prioritizing 'throughput' over latency. The Cell microprocessor was subsequently applied to a number of tasks[17][18][19] including AI.[20][21][22]
In the 2000s, CPUs also gained increasingly wide SIMD units, driven by video and gaming workloads; as well as support for packed low-precision data types.[23] Due to increasing performance of CPUs, they are also being used for running AI workloads. CPUs are superior for DNNs with DNNs with small or medium-scale parallelism, for sparse DNNs and in low-batch-size scenarios.

Use of GPU[edit]
Graphics processing units or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and image manipulation are similar, embarrassingly parallel tasks involving matrices, leading GPUs to become increasingly used for machine learning tasks.[24][25][26] As of 2016[update], GPUs are popular for AI work, and they continue to evolve in a direction to facilitate deep learning, both for training[27] and inference in devices such as self-driving cars.[28] GPU developers such as Nvidia NVLink are developing additional connective capability for the kind of dataflow workloads AI benefits from.[29] As GPUs have been increasingly applied to AI acceleration, GPU manufacturers have incorporated neural network-specific hardware to further accelerate these tasks.[30][31] Tensor cores are intended to speed up the training of neural networks.[31]

Use of FPGAs[edit]
Deep learning frameworks are still evolving, making it hard to design custom hardware. Reconfigurable devices such as field-programmable gate arrays (FPGA) make it easier to evolve hardware, frameworks, and software alongside each other.[32][12][13][33]
Microsoft has used FPGA chips to accelerate inference.[34]

Emergence of dedicated AI accelerator ASICs[edit]
While GPUs and FPGAs perform far better[quantify] than CPUs for AI related tasks, a factor of up to 10 in efficiency[35][36] may be gained with a more specific design, via an application-specific integrated circuit (ASIC).[citation needed] These accelerators employ strategies such as optimized memory use[citation needed] and the use of lower precision arithmetic to accelerate calculation and increase throughput of computation.[37][38] Some adopted low-precision floating-point formats used AI acceleration are half-precision and the bfloat16 floating-point format.[39][40][41][42][43][44][45] Companies such as Google, Qualcomm, Amazon, Apple, Facebook, AMD and Samsung are all designing their own AI ASICs.[46][47][48][49][50][51]

In-memory computing architectures[edit]
This section needs expansion. You can help by adding to it.  (October 2018)
In June 2017, IBM researchers announced an architecture in contrast to the Von Neumann architecture based on in-memory computing and phase-change memory arrays applied to temporal correlation detection, intending to generalize the approach to heterogeneous computing and massively parallel systems.[52] In October 2018, IBM researchers announced an architecture based on in-memory processing and modeled on the human brain's synaptic network to accelerate deep neural networks.[53] The system is based on phase-change memory arrays.[54]

In-memory computing with analog resistive memories[edit]
In 2019 researchers from Politecnico di Milano found a way to solve systems of linear equations in a few tens of nanoseconds via a single operation. Their algorithm is based on in-memory computing with analog resistive memories which performs with high efficiencies of time and energy, via conducting matrix-vector multiplication in one step using Ohm's law and Kirchhoff's law. The researchers showed that a feedback circuit with cross-point resistive memories can solve algebraic problems such as systems of linear equations, matrix eigenvectors, and differential equations in just one step. Such an approach improves computational times drastically in comparison with digital algorithms.[55]

Atomically thin semiconductors[edit]
In 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).[56] Such atomically thin semiconductors are considered promising for energy-efficient machine learning applications, where the same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting molybdenum disulfide.[56]

Integrated photonic tensor core[edit]
In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing.[57] The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds.[57] Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.[57]

Nomenclature[edit]
As of 2016, the field is still in flux and vendors are pushing their own marketing term for what amounts to an "AI accelerator", in the hope that their designs and APIs will become the dominant design. There is no consensus on the boundary between these devices, nor the exact form they will take; however several examples clearly aim to fill this new space, with a fair amount of overlap in capabilities.
In the past when consumer graphics accelerators emerged, the industry eventually adopted Nvidia's self-assigned term, "the GPU",[58] as the collective noun for "graphics accelerators", which had taken many forms before settling on an overall pipeline implementing a model presented by Direct3D.

Potential applications[edit]
Agricultural robots, for example herbicide-free weed control.[59]
Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application.[60]
Computer-aided diagnosis
Industrial robots, increasing the range of tasks that can be automated, by adding adaptability to variable situations.
Machine translation
Military robots
Natural-language processing
Search engines, increasing the energy efficiency of data centers and ability to use increasingly advanced queries.
Unmanned aerial vehicles, e.g. navigation systems, e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones.[61]
Voice user interface, e.g. in mobile phones, a target for Qualcomm Zeroth.[62]
See also[edit]
Cognitive computer
Deep learning processor
Neuromorphic computing
Optical neural network
Physical neural network
References[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}"Intel unveils Movidius Compute Stick USB AI Accelerator". July 21, 2017. Archived from the original on August 11, 2017. Retrieved August 11, 2017.

^ "Inspurs unveils GX4 AI Accelerator". June 21, 2017.

^ Wiggers, Kyle (November 6, 2019) [2019], Neural Magic raises $15 million to boost AI inferencing speed on off-the-shelf processors, archived from the original on March 6, 2020, retrieved March 14, 2020

^ "Google Designing AI Processors". Google using its own AI accelerators.

^ "13 Sextillion & Counting: The Long & Winding Road to the Most Frequently Manufactured Human Artifact in History". Computer History Museum. April 2, 2018. Retrieved July 28, 2019.

^ John C. Dvorak: Intelâs 80170 chip has the theoretical intelligence of a cockroach in PC Magazine Volume 9 Number 10 (May 1990), p. 77, [1], retrieved May 16, 2021

^ "Application of the ANNA Neural Network Chip to High-Speed Character Recognition" (PDF).

^ "convolutional neural network demo from 1993 featuring DSP32 accelerator".

^ "design of a connectionist network supercomputer".

^ "The end of general purpose computers (not)".This presentation covers a past attempt at neural net accelerators, notes the similarity to the modern SLI GPGPU processor setup, and argues that general purpose vector accelerators are the way forward (in relation to RISC-V hwacha project. Argues that NN's are just dense and sparse matrices, one of several recurring algorithms)

^ Ramacher, U.; Raab, W.; Hachmann, J.A.U.; Beichter, J.; Bruls, N.; Wesseling, M.; Sicheneder, E.; Glass, J.; Wurz, A.; Manner, R. (1995). Proceedings of 9th International Parallel Processing Symposium. pp.Â 774â781. CiteSeerXÂ 10.1.1.27.6410. doi:10.1109/IPPS.1995.395862. ISBNÂ 978-0-8186-7074-9. S2CIDÂ 16364797.

^ Jump up to: a b "Space Efficient Neural Net Implementation".

^ Jump up to: a b Gschwind, M.; Salapura, V.; Maischberger, O. (1996). "A Generic Building Block for Hopfield Neural Networks with On-Chip Learning". 1996 IEEE International Symposium on Circuits and Systems. Circuits and Systems Connecting the World. ISCAS 96. pp.Â 49â52. doi:10.1109/ISCAS.1996.598474. ISBNÂ 0-7803-3073-0. S2CIDÂ 17630664.

^ "Qualcomm Helps Make Your Mobile Devices Smarter With New Snapdragon Machine Learning Software Development Kit". Qualcomm.{{cite web}}:  CS1 maint: url-status (link)

^ Rubin, Ben Fox. "Qualcomm's Zeroth platform could make your smartphone much smarter". CNET. Retrieved September 28, 2021.

^ Gschwind, Michael; Hofstee, H. Peter; Flachs, Brian; Hopkins, Martin; Watanabe, Yukio; Yamazaki, Takeshi (2006). "Synergistic Processing in Cell's Multicore Architecture". IEEE Micro. 26 (2): 10â24. doi:10.1109/MM.2006.41. S2CIDÂ 17834015.

^ De Fabritiis, G. (2007). "Performance of Cell processor for biomolecular simulations". Computer Physics Communications. 176 (11â12): 660â664. arXiv:physics/0611201. doi:10.1016/j.cpc.2007.02.107. S2CIDÂ 13871063.

^ Video Processing and Retrieval on Cell architecture. CiteSeerXÂ 10.1.1.138.5133.

^ Benthin, Carsten; Wald, Ingo; Scherbaum, Michael; Friedrich, Heiko (2006). 2006 IEEE Symposium on Interactive Ray Tracing. pp.Â 15â23. CiteSeerXÂ 10.1.1.67.8982. doi:10.1109/RT.2006.280210. ISBNÂ 978-1-4244-0693-7. S2CIDÂ 1198101.

^ "Development of an artificial neural network on a heterogeneous multicore architecture to predict a successful weight loss in obese individuals" (PDF).

^ Kwon, Bomjun; Choi, Taiho; Chung, Heejin; Kim, Geonho (2008). 2008 5th IEEE Consumer Communications and Networking Conference. pp.Â 1030â1034. doi:10.1109/ccnc08.2007.235. ISBNÂ 978-1-4244-1457-4. S2CIDÂ 14429828.

^ Duan, Rubing; Strey, Alfred (2008). Euro-Par 2008 â Parallel Processing. Lecture Notes in Computer Science. Vol.Â 5168. pp.Â 665â675. doi:10.1007/978-3-540-85451-7_71. ISBNÂ 978-3-540-85450-0.

^ "Improving the performance of video with AVX". February 8, 2012.

^ "microsoft research/pixel shaders/MNIST".

^ "How GPU came to be used for general computation".

^ "ImageNet Classification with Deep Convolutional Neural Networks" (PDF).

^ "nvidia driving the development of deep learning". May 17, 2016.

^ "Nvidia introduces supercomputer for self driving cars". January 6, 2016.

^ "how nvlink will enable faster easier multi GPU computing". November 14, 2014.

^ "A Survey on Optimized Implementation of Deep Learning Models on the NVIDIA Jetson Platform", 2019

^ Jump up to: a b Harris, Mark (May 11, 2017). "CUDA 9 Features Revealed: Volta, Cooperative Groups and More". Retrieved August 12, 2017.

^ Sefat, Md Syadus; Aslan, Semih; Kellington, Jeffrey W; Qasem, Apan (August 2019). "Accelerating HotSpots in Deep Neural Networks on a CAPI-Based FPGA". 2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS): 248â256. doi:10.1109/HPCC/SmartCity/DSS.2019.00048. ISBNÂ 978-1-7281-2058-4. S2CIDÂ 203656070.

^ "FPGA Based Deep Learning Accelerators Take on ASICs". The Next Platform. August 23, 2016. Retrieved September 7, 2016.

^ "Project Brainwave". Microsoft Research. Retrieved June 16, 2020.

^ "Google boosts machine learning with its Tensor Processing Unit". May 19, 2016. Retrieved September 13, 2016.

^ "Chip could bring deep learning to mobile devices". www.sciencedaily.com. February 3, 2016. Retrieved September 13, 2016.

^ "Deep Learning with Limited Numerical Precision" (PDF).

^ Rastegari, Mohammad; Ordonez, Vicente; Redmon, Joseph; Farhadi, Ali (2016). "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks". arXiv:1603.05279 [cs.CV].

^ Khari Johnson (May 23, 2018). "Intel unveils Nervana Neural Net L-1000 for accelerated AI training". VentureBeat. Retrieved May 23, 2018. ...Intel will be extending bfloat16 support across our AI product lines, including Intel Xeon processors and Intel FPGAs.

^ Michael Feldman (May 23, 2018). "Intel Lays Out New Roadmap for AI Portfolio". TOP500 Supercomputer Sites. Retrieved May 23, 2018. Intel plans to support this format across all their AI products, including the Xeon and FPGA lines

^ Lucian Armasu (May 23, 2018). "Intel To Launch Spring Crest, Its First Neural Network Processor, In 2019". Tom's Hardware. Retrieved May 23, 2018. Intel said that the NNP-L1000 would also support bfloat16, a numerical format thatâs being adopted by all the ML industry players for neural networks. The company will also support bfloat16 in its FPGAs, Xeons, and other ML products. The Nervana NNP-L1000 is scheduled for release in 2019.

^ "Available TensorFlow Ops | Cloud TPU | Google Cloud". Google Cloud. Retrieved May 23, 2018. This page lists the TensorFlow Python APIs and graph operators available on Cloud TPU.

^ Elmar HauÃmann (April 26, 2018). "Comparing Google's TPUv2 against Nvidia's V100 on ResNet-50". RiseML Blog. Archived from the original on April 26, 2018. Retrieved May 23, 2018. For the Cloud TPU, Google recommended we use the bfloat16 implementation from the official TPU repository with TensorFlow 1.7.0. Both the TPU and GPU implementations make use of mixed-precision computation on the respective architecture and store most tensors with half-precision.

^ Tensorflow Authors (February 28, 2018). "ResNet-50 using BFloat16 on TPU". Google. Retrieved May 23, 2018.[permanent dead link]

^ Joshua V. Dillon; Ian Langmore; Dustin Tran; Eugene Brevdo; Srinivas Vasudevan; Dave Moore; Brian Patton; Alex Alemi; Matt Hoffman; Rif A. Saurous (November 28, 2017). TensorFlow Distributions (Report). arXiv:1711.10604. Bibcode:2017arXiv171110604D. Accessed May 23, 2018. All operations in TensorFlow Distributions are numerically stable across half, single, and double floating-point precisions (as TensorFlow dtypes: tf.bfloat16 (truncated floating point), tf.float16, tf.float32, tf.float64). Class constructors have a validate_args flag for numerical asserts

^ "Google Reveals a Powerful New AI Chip and Supercomputer". MIT Technology Review. Retrieved July 27, 2021.

^ "What to Expect From Apple's Neural Engine in the A11 Bionic SoC â ExtremeTech". www.extremetech.com. Retrieved July 27, 2021.

^ "Facebook has a new job posting calling for chip designers".

^ "Facebook joins Amazon and Google in AI chip race". www.ft.com.

^ Amadeo, Ron (May 11, 2021). "Samsung and AMD will reportedly take on Apple's M1 SoC later this year". Ars Technica. Retrieved July 28, 2021.

^ Smith, Ryan. "The AI Race Expands: Qualcomm Reveals "Cloud AI 100" Family of Datacenter AI Inference Accelerators for 2020". www.anandtech.com. Retrieved September 28, 2021.

^ Abu Sebastian; Tomas Tuma; Nikolaos Papandreou; Manuel Le Gallo; Lukas Kull; Thomas Parnell; Evangelos Eleftheriou (2017). "Temporal correlation detection using computational phase-change memory". Nature Communications. 8 (1): 1115. arXiv:1706.00511. doi:10.1038/s41467-017-01481-9. PMCÂ 5653661. PMIDÂ 29062022.

^ "A new brain-inspired architecture could improve how computers handle data and advance AI". American Institute of Physics. October 3, 2018. Retrieved October 5, 2018.

^ Carlos RÃ­os; Nathan Youngblood; Zengguang Cheng; Manuel Le Gallo; Wolfram H.P. Pernice; C. David Wright; Abu Sebastian; Harish Bhaskaran (2018). "In-memory computing on a photonic platform". arXiv:1801.06228 [cs.ET].

^ Zhong Sun; Giacomo Pedretti; Elia Ambrosi; Alessandro Bricalli; Wei Wang; Daniele Ielmini (2019). "Solving matrix equations in one step with cross-point resistive arrays". Proceedings of the National Academy of Sciences. 116 (10): 4123â4128. doi:10.1073/pnas.1815682116. PMCÂ 6410822. PMIDÂ 30782810.

^ Jump up to: a b Marega, Guilherme Migliato; Zhao, Yanfei; Avsar, Ahmet; Wang, Zhenyu; Tripati, Mukesh; Radenovic, Aleksandra; Kis, Anras (2020). "Logic-in-memory based on an atomically thin semiconductor". Nature. 587 (2): 72â77. doi:10.1038/s41586-020-2861-0. PMCÂ 7116757. PMIDÂ 33149289.

^ Jump up to: a b c Feldmann, J.; Youngblood, N.; Karpov, M.;  etÂ al. (2021). "Parallel convolutional processing using an integrated photonic tensor". Nature. 589 (2): 52â58. arXiv:2002.00281. doi:10.1038/s41586-020-03070-1. PMIDÂ 33408373. S2CIDÂ 211010976.

^ "NVIDIA launches the World's First Graphics Processing Unit, the GeForce 256".

^ "Design of a machine vision system for weed control". CiteSeerXÂ 10.1.1.7.342. Archived (PDF) from the original on June 23, 2010. Retrieved July 29, 2021. {{cite journal}}: Cite journal requires |journal= (help)

^ "Self-Driving Cars Technology & Solutions from NVIDIA Automotive". NVIDIA.

^ "movidius powers worlds most intelligent drone". March 16, 2016.

^ "Qualcomm Research brings server class machine learning to everyday devicesâmaking them smarter [VIDEO]". October 2015.


External links[edit]
Nvidia Puts The Accelerator To The Metal With Pascal.htm, The Next Platform
Eyeriss Project, MIT
https://alphaics.ai/
.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vteHardware accelerationTheory
Universal Turing machine
Parallel computing
Distributed computing
Applications
GPU
GPGPU
DirectX
Audio
Digital signal processing
Hardware random number generation
Artificial intelligence
Cryptography
TLS
Machine vision
Custom hardware attack
scrypt
Networking
Data
Implementations
High-level synthesis
C to HDL
FPGA
ASIC
CPLD
System on a chip
Network on a chip
Architectures
Data flow
Transport triggered
Multicore
Manycore
Heterogeneous
In-memory computing
Systolic array
Neuromorphic
Related
Programmable logic
Processor
design
chronology
Digital electronics
Virtualization
Hardware emulation
Logic synthesis
Embedded systems





<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=AI_accelerator&oldid=1069373653"
		Categories: Application-specific integrated circuitsAI acceleratorsCoprocessorsComputer optimizationGate arraysHidden categories: CS1 maint: url-statusAll articles with dead external linksArticles with dead external links from April 2019Articles with permanently dead external linksCS1 errors: missing periodicalArticles with short descriptionShort description is different from WikidataUse American English from January 2019All Wikipedia articles written in American EnglishUse mdy dates from October 2021Articles to be merged from October 2021All articles to be mergedArticles containing potentially dated statements from 2018All articles containing potentially dated statementsArticles containing potentially dated statements from 2016All articles with unsourced statementsArticles with unsourced statements from October 2018Articles with unsourced statements from November 2017Articles to be expanded from October 2018All articles to be expandedArticles using small message boxes
	
