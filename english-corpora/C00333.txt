
Title:
Memory hierarchy
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		  Diagram of the computer memory hierarchy
.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}Computer memory and data storage types
General
Memory cell
Memory coherence
Cache coherence
Memory hierarchy
Memory access pattern
Memory map
Secondary storage
MOS memory
floating-gate
Continuous availability
Areal density (computer storage)
Block (data storage)
Object storage
Direct-attached storage
Network-attached storage
Storage area network
Block-level storage
Single-instance storage
Data
Structured data
Unstructured data
Big data
Metadata
Data compression
Data corruption
Data cleansing
Data degradation
Data integrity
Data security
Data validation
Data validation and reconciliation
Data recovery
Storage
Data cluster
Directory
Shared resource
File sharing
File system
Clustered file system
Distributed file system
Distributed file system for cloud
Distributed data store
Distributed database
Database
Data bank
Data storage
Data store
Data deduplication
Data structure
Data redundancy
Replication (computing)
Memory refresh
Storage record
Information repository
Knowledge base
Computer file
Object file
File deletion
File copying
Backup
Core dump
Hex dump
Data transmission
Information transfer
Temporary file
Copy protection
Digital rights management
Volume (computing)
Boot sector
Master boot record
Volume boot record
Disk array
Disk image
Disk mirroring
Disk aggregation
Disk partitioning
Memory segmentation
Locality of reference
Logical disk
Storage virtualization
Virtual memory
Memory-mapped file
Software entropy
Software rot
In-memory database
In-memory processing
Persistence (computer science)
Persistent data structure
RAID
Non-RAID drive architectures
Memory paging
Bank switching
Grid computing
Cloud computing
Cloud storage
Fog computing
Edge computing
Dew computing
Amdahl's law
Moore's law
Kryder's law

Volatile
RAM
Hardware cache
CPU cache
Scratchpad memory
DRAM
eDRAM
SDRAM
SGRAM
LPDDR
QDRSRAM
EDO DRAM
XDR DRAM
RDRAM
SDRAM
DDR
GDDR
HBM
SRAM
1T-SRAM
ReRAM
QRAM
Content-addressable memory (CAM)
VRAM
Dual-ported RAM
Video RAM (dual-ported DRAM)

Historical
WilliamsâKilburn tube (1946â47)
Delay-line memory (1947)
Mellon optical memory (1951)
Selectron tube (1952)
Dekatron
T-RAM (2009)
Z-RAM (2002â2010)

Non-volatile
ROM
MROM
PROM
EPROM
EEPROM
ROM cartridge
Solid-state storage (SSS)
Flash memory is used in:
Solid-state drive (SSD)
Solid-state hybrid drive (SSHD)
USB flash drive
IBM FlashSystem
Flash Core Module
Memory card
Memory Stick
CompactFlash
PC Card
MultiMediaCard
SD card
SIM card
SmartMedia
Universal Flash Storage
SxS
MicroP2
XQD card
Programmable metallization cell

NVRAM
Memistor
Memristor
PCM (3D XPoint)
MRAM
Electrochemical RAM (ECRAM)
Nano-RAM
CBRAM

Early stage NVRAM
FeRAM
ReRAM
FeFET memory

Analog recording
Phonograph cylinder
Phonograph record
Quadruplex videotape
Vision Electronic Recording Apparatus
Magnetic recording
Magnetic storage
Magnetic tape
Magnetic tape data storage
Tape drive
Tape library
Digital Data Storage (DDS)
Videotape
Videocassette
Cassette tape
Linear Tape-Open
Betamax
8 mm video format
DV
MiniDV
MicroMV
U-matic
VHS
S-VHS
VHS-C
D-VHS
Hard disk drive

Optical
3D optical data storage
Optical disc
LaserDisc
Compact Disc Digital Audio (CDDA)
CD
CD Video
CD-R
CD-RW
Video CD
Super Video CD
Mini CD
Nintendo optical discs
CD-ROM
Hyper CD-ROM
DVD
DVD+R
DVD-Video
DVD card
DVD-RAM
MiniDVD
HD DVD
Blu-ray
Ultra HD Blu-ray
Holographic Versatile Disc
WORM

In development
CBRAM
Racetrack memory
NRAM
Millipede memory
ECRAM
Patterned media
Holographic data storage
Electronic quantum holography
5D optical data storage
DNA digital data storage
Universal memory
Time crystal
Quantum memory

Historical
Paper data storage (1725)
Punched card (1725)
Punched tape (1725)
Plugboard
Delay-line memory
Drum memory (1932)
Magnetic-core memory (1949)
Plated wire memory (1957)
Core rope memory (1960s)
Thin-film memory (1962)
Disk pack (1962)
Twistor memory (~1968)
Bubble memory (~1970)
Floppy disk (1971)
.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vte
In computer architecture, the memory hierarchy separates computer storage into a hierarchy based on response time. Since response time, complexity, and capacity are related, the levels may also be distinguished by their performance and controlling technologies.[1] Memory hierarchy affects performance in computer architectural design, algorithm predictions, and lower level programming constructs involving locality of reference. 
Designing for high performance requires considering the restrictions of the memory hierarchy, i.e. the size and capabilities of each component. Each of the various components can be viewed as part of a hierarchy of memories (m1, m2, ..., mn) in which each member mi is typically smaller and faster than the next highest member mi+1 of the hierarchy. To limit waiting by higher levels, a lower level will respond by filling a buffer and then signaling for activating the transfer.
There are four major storage levels.[1]

Internal â Processor registers and cache.
Main â the system RAM and controller cards.
On-line mass storage â Secondary storage.
Off-line bulk storage â Tertiary and Off-line storage.
This is a general memory hierarchy structuring. Many other structures are useful.  For example, a paging algorithm may be considered as a level for virtual memory when designing a computer architecture, and one can include a level of nearline storage between online and offline storage.

Contents

1 Properties of the technologies in the memory hierarchy
2 Examples
3 See also
4 References



Properties of the technologies in the memory hierarchy[edit]
Adding complexity slows down the memory hierarchy.[2]
CMOx memory technology stretches the Flash space in the memory hierarchy[3]
One of the main ways to increase system performance is minimising how far down the memory hierarchy one has to go to manipulate data.[4]
Latency and bandwidth are two metrics associated with caches. Neither of them is uniform, but is specific to a particular component of the memory hierarchy.[5]
Predicting where in the memory hierarchy the data resides is difficult.[5]
...the location in the memory hierarchy dictates the time required for the prefetch to occur.[5]
Examples[edit]
  Memory hierarchy of an AMD Bulldozer server.
The number of levels in the memory hierarchy and the performance at each level has increased over time. The type of memory or storage components also change historically.[6]  For example, the memory hierarchy of an Intel Haswell Mobile[7] processor circa 2013 is:

Processor registers â the fastest possible access (usually 1 CPU cycle). A few thousand bytes in size
Cache
Level 0 (L0) Micro operations cache â 6,144 bytes (6 KiB[citation needed][original research])[8] in size
Level 1 (L1) Instruction cache â 128 KiB[citation needed][original research] in size
Level 1 (L1) Data cache â 128 KiB[citation needed][original research] in size. Best access speed is around 700 GB/s[9]
Level 2 (L2) Instruction and data (shared) â 1 MiB[citation needed][original research] in size. Best access speed is around 200 GB/s[9]
Level 3 (L3) Shared cache â 6 MiB[citation needed][original research] in size. Best access speed is around 100 GB/s[9]
Level 4 (L4) Shared cache â 128 MiB[citation needed][original research] in size. Best access speed is around 40 GB/s[9]
Main memory (Primary storage) â GiB[citation needed][original research] in size. Best access speed is around 10 GB/s.[9] In the case of a NUMA machine, access times may not be uniform
Disk storage (Secondary storage) â Terabytes in size. As of 2017, best access speed is from a consumer solid state drive is about 2000 MB/s[10]
Nearline storage (Tertiary storage) â Up to exabytes in size.  As of 2013, best access speed is about 160 MB/s[11]
Offline storage
The lower levels of the hierarchy â from disks downwards â are also known as tiered storage. The formal distinction between online, nearline, and offline storage is:[12]

Online storage is immediately available for I/O.
Nearline storage is not immediately available, but can be made online quickly without human intervention.
Offline storage is not immediately available, and requires some human intervention to bring online.
For example, always-on spinning disks are online, while spinning disks that spin-down, such as massive array of idle disk (MAID), are nearline. Removable media such as tape cartridges that can be automatically loaded, as in a tape library, are nearline, while cartridges that must be manually loaded are offline.
Most modern CPUs are so fast that for most program workloads, the bottleneck is the locality of reference of memory accesses and the efficiency of the caching and memory transfer between different levels of the hierarchy[citation needed]. As a result, the CPU spends much of its time idling, waiting for memory I/O to complete.  This is sometimes called the space cost, as a larger memory object is more likely to overflow a small/fast level and require use of a larger/slower level. The resulting load on memory use is known as pressure (respectively register pressure, cache pressure, and (main) memory pressure). Terms for data being missing from a higher level and needing to be fetched from a lower level are, respectively: register spilling (due to register pressure: register to cache), cache miss (cache to main memory), and (hard) page fault (main memory to disk).
Modern programming languages mainly assume two levels of memory, main memory and disk storage, though in assembly language and inline assemblers in languages such as C, registers can be directly accessed. Taking optimal advantage of the memory hierarchy requires the cooperation of programmers, hardware, and compilers (as well as underlying support from the operating system):

Programmers are responsible for moving data between disk and memory through file I/O.
Hardware is responsible for moving data between memory and caches.
Optimizing compilers are responsible for generating code that, when executed, will cause the hardware to use caches and registers efficiently.
Many programmers assume one level of memory.  This works fine until the application hits a performance wall.  Then the memory hierarchy will be assessed during code refactoring.

See also[edit]
Cache hierarchy
Use of spatial and temporal locality: hierarchical memory
Buffer vs. cache
Cache hierarchy in a modern processor
Memory wall
Computer memory
Hierarchical storage management
Cloud storage
Memory access pattern
Communication-avoiding algorithm
References[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ Jump up to: a b .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Toy, Wing; Zee, Benjamin (1986). Computer Hardware/Software Architecture. Prentice Hall. p.Â 30. ISBNÂ 0-13-163502-6.

^ Write-combining

^ "Memory Hierarchy". Unitity Semiconductor Corporation. Archived from the original on 5 August 2009. Retrieved 16 September 2009.

^ PÃ¡draig Brady. "Multi-Core". Retrieved 16 September 2009.

^ Jump up to: a b c van der Pas, Ruud (2002). "Memory Hierarchy in Cache-Based Systems" (PDF). Santa Clara, California: Sun Microsystems: 26. 817-0742-10. {{cite journal}}: Cite journal requires |journal= (help)

^ "Memory & Storage - Timeline of Computer History - Computer History Museum". www.computerhistory.org.

^ Crothers, Brooke. "Dissecting Intel's top graphics in Apple's 15-inch MacBook Pro - CNET". News.cnet.com. Retrieved 2014-07-31.

^ "Intel's Haswell Architecture Analyzed: Building a New PC and a New Intel". AnandTech. Retrieved 2014-07-31.

^ Jump up to: a b c d e "SiSoftware Zone". Sisoftware.co.uk. Archived from the original on 2014-09-13. Retrieved 2014-07-31.

^ "Samsung 960 Pro M.2 NVMe SSD Review". storagereview.com. Retrieved 2017-04-13.

^ "Ultrium - LTO Technology - Ultrium GenerationsLTO". Lto.org. Archived from the original on 2011-07-27. Retrieved 2014-07-31.

^ Pearson, Tony (2010). "Correct use of the term Nearline". IBM Developerworks, Inside System Storage. Archived from the original on 2018-11-27. Retrieved 2015-08-16.






<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=Memory_hierarchy&oldid=1065999585"
		Categories: Computer architectureComputer data storageHierarchyHidden categories: CS1 errors: missing periodicalAll articles with unsourced statementsArticles with unsourced statements from May 2021All articles that may contain original researchArticles that may contain original research from May 2021Articles with unsourced statements from September 2009
	
