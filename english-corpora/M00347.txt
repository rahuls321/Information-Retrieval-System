The rank–nullity theorem is a theorem in linear algebra, which asserts that the dimension of the domain of a linear map is the sum of its rank (the dimension of its image) and its nullity (the dimension of its kernel). Stating the theorem Let                     T        :        V        →        W              {\displaystyle T:V\to W}   be a linear transformation between two vector spaces where                     T              {\displaystyle T}  's domain                     V              {\displaystyle V}   is finite dimensional. ThenwhereIn other words,This theorem can be refined via the splitting lemma to be a statement about an isomorphism of spaces, not just dimensions. Explicitly, since T induces an isomorphism from                     V                  /                Ker        ⁡        (        T        )              {\displaystyle V/\operatorname {Ker} (T)}   to                     Image        ⁡        (        T        )        ,              {\displaystyle \operatorname {Image} (T),}   the existence of a basis for V that extends any given basis of                     Ker        ⁡        (        T        )              {\displaystyle \operatorname {Ker} (T)}   implies, via the splitting lemma, that                     Image        ⁡        (        T        )        ⊕        Ker        ⁡        (        T        )        ≅        V        .              {\displaystyle \operatorname {Image} (T)\oplus \operatorname {Ker} (T)\cong V.}   Taking dimensions, the rank–nullity theorem follows.= Matrices =Since                               Mat                      m            ×            n                          ⁡        (                  F                )        ≅        Hom        ⁡                  (                                                    F                                            n                                      ,                                          F                                            m                                              )                ,              {\displaystyle \operatorname {Mat} _{m\times n}(\mathbb {F} )\cong \operatorname {Hom} \left(\mathbb {F} ^{n},\mathbb {F} ^{m}\right),}   matrices immediately come to mind when discussing linear maps. In the case of an                     m        ×        n              {\displaystyle m\times n}   matrix, the dimension of the domain is                     n        ,              {\displaystyle n,}   the number of columns in the matrix. Thus the rank–nullity theorem for a given matrix                     M        ∈                  Mat                      m            ×            n                          ⁡        (                  F                )              {\displaystyle M\in \operatorname {Mat} _{m\times n}(\mathbb {F} )}   immediately becomes Proofs Here we provide two proofs. The first operates in the general case, using linear maps. The second proof looks at the homogeneous system                               A          x                =                  0                      {\displaystyle \mathbf {Ax} =\mathbf {0} }   for                               A                ∈                  Mat                      m            ×            n                          ⁡        (                  F                )              {\displaystyle \mathbf {A} \in \operatorname {Mat} _{m\times n}(\mathbb {F} )}   with rank                     r              {\displaystyle r}   and shows explicitly that there exists a set of                     n        −        r              {\displaystyle n-r}   linearly independent solutions that span the kernel of                               A                      {\displaystyle \mathbf {A} }  .While the theorem requires that the domain of the linear map be finite-dimensional, there is no such assumption on the codomain. This means that there are linear maps not given by matrices for which the theorem applies. Despite this, the first proof is not actually more general than the second: since the image of the linear map is finite-dimensional, we can represent the map from its domain to its image by a matrix, prove the theorem for that matrix, then compose with the inclusion of the image into the full codomain.= First proof =Let                     V        ,        W              {\displaystyle V,W}   be vector spaces over some field                               F                      {\displaystyle \mathbb {F} }   and                     T              {\displaystyle T}   defined as in the statement of the theorem with                     dim        ⁡        V        =        n              {\displaystyle \dim V=n}  .As                     Ker        ⁡        T        ⊂        V              {\displaystyle \operatorname {Ker} T\subset V}   is a subspace, there exists a basis for it. Suppose                     dim        ⁡        Ker        ⁡        T        =        k              {\displaystyle \dim \operatorname {Ker} T=k}   and letbe such a basis.We may now, by the Steinitz exchange lemma, extend                                           K                                {\displaystyle {\mathcal {K}}}   with                     n        −        k              {\displaystyle n-k}   linearly independent vectors                               w                      1                          ,        …        ,                  w                      n            −            k                                {\displaystyle w_{1},\ldots ,w_{n-k}}   to form a full basis of                     V              {\displaystyle V}  .Letsuch thatis a basis for                     V              {\displaystyle V}  .From this, we know thatWe now claim that                     T        (                              S                          )              {\displaystyle T({\mathcal {S}})}   is a basis for                     Im        ⁡        T              {\displaystyle \operatorname {Im} T}  .The above equality already states that                     T        (                              S                          )              {\displaystyle T({\mathcal {S}})}   is a generating set for                     Im        ⁡        T              {\displaystyle \operatorname {Im} T}  ; it remains to be shown that it is also linearly independent to conclude that it is a basis.Suppose                     T        (                              S                          )              {\displaystyle T({\mathcal {S}})}   is not linearly independent, and letfor some                               α                      j                          ∈                  F                      {\displaystyle \alpha _{j}\in \mathbb {F} }  .Thus, owing to the linearity of                     T              {\displaystyle T}  , it follows thatThis is a contradiction to                                           B                                {\displaystyle {\mathcal {B}}}   being a basis, unless all                               α                      j                                {\displaystyle \alpha _{j}}   are equal to zero. This shows that                     T        (                              S                          )              {\displaystyle T({\mathcal {S}})}   is linearly independent, and more specifically that it is a basis for                     Im        ⁡        T              {\displaystyle \operatorname {Im} T}  .To summarize, we have                                           K                                {\displaystyle {\mathcal {K}}}  , a basis for                     Ker        ⁡        T              {\displaystyle \operatorname {Ker} T}  , and                     T        (                              S                          )              {\displaystyle T({\mathcal {S}})}  , a basis for                     Im        ⁡        T              {\displaystyle \operatorname {Im} T}  .Finally we may state thatThis concludes our proof.= Second proof =Let                               A                ∈                  Mat                      m            ×            n                          ⁡        (                  F                )              {\displaystyle \mathbf {A} \in \operatorname {Mat} _{m\times n}(\mathbb {F} )}   with                     r              {\displaystyle r}   linearly independent columns (i.e.                     Rank        ⁡        (                  A                )        =        r              {\displaystyle \operatorname {Rank} (\mathbf {A} )=r}  ). We will show that:To do this, we will produce a matrix                               X                ∈                  Mat                      n            ×            (            n            −            r            )                          ⁡        (                  F                )              {\displaystyle \mathbf {X} \in \operatorname {Mat} _{n\times (n-r)}(\mathbb {F} )}   whose columns form a basis of the null space of                               A                      {\displaystyle \mathbf {A} }  .Without loss of generality, assume that the first                     r              {\displaystyle r}   columns of                               A                      {\displaystyle \mathbf {A} }   are linearly independent. So, we can writewhere                                          A                                1                          ∈                  Mat                      m            ×            r                          ⁡        (                  F                )              {\displaystyle \mathbf {A} _{1}\in \operatorname {Mat} _{m\times r}(\mathbb {F} )}   with                     r              {\displaystyle r}   linearly independent column vectors, and                                          A                                2                          ∈                  Mat                      m            ×            (            n            −            r            )                          ⁡        (                  F                )              {\displaystyle \mathbf {A} _{2}\in \operatorname {Mat} _{m\times (n-r)}(\mathbb {F} )}  , each of whose                     n        −        r              {\displaystyle n-r}   columns are linear combinations of the columns of                                           A                                1                                {\displaystyle \mathbf {A} _{1}}  .This means that                                           A                                2                          =                              A                                1                                    B                      {\displaystyle \mathbf {A} _{2}=\mathbf {A} _{1}\mathbf {B} }   for some                               B                ∈                  Mat                      r            ×            (            n            −            r            )                                {\displaystyle \mathbf {B} \in \operatorname {Mat} _{r\times (n-r)}}   (see rank factorization) and, hence,Letwhere                                           I                                n            −            r                                {\displaystyle \mathbf {I} _{n-r}}   is the                     (        n        −        r        )        ×        (        n        −        r        )              {\displaystyle (n-r)\times (n-r)}   identity matrix. We note that                               X                ∈                  Mat                      n            ×            (            n            −            r            )                          ⁡        (                  F                )              {\displaystyle \mathbf {X} \in \operatorname {Mat} _{n\times (n-r)}(\mathbb {F} )}   satisfiesTherefore, each of the                     n        −        r              {\displaystyle n-r}   columns of                               X                      {\displaystyle \mathbf {X} }   are particular solutions of                               A          x                =                              0                                                              F                                            m                                                          {\displaystyle \mathbf {Ax} =\mathbf {0} _{\mathbb {F} ^{m}}}  .Furthermore, the                     n        −        r              {\displaystyle n-r}   columns of                               X                      {\displaystyle \mathbf {X} }   are linearly independent because                               X          u                =                              0                                                              F                                            n                                                          {\displaystyle \mathbf {Xu} =\mathbf {0} _{\mathbb {F} ^{n}}}   will imply                               u                =                              0                                                              F                                            n                −                r                                                          {\displaystyle \mathbf {u} =\mathbf {0} _{\mathbb {F} ^{n-r}}}   for                               u                ∈                              F                                n            −            r                                {\displaystyle \mathbf {u} \in \mathbb {F} ^{n-r}}  :Therefore, the column vectors of                               X                      {\displaystyle \mathbf {X} }   constitute a set of                     n        −        r              {\displaystyle n-r}   linearly independent solutions for                               A          x                =                              0                                                              F                                            m                                                          {\displaystyle \mathbf {Ax} =\mathbf {0} _{\mathbb {F} ^{m}}}  .We next prove that any solution of                               A          x                =                              0                                                              F                                            m                                                          {\displaystyle \mathbf {Ax} =\mathbf {0} _{\mathbb {F} ^{m}}}   must be a linear combination of the columns of                               X                      {\displaystyle \mathbf {X} }  .For this, letbe any vector such that                               A          u                =                              0                                                              F                                            m                                                          {\displaystyle \mathbf {Au} =\mathbf {0} _{\mathbb {F} ^{m}}}  . Note that since the columns of                                           A                                1                                {\displaystyle \mathbf {A} _{1}}   are linearly independent,                                           A                                1                                    x                =                              0                                                              F                                            m                                                          {\displaystyle \mathbf {A} _{1}\mathbf {x} =\mathbf {0} _{\mathbb {F} ^{m}}}   implies                               x                =                              0                                                              F                                            r                                                          {\displaystyle \mathbf {x} =\mathbf {0} _{\mathbb {F} ^{r}}}  .Therefore,This proves that any vector                               u                      {\displaystyle \mathbf {u} }   that is a solution of                               A          x                =                  0                      {\displaystyle \mathbf {Ax} =\mathbf {0} }   must be a linear combination of the                     n        −        r              {\displaystyle n-r}   special solutions given by the columns of                               X                      {\displaystyle \mathbf {X} }  . And we have already seen that the columns of                               X                      {\displaystyle \mathbf {X} }   are linearly independent. Hence, the columns of                               X                      {\displaystyle \mathbf {X} }   constitute a basis for the null space of                               A                      {\displaystyle \mathbf {A} }  . Therefore, the nullity of                               A                      {\displaystyle \mathbf {A} }   is                     n        −        r              {\displaystyle n-r}  . Since                     r              {\displaystyle r}   equals rank of                               A                      {\displaystyle \mathbf {A} }  , it follows that                     Rank        ⁡        (                  A                )        +        Nullity        ⁡        (                  A                )        =        n              {\displaystyle \operatorname {Rank} (\mathbf {A} )+\operatorname {Nullity} (\mathbf {A} )=n}  . This concludes our proof. Reformulations and generalizations This theorem is a statement of the first isomorphism theorem of algebra for the case of vector spaces; it generalizes to the splitting lemma.In more modern language, the theorem can also be phrased as saying that each short exact sequence of vector spaces splits. Explicitly, given thatis a short exact sequence of vector spaces, then                     U        ⊕        R        ≅        V              {\displaystyle U\oplus R\cong V}  , henceHere R plays the role of im T and U is ker T, i.e.In the finite-dimensional case, this formulation is susceptible to a generalization: ifis an exact sequence of finite-dimensional vector spaces, thenThe rank–nullity theorem for finite-dimensional vector spaces may also be formulated in terms of the index of a linear map. The index of a linear map                     T        ∈        Hom        ⁡        (        V        ,        W        )              {\displaystyle T\in \operatorname {Hom} (V,W)}  , where                     V              {\displaystyle V}   and                     W              {\displaystyle W}   are finite-dimensional, is defined byIntuitively,                     dim        ⁡        Ker        ⁡        T              {\displaystyle \dim \operatorname {Ker} T}   is the number of independent solutions                     v              {\displaystyle v}   of the equation                     T        v        =        0              {\displaystyle Tv=0}  , and                     dim        ⁡        Coker        ⁡        T              {\displaystyle \dim \operatorname {Coker} T}   is the number of independent restrictions that have to be put on                     w              {\displaystyle w}   to make                     T        v        =        w              {\displaystyle Tv=w}   solvable. The rank–nullity theorem for finite-dimensional vector spaces is equivalent to the statementWe see that we can easily read off the index of the linear map                     T              {\displaystyle T}   from the involved spaces, without any need to analyze                     T              {\displaystyle T}   in detail. This effect also occurs in a much deeper result: the Atiyah–Singer index theorem states that the index of certain differential operators can be read off the geometry of the involved spaces. Citations  References Axler, Sheldon (2015). Linear Algebra Done Right. Undergraduate Texts in Mathematics (3rd ed.). Springer. ISBN 978-3-319-11079-0.Banerjee, Sudipto; Roy, Anindya (2014), Linear Algebra and Matrix Analysis for Statistics, Texts in Statistical Science (1st ed.), Chapman and Hall/CRC, ISBN 978-1420095388Friedberg, Stephen H.; Insel, Arnold J.; Spence, Lawrence E. (2014). Linear Algebra (4th ed.). Pearson Education. ISBN 978-0130084514.Meyer, Carl D. (2000), Matrix Analysis and Applied Linear Algebra, SIAM, ISBN 978-0-89871-454-8.Katznelson, Yitzhak; Katznelson, Yonatan R. (2008). A (Terse) Introduction to Linear Algebra. American Mathematical Society. ISBN 978-0-8218-4419-9.Valenza, Robert J. (1993) [1951]. Linear Algebra: An Introduction to Abstract Mathematics. Undergraduate Texts in Mathematics (3rd ed.). Springer. ISBN 3-540-94099-5.