
Title:
Deep learning super sampling
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Image upscaling technology by Nvidia
Deep learning super sampling (DLSS) is a machine-learning and spatial image upscaling technology developed by Nvidia and exclusive to its graphics cards for real-time use in select video games, using deep learning to upscale lower-resolution images to a higher resolution for display on higher-resolution computer monitors. Nvidia claims this technology upscales images with quality similar to that of rendering the image natively in higher resolution but with less computation done by the video card, allowing for higher graphical settings and frame rates for a given resolution.[1]
As of June 2021, this technology is available exclusively on GeForce RTX 20 and GeForce RTX 30 series GPUs.

Contents

1 History

1.1 Release history
1.2 Quality presets


2 Algorithm

2.1 DLSS 1.0
2.2 DLSS 2.0


3 Architecture
4 Anti-Aliasing
5 See also
6 References



History[edit]
Nvidia advertised DLSS as a key feature of the GeForce RTX 20 series GPUs when they launched in September 2018.[2] At that time, the results were limited to a few video games (namely Battlefield V[3] and Metro Exodus) because the algorithm had to be trained specifically on each game on which it was applied and the results were usually not as good as simple resolution upscaling.[4][5]
In 2019, the video game Control shipped with ray tracing and an improved version of DLSS, which did not use the Tensor Cores.[6][7]
In April 2020, Nvidia advertised and shipped with driver version 445.75 an improved version of DLSS named DLSS 2.0, which was available for a few existing games including Control and Wolfenstein: Youngblood, and would be available later for upcoming games. This time Nvidia said that it used the Tensor Cores again, and that the AI did not need to be trained specifically on each game.[2][8]
As of January 2022, DLSS 2.0 must still be included on a per-game basis by the game developers.

Release history[edit]



Release

Release date

Highlights


1.0
February 2019
First version, using AI and specifically trained for certain games, including Battlefield V and Metro Exodus[3]


2.0 (first iteration)
August 2019
First 2.0 version, also referenced as version 1.9, using an approximated AI of the in-progress version 2.0 running on the CUDA shader cores and specifically adapted for Control[6][2][9]


2.0 (second iteration)
April 2020
Second 2.0 version, using Tensor Cores again and trained generically[10]


Quality presets[edit]

Standard DLSS Presets[11]


Quality preset[a]

Scale factor[b]

Render scale[c]


Quality

1.50x

66.6%


Balanced

1.72x

58.0%


Performance

2.00x

50.0%


Ultra Performancesince v2.1

3.00x

33.3%

.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^  The algorithm does not necessarily need to be implemented using these presets; it is possible for the implementer to define custom input and output resolutions.

^  The linear scale factor used for upsampling the input resolution to the output resolution. For example, a scene rendered at 540p with a 2.00x scale factor would have an output resolution of 1080p.

^  The linear render scale, compared to the output resolution, that the technology uses to render scenes internally before upsampling. For example, a 1080p scene with a 50% render scale would have an internal resolution of 540p.


Algorithm[edit]
DLSS 1.0[edit]
Nvidia explained that DLSS 1.0 worked for each target game image by generating a "perfect frame" using traditional supersampling, then trained the neural network on these resulting images. On a second step, the model was trained to recognize aliased inputs on the initial result.[12][13]

DLSS 2.0[edit]
DLSS 2.0 works as follows:[14]

The neural network is trained by Nvidia using "ideal" images of video games of ultra-high resolution on supercomputers and low resolution images of the same games. The result is stored on the video card driver. It is said that Nvidia uses DGX-1 servers to perform the training of the network.[15]
The neural network stored on the driver compares the actual low resolution image with the reference and produces a full high resolution result. The inputs used by the trained neural network are the low resolution aliased images rendered by the game engine, and the low resolution motion vectors from the same images, also generated by the game engine. The motion vectors tell the network which direction objects in the scene are moving from frame to frame, in order to estimate what the next frame will look like.[16]
Architecture[edit]
DLSS is only available on GeForce RTX 20 and GeForce RTX 30 series GPUs, using dedicated AI accelerators called Tensor Cores.[16][17]
Tensor Cores are available since the Nvidia Volta  GPU microarchitecture, which was first used on the Tesla V100 line of products.[18] Their specificity is that each Tensor Core operates on 16 bits floating point 4 x 4 matrices, and seem to be designed to be used at the CUDA C++ level, even at the compiler level.[19]
The Tensor Cores use CUDA Warp-Level Primitives on 32 parallel threads to take advantage of their parallel architecture.[20] A Warp is a set of 32 threads which are configured to execute the same instruction.

Anti-Aliasing[edit]
DLSS requires and applies its own anti-aliasing method.  
It operates on similar principles to TAA. Like TAA, it uses information from past frames to produce the current frame. Unlike TAA, DLSS does not sample every pixel in every frame. Instead, it samples different pixels in different frames and uses pixels sampled in past frames to fill in the unsampled pixels in the current frame. DLSS uses machine learning to combine samples in the current frame and past frames, and it can be thought of as an advanced and superior TAA implementation made possible by the available tensor cores.[21]

See also[edit]
Image scaling
Deep learning
Supersampling
GeForce
Nvidia DGX
FidelityFX Super Resolution, AMD FSR is the equivalent upsampling technology for Radeon
XeSS, Intel XeSS is an AI-augmented upscaling technology
Tensor Processing Unit, an AI accelerator application-specific integrated circuit (ASIC) developed by Google
List of games with DLSS support
References[edit]


^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}"Nvidia RTX DLSS: Everything you need to know". Digital Trends. 2020-02-14. Retrieved 2020-04-05. Deep learning super sampling uses artificial intelligence and machine learning to produce an image that looks like a higher-resolution image, without the rendering overhead. Nvidiaâs algorithm learns from tens of thousands of rendered sequences of images that were created using a supercomputer. That trains the algorithm to be able to produce similarly beautiful images, but without requiring the graphics card to work as hard to do it.

^ Jump up to: a b c "Nvidia DLSS in 2020: stunning results". techspot.com. 2020-02-26. Retrieved 2020-04-05.

^ Jump up to: a b "Battlefield V DLSS Tested: Overpromised, Underdelivered". techspot.com. 2019-02-19. Retrieved 2020-04-06. Of course, this is to be expected. DLSS was never going to provide the same image quality as native 4K, while providing a 37% performance uplift. That would be black magic. But the quality difference comparing the two is almost laughable, in how far away DLSS is from the native presentation in these stressful areas.

^ "AMD Thinks NVIDIA DLSS is not Good Enough; Calls TAA & SMAA Better Alternatives". techquila.co.in. 2019-02-15. Retrieved 2020-04-06. Recently, two big titles received NVIDIA DLSS support, namely Metro Exodus and Battlefield V. Both these games come with NVIDIAâs DXR (DirectX Raytracing) implementation that at the moment is only supported by the GeForce RTX cards. DLSS makes these games playable at higher resolutions with much better frame rates, although there is a notable decrease in image sharpness. Now, AMD has taken a jab at DLSS, saying that traditional AA methods like SMAA and TAA "offer superior combinations of image quality and performance."

^ "Nvidia Very Quietly Made DLSS A Hell Of A Lot Better". Kotaku. 2020-02-22. Retrieved 2020-04-06. The benefit for most people is that, generally, DLSS comes with a sizeable FPS improvement. How much varies from game to game. In Metro Exodus, the FPS jump was barely there and certainly not worth the bizarre hit to image quality.

^ Jump up to: a b "Remedy's Control vs DLSS 2.0 â AI upscaling reaches the next level". Eurogamer. 2020-04-04. Retrieved 2020-04-05. Of course, this isn't the first DLSS implementation we've seen in Control. The game shipped with a decent enough rendition of the technology that didn't actually use the machine learning

^ "NVIDIA DLSS 2.0 Update Will Fix The GeForce RTX Cards' Big Mistake". techquila.co.in. 2020-03-24. Retrieved 2020-04-06. As promised, NVIDIA has updated the DLSS network in a new GeForce update that provides better, sharper image quality while still retaining higher framerates in raytraced games. While the feature wasn't used as well in its first iteration, NVIDIA is now confident that they have successfully fixed all the issues it had before

^ 
"HW News - Crysis Remastered Ray Tracing, NVIDIA DLSS 2, Ryzen 3100 Rumors". 2020-04-19. Retrieved 2020-04-19. The original DLSS required training the AI network for each new game. DLSS 2.0 trains using non-game-specific content, delivering a generalized network that works across games. This means faster game integrations, and ultimately more DLSS games.

^ Edelsten, Andrew (30 August 2019). "NVIDIA DLSS: Control and Beyond". nividia.com. Retrieved 11 August 2020. we developed a new image processing algorithm that approximated our AI research model and fit within our performance budget. This image processing approach to DLSS is integrated into Control

^ "NVIDIA DLSS 2.0 Review with Control â Is This Magic?". techquila.co.in. 2020-04-05. Retrieved 2020-04-06.

^ "NVIDIA preparing Ultra Quality mode for DLSS, 2.2.9.0 version spotted". VideoCardz.com. Retrieved 2021-07-06.

^ "NVIDIA DLSS: Your Questions, Answered". Nvidia. 2019-02-15. Retrieved 2020-04-19. The DLSS team first extracts many aliased frames from the target game, and then for each one we generate a matching âperfect frameâ using either super-sampling or accumulation rendering. These paired frames are fed to NVIDIAâs supercomputer. The supercomputer trains the DLSS model to recognize aliased inputs and generate high quality anti-aliased images that match the âperfect frameâ as closely as possible. We then repeat the process, but this time we train the model to generate additional pixels rather than applying AA. This has the effect of increasing the resolution of the input. Combining both techniques enables the GPU to render the full monitor resolution at higher frame rates.

^ A Supercomputer & AI Will Power NVIDIA RTX GPU's - NVIDIA RTX 2080 Performance. JAGS gaming. 2018-08-23. Retrieved 2020-04-19.

^ 
"NVIDIA's Deep Learning Super Sampling (DLSS) 2.0 Technology Is The Real Deal". Forbes. 2020-03-29. Retrieved 2020-04-07.

^ "NVIDIA DLSS 2.0: A Big Leap In AI Rendering". Nvidia. 2020-03-23. Retrieved 2020-11-25.

^ Jump up to: a b "NVIDIA DLSS 2.0: A Big Leap In AI Rendering". Nvidia. 2020-03-23. Retrieved 2020-04-07.

^ 
"NVIDIA TENSOR CORES". Nvidia. Retrieved 2020-04-07.

^ 
"On Tensors, Tensorflow, And Nvidia's Latest 'Tensor Cores'". tomshardware.com. 2017-04-11. Retrieved 2020-04-08.

^ "The NVIDIA Titan V Deep Learning Deep Dive: It's All About The Tensor Cores". AnandTech. 2018-07-03. Retrieved 2020-04-08.

^ "Using CUDA Warp-Level Primitives". Nvidia. 2018-01-15. Retrieved 2020-04-08. NVIDIA GPUs execute groups of threads known as warps in SIMT (Single Instruction, Multiple Thread) fashion

^ Edward Liu, NVIDIA "DLSS 2.0 - Image Reconstruction for Real-time Rendering with Deep Learning"


.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vteNvidiaGeForce (List of GPUs)Fixed pixel pipelinePre-GeForce
NV1
NV2

     
RIVA 128
RIVA TNT
TNT2

Â GeForce 256
2
4 MXVertex and pixel shaders
GeForce 3
4 Ti
FX
6
7
Unified shaders
GeForce 8
9
100
200
300
400
500
Unified shaders & NUMA
GeForce 600
700
800M
900
10
16
Ray tracing
GeForce 20
30
Software and technologiesMultimedia acceleration
NVENC (video encoding)
NVDEC (video decoding)
PureVideo (video decoding)
Software
Cg (shading language)
CUDA
Gelato (offline renderer)
Nvidia GameWorks
OptiX (ray tracing API)
PhysX (physics SDK)
Nvidia RTX (ray tracing platform)
Nvidia System Tools
VDPAU (video decode API)
Technologies
Nvidia 3D Vision (stereo 3D)
Nvidia G-Sync (variable refresh rate)
Nvidia Optimus (GPU switching)
Nvidia Surround (multi-monitor)
NVLink (protocol)
Scalable Link Interface (multi-GPU)
TurboCache (framebuffer in system memory)
GPU microarchitectures

Fahrenheit
Celsius
Kelvin
Rankine
Curie
Tesla
Fermi
Kepler
Maxwell
Pascal
Volta
Turing
Ampere
Lovelace
Hopper
Other productsGraphics processing
Nvidia Quadro
Quadro Plex
GPGPU
Nvidia Tesla
DGX
Console components
NV2A (Xbox)
RSX 'Reality Synthesizer' (PlayStation 3)
Tegra NX-SoC (Nintendo Switch)
Nvidia Shield
Shield Portable
Shield Tablet
Shield Android TV
GeForce Now
SoCs and embedded
GoForce
Drive
Jetson
Tegra
CPUs
Project Denver
Computer chipsets
nForce
CompanyKey people
Jen-Hsun Huang
Chris Malachowsky
Curtis Priem
David Kirk
Bill Dally
Debora Shoquist
Ranga Jayaraman
Jonah M. Alben
Acquisitions
3dfx Interactive
Ageia
ULi
Arm Holdings
Icera
Mellanox Technologies
Mental Images
PortalPlayer
Exluna
MediaQ
Stexar





<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=Deep_learning_super_sampling&oldid=1068542845"
		Categories: Graphics processing unitsGraphics cards3D computer graphicsNvidiaHidden categories: Articles with short descriptionShort description is different from Wikidata
	
