
Title:
Kruskal's algorithm
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Minimum spanning forest algorithm that greedily adds edges
This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources:Â "Kruskal's algorithm"Â âÂ newsÂ Â· newspapersÂ Â· booksÂ Â· scholarÂ Â· JSTOR  (September 2018) (Learn how and when to remove this template message)
.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}Graph and treesearch algorithms
Î±âÎ²
A*
B*
Backtracking
Beam
BellmanâFord
Best-first
Bidirectional
BorÅ¯vka
Branch & bound
BFS
British Museum
D*
DFS
Dijkstra
Edmonds
FloydâWarshall
Fringe search
Hill climbing
IDA*
Iterative deepening
Johnson
Jump point
Kruskal
Lexicographic BFS
LPA*
Prim
SMA*

Listings
Graph algorithms
Search algorithms
List of graph algorithms

Related topics
Dynamic programming
Graph traversal
Tree traversal
Search games
Graph coloring
.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vte
Kruskal's algorithm finds a minimum spanning forest of an undirected edge-weighted graph. If the graph is connected, it finds a minimum spanning tree. (A minimum spanning tree of a connected graph is a subset of the edges that forms a tree that includes every vertex, where the sum of the weights of all the edges in the tree is minimized. For a disconnected graph, a minimum spanning forest is composed of a minimum spanning tree for each connected component.) It is a greedy algorithm in graph theory as in each step it adds the next lowest-weight edge that will not form a cycle to the minimum spanning forest.[1]
This algorithm first appeared in Proceedings of the American Mathematical Society, pp.Â 48â50 in 1956, and was written by Joseph Kruskal.[2]
Other algorithms for this problem include Prim's algorithm, the reverse-delete algorithm, and BorÅ¯vka's algorithm.

Contents

1 Algorithm
2 Pseudocode
3 Complexity
4 Example
5 Proof of correctness

5.1 Spanning tree
5.2 Minimality


6 Parallel algorithm
7 See also
8 References
9 External links



Algorithm[edit]
create a forest F (a set of trees), where each vertex in the graph is a separate tree
create a set S containing all the edges in the graph
while S is nonempty and F is not yet spanning
remove an edge with minimum weight from S
if the removed edge connects two different trees then add it to the forest F, combining two trees into a single tree
At the termination of the algorithm, the forest forms a minimum spanning forest of the graph. If the graph is connected, the forest has a single component and forms a minimum spanning tree.

Pseudocode[edit]
  A demo for Kruskal's algorithm on a complete graph with weights based on Euclidean distance.
The following code is implemented with a disjoint-set data structure. Here, we represent our forest F as a set of edges, and use the disjoint-set data structure to efficiently determine whether two vertices are part of the same tree.

algorithm Kruskal(G) is
    F:= â
    for each v â G.V do
        MAKE-SET(v)
    for each (u, v) in G.E ordered by weight(u, v), increasing do
        if FIND-SET(u) â  FIND-SET(v) then
            F:= F âª {(u, v)} âª {(v, u)}
            UNION(FIND-SET(u), FIND-SET(v))
    return F

Complexity[edit]
For a graph with E edges and V vertices, Kruskal's algorithm can be shown to run in O(E log E) time, or equivalently, O(E log V) time, all with simple data structures. These running times are equivalent because:

E is at most 
  
    
      
        
          V
          
            2
          
        
      
    
    {\displaystyle V^{2}}
  
 and 
  
    
      
        log
        â¡
        
          V
          
            2
          
        
        =
        2
        log
        â¡
        V
        â
        O
        (
        log
        â¡
        V
        )
      
    
    {\displaystyle \log V^{2}=2\log V\in O(\log V)}
  
.
Each isolated vertex is a separate component of the minimum spanning forest. If we ignore isolated vertices we obtain V â¤ 2E, so log V is 
  
    
      
        O
        (
        log
        â¡
        E
        )
      
    
    {\displaystyle O(\log E)}
  
.
We can achieve this bound as follows: first sort the edges by weight using a comparison sort in O(E log E) time; this allows the step "remove an edge with minimum weight from S" to operate in constant time. Next, we use a disjoint-set data structure to keep track of which vertices are in which components. We place each vertex into its own disjoint set, which takes O(V) operations. Finally, in worst case, we need to iterate through all edges, and for each edge we need to do two 'find' operations and possibly one union. Even a simple disjoint-set data structure such as disjoint-set forests with union by rank can perform O(E) operations in O(E log V) time. Thus the total time is O(E log E) = O(E log V).
Provided that the edges are either already sorted or can be sorted in linear time (for example with counting sort or radix sort), the algorithm can use a more sophisticated disjoint-set data structure to run in O(E Î±(V)) time, where Î± is the extremely slowly growing inverse of the single-valued Ackermann function.

Example[edit]


Image
Description




AD and CE are the shortest edges, with length 5, and AD has been arbitrarily chosen, so it is highlighted.




CE is now the shortest edge that does not form a cycle, with length 5, so it is highlighted as the second edge.




The next edge, DF with length 6, is highlighted using much the same method.




The next-shortest edges are AB and BE, both with length 7. AB is chosen arbitrarily, and is highlighted. The edge BD has been highlighted in red, because there already exists a path (in green) between B and D, so it would form a cycle (ABD) if it were chosen.




The process continues to highlight the next-smallest edge, BE with length 7. Many more edges are highlighted in red at this stage: BC because it would form the loop BCE,  DE because it would form the loop DEBA, and FE because it would form FEBAD.




Finally, the process finishes with the edge EG of length 9, and the minimum spanning tree is found.

Proof of correctness[edit]
The proof consists of two parts. First, it is proved that the algorithm produces a spanning tree. Second, it is proved that the constructed spanning tree is of minimal weight.

Spanning tree[edit]
Let 
  
    
      
        G
      
    
    {\displaystyle G}
  
 be a connected, weighted graph and let 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
 be the subgraph of 
  
    
      
        G
      
    
    {\displaystyle G}
  
 produced by the algorithm. 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
 cannot have a cycle, as by definition an edge is not added if it results in a cycle. 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
 cannot be disconnected, since the first encountered edge that joins two components of 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
 would have been added by the algorithm. Thus, 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
 is a spanning tree of 
  
    
      
        G
      
    
    {\displaystyle G}
  
.

Minimality[edit]
We show that the following proposition P is true by induction: If F is the set of edges chosen at any stage of the algorithm, then there is some minimum spanning tree that contains F and none of the edges rejected by the algorithm.

Clearly P is true at the beginning, when F is empty: any minimum spanning tree will do, and there exists one because a weighted connected graph always has a minimum spanning tree.
Now assume P is true for some non-final edge set F and let T be a minimum spanning tree that contains F.
If the next chosen edge e is also in T, then P is true for F + e.
Otherwise, if e is not in T then T + e has a cycle C. This cycle contains edges which do not belong to F, since e does not form a cycle when added to F but does in T.  Let f be an edge which is in C but not in F + e.  Note that f also belongs to T, and by  P has not been considered by the algorithm. f must therefore have a weight at least as large as e. Then T â f + e is a tree, and it has the same or less weight as T.  So T â f + e is a minimum spanning tree containing F + e and again P holds.
Therefore, by the principle of induction, P holds when F has become a spanning tree, which is only possible if F is a minimum spanning tree itself.
Parallel algorithm[edit]
Kruskal's algorithm is inherently sequential and hard to parallelize. It is, however, possible to perform the initial sorting of the edges in parallel or, alternatively, to use a parallel implementation of a binary heap to extract the minimum-weight edge in every iteration.[3]
As parallel sorting is possible in time 
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
  
 on 
  
    
      
        O
        (
        log
        â¡
        n
        )
      
    
    {\displaystyle O(\log n)}
  
 processors,[4] the runtime of Kruskal's algorithm can be reduced to O(E Î±(V)), where Î± again is the inverse of the single-valued Ackermann function.
A variant of Kruskal's algorithm, named Filter-Kruskal, has been described by Osipov et al.[5] and is better suited for parallelization. The basic idea behind Filter-Kruskal is to partition the edges in a similar way to quicksort and filter out edges that connect vertices of the same tree to reduce the cost of sorting. The following pseudocode demonstrates this.

function filter_kruskal(G) is
    if |G.E| < kruskal_threshold:
        return kruskal(G)
    pivot = choose_random(G.E)
    
  
    
      
        
          E
          
            â¤
          
        
      
    
    {\displaystyle E_{\leq }}
  
, 
  
    
      
        
          E
          
            >
          
        
      
    
    {\displaystyle E_{>}}
  
 = partition(G.E, pivot)
    A = filter_kruskal(
  
    
      
        
          E
          
            â¤
          
        
      
    
    {\displaystyle E_{\leq }}
  
)
    
  
    
      
        
          E
          
            >
          
        
      
    
    {\displaystyle E_{>}}
  
 = filter(
  
    
      
        
          E
          
            >
          
        
      
    
    {\displaystyle E_{>}}
  
)
    A = A âª filter_kruskal(
  
    
      
        
          E
          
            >
          
        
      
    
    {\displaystyle E_{>}}
  
)
    return A

function partition(E, pivot) is
    
  
    
      
        
          E
          
            â¤
          
        
      
    
    {\displaystyle E_{\leq }}
  
 = â, 
  
    
      
        
          E
          
            >
          
        
      
    
    {\displaystyle E_{>}}
  
 = â
    foreach (u, v) in E do
        if weight(u, v) <= pivot then
            
  
    
      
        
          E
          
            â¤
          
        
      
    
    {\displaystyle E_{\leq }}
  
 = 
  
    
      
        
          E
          
            â¤
          
        
      
    
    {\displaystyle E_{\leq }}
  
 âª {(u, v)}
        else
            
  
    
      
        
          E
          
            >
          
        
      
    
    {\displaystyle E_{>}}
  
 = 
  
    
      
        
          E
          
            >
          
        
      
    
    {\displaystyle E_{>}}
  
 âª {(u, v)}
    return 
  
    
      
        
          E
          
            â¤
          
        
      
    
    {\displaystyle E_{\leq }}
  
, 
  
    
      
        
          E
          
            >
          
        
      
    
    {\displaystyle E_{>}}
  


function filter(E) is
    
  
    
      
        
          E
          
            filtered
          
        
      
    
    {\displaystyle E_{\text{filtered}}}
  
 = â
    foreach (u, v) in E do
        if find_set(u) â  find_set(v) then
            
  
    
      
        
          E
          
            filtered
          
        
      
    
    {\displaystyle E_{\text{filtered}}}
  
 = 
  
    
      
        
          E
          
            filtered
          
        
      
    
    {\displaystyle E_{\text{filtered}}}
  
 âª {(u, v)}
    return 
  
    
      
        
          E
          
            filtered
          
        
      
    
    {\displaystyle E_{\text{filtered}}}
  


Filter-Kruskal lends itself better for parallelization as sorting, filtering, and partitioning can easily be performed in parallel by distributing the edges between the processors.[5]
Finally, other variants of a parallel implementation of Kruskal's algorithm have been explored. Examples include a scheme that uses helper threads to remove edges that are definitely not part of the MST in the background,[6] and a variant which runs the sequential algorithm on p subgraphs, then merges those subgraphs until only one, the final MST, remains.[7]

See also[edit]
Prim's algorithm
Dijkstra's algorithm
BorÅ¯vka's algorithm
Reverse-delete algorithm
Single-linkage clustering
Greedy geometric spanner
References[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Cormen, Thomas; Charles E Leiserson, Ronald L Rivest, Clifford Stein (2009). Introduction To Algorithms (ThirdÂ ed.). MIT Press. pp.Â 631. ISBNÂ 978-0262258104.{{cite book}}:  CS1 maint: multiple names: authors list (link)

^ Kruskal, J. B. (1956). "On the shortest spanning subtree of a graph and the traveling salesman problem". Proceedings of the American Mathematical Society. 7 (1): 48â50. doi:10.1090/S0002-9939-1956-0078686-7. JSTORÂ 2033241.

^ Quinn, Michael J.; Deo, Narsingh (1984). "Parallel graph algorithms". ACM Computing Surveys. 16 (3): 319â348. doi:10.1145/2514.2515. S2CIDÂ 6833839.

^ Grama, Ananth; Gupta, Anshul; Karypis, George; Kumar, Vipin (2003). Introduction to Parallel Computing. pp.Â 412â413. ISBNÂ 978-0201648652.

^ a b Osipov, Vitaly; Sanders, Peter; Singler, Johannes (2009). "The filter-kruskal minimum spanning tree algorithm" (PDF). Proceedings of the Eleventh Workshop on Algorithm Engineering and Experiments (ALENEX). Society for Industrial and Applied Mathematics: 52â61.

^ Katsigiannis, Anastasios; Anastopoulos, Nikos; Konstantinos, Nikas; Koziris, Nectarios (2012). "An approach to parallelize kruskal's algorithm using helper threads" (PDF). Parallel and Distributed Processing Symposium Workshops & PhD Forum (IPDPSW), 2012 IEEE 26th International: 1601â1610. doi:10.1109/IPDPSW.2012.201. ISBNÂ 978-1-4673-0974-5. S2CIDÂ 14430930.

^ LonÄar, Vladimir; Å krbiÄ, Srdjan; BalaÅ¾, Antun (2014). "Parallelization of Minimum Spanning Tree Algorithms Using Distributed Memory Architectures". Transactions on Engineering Technologies.: 543â554. doi:10.1007/978-94-017-8832-8_39. ISBNÂ 978-94-017-8831-1.


Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBNÂ 0-262-03293-7. Section 23.2: The algorithms of Kruskal and Prim, pp.Â 567â574.
Michael T. Goodrich and Roberto Tamassia. Data Structures and Algorithms in Java, Fourth Edition. John Wiley & Sons, Inc., 2006. ISBNÂ 0-471-73884-0. Section 13.7.1: Kruskal's Algorithm, pp.Â 632..
External links[edit]
Data for the article's example.
Gephi Plugin For Calculating a Minimum Spanning Tree source code.
Kruskal's Algorithm with example and program in c++
Kruskal's Algorithm code in C++ as applied to random numbers
Kruskal's Algorithm code in Python with explanation




<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=Kruskal%27s_algorithm&oldid=1055162445"
		Categories: Graph algorithmsSpanning treeGreedy algorithmsHidden categories: CS1 maint: multiple names: authors listArticles with short descriptionShort description matches WikidataArticles needing additional references from September 2018All articles needing additional referencesArticles with example pseudocodeArticles containing proofs
	
