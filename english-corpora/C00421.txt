
Title:
Deterministic finite automaton
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		

.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}"DFSA" redirects here. DFSA may also refer to drug-facilitated sexual assault.
  An example of a deterministic finite automaton that accepts only binary numbers that are multiples of 3. The state S0 is both the start state and an accept state. For example, the string "1001" leads to the state sequence S0, S1, S2, S1, S0, and is hence accepted.
In the theory of computation, a branch of theoretical computer science, a deterministic finite automaton (DFA)âalso known as deterministic finite acceptor (DFA), deterministic finite-state machine (DFSM), or deterministic finite-state automaton (DFSA)âis a finite-state machine that accepts or rejects a given string of symbols, by running through a state sequence uniquely determined by the string.[1] Deterministic refers to the uniqueness of the computation run. In search of the simplest models to capture finite-state machines, Warren McCulloch and Walter Pitts were among the first researchers to introduce a concept similar to finite automata in 1943.[2][3]
The figure illustrates a deterministic finite automaton using a state diagram. In this example automaton, there are three states: S0, S1, and S2 (denoted graphically by circles). The automaton takes a finite sequence of 0s and 1s as input. For each state, there is a transition arrow leading out to a next state for both 0 and 1. Upon reading a symbol, a DFA jumps deterministically from one state to another by following the transition arrow. For example, if the automaton is currently in state S0 and the current input symbol is 1, then it deterministically jumps to state S1. A DFA has a start state (denoted graphically by an arrow coming in from nowhere) where computations begin, and a set of accept states (denoted graphically by a double circle) which help define when a computation is successful.
A DFA is defined as an abstract mathematical concept, but is often implemented in hardware and software for solving various specific problems such as lexical analysis and pattern matching. For example, a DFA can model software that decides whether or not online user input such as email addresses are syntactically valid.[4]
DFAs have been generalized to nondeterministic finite automata (NFA) which may have several arrows of the same label starting from a state. Using the powerset construction method, every NFA can be translated to a DFA that recognizes the same language. DFAs, and NFAs as well, recognize exactly the set of regular languages.[1]

Contents

1 Formal definition
2 Complete and incomplete
3 Example
4 Closure properties
5 As a transition monoid
6 Local automata
7 Random
8 Advantages and disadvantages
9 DFA identification from labeled words
10 See also
11 Notes
12 References



Formal definition[edit]
A deterministic finite automaton M is a 5-tuple, (Q, Î£, Î´, q0, F), consisting of

a finite set of states Q
a finite set of input symbols called the alphabet Î£
a transition function Î´Â : Q Ã Î£ â Q
an initial or start state 
  
    
      
        
          q
          
            0
          
        
        â
        Q
      
    
    {\displaystyle q_{0}\in Q}
  

a set of accept states 
  
    
      
        F
        â
        Q
      
    
    {\displaystyle F\subseteq Q}
  

Let w = a1a2â¦an be a string over the alphabet Î£. The automaton M accepts the string w if a sequence of states, r0, r1, â¦, rn, exists in Q with the following conditions:

r0 = q0
ri+1 = Î´(ri, ai+1), for i = 0, â¦, n â 1

  
    
      
        
          r
          
            n
          
        
        â
        F
      
    
    {\displaystyle r_{n}\in F}
  
.
In words, the first condition says that the machine starts in the start state q0. The second condition says that given each character of string w, the machine will transition from state to state according to the transition function Î´. The last condition says that the machine accepts w if the last input of w causes the machine to halt in one of the accepting states. Otherwise, it is said that the automaton rejects the string. The set of strings that M accepts is the language recognized by M and this language is denoted by L(M).
A deterministic finite automaton without accept states and without a starting state is known as a transition system or semiautomaton.
For more comprehensive introduction of the formal definition see automata theory.

Complete and incomplete[edit]
According to the above definition, deterministic finite automata are always complete: they define from each state a transition for each input symbol.
While this is the most common definition, some authors use the term deterministic finite automaton for a slightly different notion: an automaton that defines at most one transition for each state and each input symbol; the transition function is allowed to be partial.[5] When no transition is defined, such an automaton halts.

Example[edit]
The following example is of a DFA M, with a binary alphabet, which requires that the input contains an even number of 0s.

  The state diagram for M
M = (Q, Î£, Î´, q0, F) where

Q = {S1, S2}
Î£ = {0, 1}
q0 = S1
F = {S1} and
Î´ is defined by the following state transition table:



0
1


S1
S2
S1


S2
S1
S2

The state S1 represents that there has been an even number of 0s in the input so far, while S2 signifies an odd number. A 1 in the input does not change the state of the automaton. When the input ends, the state will show whether the input contained an even number of 0s or not. If the input did contain an even number of 0s, M will finish in state S1, an accepting state, so the input string will be accepted.
The language recognized by M is the regular language given by the regular expression (1*) (0 (1*) 0 (1*))*, where * is the Kleene star, e.g., 1* denotes any number (possibly zero) of consecutive ones.

Closure properties[edit]
  The upper left automaton recognizes the language of all binary strings containing at least one occurrence of "00". The lower right automaton recognizes all binary strings with an even number of "1". The lower left automaton is obtained as product of the former two, it recognizes the intersection of both languages.
If DFAs recognize the languages that are obtained by applying an operation on the DFA recognizable languages then DFAs are said to be closed under the operation. The DFAs are closed under the following operations.

.mw-parser-output .div-col{margin-top:0.3em;column-width:30em}.mw-parser-output .div-col-small{font-size:90%}.mw-parser-output .div-col-rules{column-rule:1px solid #aaa}.mw-parser-output .div-col dl,.mw-parser-output .div-col ol,.mw-parser-output .div-col ul{margin-top:0}.mw-parser-output .div-col li,.mw-parser-output .div-col dd{page-break-inside:avoid;break-inside:avoid-column}
Union
Intersection[6]:â59â60â (see picture)
Concatenation
Negation
Kleene closure
Reversal
Init
Quotient[citation needed]
Substitution[citation needed]
Homomorphism[citation needed]
For each operation, an optimal construction with respect to the number of states has been determined in state complexity research.
Since DFAs are equivalent to nondeterministic finite automata (NFA), these closures may also be proved using closure properties of NFA.

As a transition monoid[edit]
A run of a given DFA can be seen as a sequence of compositions of a very general formulation of the transition function with itself. Here we construct that function.
For a given input symbol 
  
    
      
        a
        â
        Î£
      
    
    {\displaystyle a\in \Sigma }
  
, one may construct a transition function 
  
    
      
        
          Î´
          
            a
          
        
        :
        Q
        â
        Q
      
    
    {\displaystyle \delta _{a}:Q\rightarrow Q}
  
 by defining 
  
    
      
        
          Î´
          
            a
          
        
        (
        q
        )
        =
        Î´
        (
        q
        ,
        a
        )
      
    
    {\displaystyle \delta _{a}(q)=\delta (q,a)}
  
 for all 
  
    
      
        q
        â
        Q
      
    
    {\displaystyle q\in Q}
  
.  (This trick is called currying.) From this perspective, 
  
    
      
        
          Î´
          
            a
          
        
      
    
    {\displaystyle \delta _{a}}
  
 "acts" on a state in Q to yield another state.  One may then consider the result of function composition repeatedly applied to the various functions 
  
    
      
        
          Î´
          
            a
          
        
      
    
    {\displaystyle \delta _{a}}
  
, 
  
    
      
        
          Î´
          
            b
          
        
      
    
    {\displaystyle \delta _{b}}
  
, and so on.  Given a pair of letters 
  
    
      
        a
        ,
        b
        â
        Î£
      
    
    {\displaystyle a,b\in \Sigma }
  
, one may define a new function 
  
    
      
        
          
            
              
                Î´
                ^
              
            
          
          
            a
            b
          
        
        =
        
          Î´
          
            a
          
        
        â
        
          Î´
          
            b
          
        
      
    
    {\displaystyle {\widehat {\delta }}_{ab}=\delta _{a}\circ \delta _{b}}
  
, where 
  
    
      
        â
      
    
    {\displaystyle \circ }
  
 denotes function composition.
Clearly, this process may be recursively continued, giving the following recursive definition of 
  
    
      
        
          
            
              Î´
              ^
            
          
        
        :
        Q
        Ã
        
          Î£
          
            â
          
        
        â
        Q
      
    
    {\displaystyle {\widehat {\delta }}:Q\times \Sigma ^{\star }\rightarrow Q}
  
:


  
    
      
        
          
            
              Î´
              ^
            
          
        
        (
        q
        ,
        Ïµ
        )
        =
        q
      
    
    {\displaystyle {\widehat {\delta }}(q,\epsilon )=q}
  
, where 
  
    
      
        Ïµ
      
    
    {\displaystyle \epsilon }
  
 is the empty string and

  
    
      
        
          
            
              Î´
              ^
            
          
        
        (
        q
        ,
        w
        a
        )
        =
        
          Î´
          
            a
          
        
        (
        
          
            
              Î´
              ^
            
          
        
        (
        q
        ,
        w
        )
        )
      
    
    {\displaystyle {\widehat {\delta }}(q,wa)=\delta _{a}({\widehat {\delta }}(q,w))}
  
, where 
  
    
      
        w
        â
        
          Î£
          
            â
          
        
        ,
        a
        â
        Î£
      
    
    {\displaystyle w\in \Sigma ^{*},a\in \Sigma }
  
 and 
  
    
      
        q
        â
        Q
      
    
    {\displaystyle q\in Q}
  
.

  
    
      
        
          
            
              Î´
              ^
            
          
        
      
    
    {\displaystyle {\widehat {\delta }}}
  
  is defined for all words 
  
    
      
        w
        â
        
          Î£
          
            â
          
        
      
    
    {\displaystyle w\in \Sigma ^{*}}
  
.  A run of the DFA is a sequence of compositions of 
  
    
      
        
          
            
              Î´
              ^
            
          
        
      
    
    {\displaystyle {\widehat {\delta }}}
  
 with itself.
Repeated function composition forms a monoid. For the transition functions, this monoid is known as the transition monoid, or sometimes the transformation semigroup. The construction can also be reversed: given a 
  
    
      
        
          
            
              Î´
              ^
            
          
        
      
    
    {\displaystyle {\widehat {\delta }}}
  
, one can reconstruct a 
  
    
      
        Î´
      
    
    {\displaystyle \delta }
  
, and so the two descriptions are equivalent.

Local automata[edit]
A local automaton is a DFA, not necessarily complete, for which all edges with the same label lead to a single vertex.  Local automata accept the class of local languages, those for which membership of a word in the language is determined by a "sliding window" of length two on the word.[7][8]
A Myhill graph over an alphabet A is a directed graph with vertex set A and subsets of vertices labelled "start" and "finish".  The language accepted by a Myhill graph is the set of directed paths from a start vertex to a finish vertex: the graph thus acts as an automaton.[7] The class of languages accepted by Myhill graphs is the class of local languages.[9]

Random[edit]
When the start state and accept states are ignored, a DFA of n states and an alphabet of size k can be seen as a digraph of n vertices in which all vertices have k out-arcs labeled 1, â¦, k (a k-out digraph). It is known that when k â¥ 2 is a fixed integer, with high probability, the largest strongly connected component (SCC) in such a k-out digraph chosen uniformly at random is of linear size and it can be reached by all vertices.[10] It has also been proven that if k is allowed to increase as n increases, then the whole digraph has a phase transition for strong connectivity similar to ErdÅsâRÃ©nyi model for connectivity.[11]
In a random DFA, the maximum number of vertices reachable from one vertex is very close to the number of vertices in the largest SCC with high probability.[10][12] This is also true for the largest induced sub-digraph of minimum in-degree one, which can be seen as a directed version of 1-core.[11]

Advantages and disadvantages[edit]
DFAs are one of the most practical models of computation, since there is a trivial linear time, constant-space, online algorithm to simulate a DFA on a stream of input. Also, there are efficient algorithms to find a DFA recognizing:

the complement of the language recognized by a given DFA.
the union/intersection of the languages recognized by two given DFAs.
Because DFAs can be reduced to a canonical form (minimal DFAs), there are also efficient algorithms to determine:

whether a DFA accepts any strings (Emptiness Problem)
whether a DFA accepts all strings (Universality Problem)
whether two DFAs recognize the same language (Equality Problem)
whether the language recognized by a DFA is included in the language recognized by a second DFA (Inclusion Problem)
the DFA with a minimum number of states for a particular regular language (Minimization Problem)
DFAs are equivalent in computing power to nondeterministic finite automata (NFAs). This is because, firstly any DFA is also an NFA, so an NFA can do what a DFA can do. Also, given an NFA, using the powerset construction one can build a DFA that recognizes the same language as the NFA, although the DFA could have exponentially larger number of states than the NFA.[13][14] However, even though NFAs are computationally equivalent to DFAs, the above mentioned problems are not necessarily solved efficiently also for NFAs. The non-universality problem for NFAs is PSPACE complete since there are small NFAs with shortest rejecting word in exponential size. A DFA is universal if and only if all states are final states, but this does not hold for NFAs. The Equality, Inclusion and Minimization Problems are also PSPACE complete since they require forming the complement of an NFA which results in an exponential blow up of size.[15]
On the other hand, finite-state automata are of strictly limited power in the languages they can recognize; many simple languages, including any problem that requires more than constant space to solve, cannot be recognized by a DFA. The classic example of a simply described language that no DFA can recognize is bracket or Dyck language, i.e., the language that consists of properly paired brackets such as word "(()())". Intuitively, no DFA can recognize the Dyck language because DFAs are not capable of counting: a DFA-like automaton needs to have a state to represent any possible number of "currently open" parentheses, meaning it would need an unbounded number of states. Another simpler example is the language consisting of strings of the form anbn for some finite but arbitrary number of a's, followed by an equal number of b's.[16]

DFA identification from labeled words[edit]
Main article: Induction of regular languages
Given a set of positive words 
  
    
      
        
          S
          
            +
          
        
        â
        
          Î£
          
            â
          
        
      
    
    {\displaystyle S^{+}\subset \Sigma ^{*}}
  
 and a set of negative words 
  
    
      
        
          S
          
            â
          
        
        â
        
          Î£
          
            â
          
        
      
    
    {\displaystyle S^{-}\subset \Sigma ^{*}}
  
 one can construct a DFA that accepts all words from 
  
    
      
        
          S
          
            +
          
        
      
    
    {\displaystyle S^{+}}
  
 and rejects all words from 
  
    
      
        
          S
          
            â
          
        
      
    
    {\displaystyle S^{-}}
  
: this problem is called DFA identification (synthesis, learning).
While some DFA can be constructed in linear time, the problem of identifying a DFA with the minimal number of states is NP-complete.[17]
The first algorithm for minimal DFA identification has been proposed by Trakhtenbrot and Barzdin in[18] and is called the TB-algorithm.
However, the TB-algorithm assumes that all words from 
  
    
      
        Î£
      
    
    {\displaystyle \Sigma }
  
 up to a given length are contained in either 
  
    
      
        
          S
          
            +
          
        
        âª
        
          S
          
            â
          
        
      
    
    {\displaystyle S^{+}\cup S^{-}}
  
.
Later, K. Lang proposed an extension of the TB-algorithm that does not use any assumptions about 
  
    
      
        
          S
          
            +
          
        
      
    
    {\displaystyle S^{+}}
  
 and 
  
    
      
        
          S
          
            â
          
        
      
    
    {\displaystyle S^{-}}
  
 the Traxbar algorithm.[19]
However, Traxbar does not guarantee the minimality of the constructed DFA.
In his work[17] E.M. Gold also proposed a heuristic algorithm for minimal DFA identification.
Gold's algorithm assumes that 
  
    
      
        
          S
          
            +
          
        
      
    
    {\displaystyle S^{+}}
  
 and 
  
    
      
        
          S
          
            â
          
        
      
    
    {\displaystyle S^{-}}
  
 contain a characteristic set of the regular language; otherwise, the constructed DFA will be inconsistent either with 
  
    
      
        
          S
          
            +
          
        
      
    
    {\displaystyle S^{+}}
  
 or 
  
    
      
        
          S
          
            â
          
        
      
    
    {\displaystyle S^{-}}
  
.
Other notable DFA identification algorithms include the RPNI algorithm,[20] the Blue-Fringe evidence-driven state-merging algorithm,[21] 
Windowed-EDSM.[22]
Another research direction is the application of evolutionary algorithms: the smart state labeling evolutionary algorithm[23] allowed to solve a modified DFA identification problem in which the training data (sets 
  
    
      
        
          S
          
            +
          
        
      
    
    {\displaystyle S^{+}}
  
 and 
  
    
      
        
          S
          
            â
          
        
      
    
    {\displaystyle S^{-}}
  
) is noisy in the sense that some words are attributed to wrong classes.
Yet another step forward is due to application of SAT solvers by Marjin J. H. Heule and S. Verwer: the minimal DFA identification problem is reduced to deciding the satisfiability of a Boolean formula.[24] The main idea is to build a augmented prefix-tree acceptor (a trie containing all input words with corresponding labels) based on the input sets and reduce the problem of finding a DFA with 
  
    
      
        C
      
    
    {\displaystyle C}
  
 states to coloring the tree vertices with 
  
    
      
        C
      
    
    {\displaystyle C}
  
 states in such a way that when vertices with one color are merged to one state, the generated automaton is deterministic and complies with 
  
    
      
        
          S
          
            +
          
        
      
    
    {\displaystyle S^{+}}
  
 and 
  
    
      
        
          S
          
            â
          
        
      
    
    {\displaystyle S^{-}}
  
.
Though this approach allows finding the minimal DFA, it suffers from exponential blow-up of execution time when the size of input data increases.
Therefore, Heule and Verwer's initial algorithm has later been augmented with making several steps of the EDSM algorithm prior to SAT solver execution: the DFASAT algorithm.[25]
This allows reducing the search space of the problem, but leads to loss of the minimality guarantee.
Another way of reducing the search space has been proposed in[26] by means of new symmetry breaking predicates based on the breadth-first search algorithm:
the sought DFA's states are constrained to be numbered according to the BFS algorithm launched from the initial state. This approach reduces the search space by 
  
    
      
        C
        !
      
    
    {\displaystyle C!}
  
 by eliminating isomorphic automata.

See also[edit]

Acyclic deterministic finite automata
DFA minimization
Monadic second-order logic
NFA to DFA conversion
Quantum finite automata
Read-only right moving Turing machines
Separating words problem
Turing machine
Two-way deterministic finite automaton
Notes[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ Jump up to: a b Hopcroft 2001:

^ McCulloch and Pitts (1943):

^ Rabin and Scott (1959):

^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Gouda, Prabhakar, Application of Finite automata

^ Mogensen, Torben Ãgidius (2011). "Lexical Analysis". Introduction to Compiler Design. Undergraduate Topics in Computer Science. London: Springer. p.Â 12. doi:10.1007/978-0-85729-829-4_1. ISBNÂ 978-0-85729-828-7.

^ John E. Hopcroft and Jeffrey D. Ullman (1979). Introduction to Automata Theory, Languages, and Computation. Reading/MA: Addison-Wesley. ISBNÂ 0-201-02988-X.

^ Jump up to: a b Lawson (2004) p.129

^ Sakarovitch (2009) p.228

^ Lawson (2004) p.128

^ Jump up to: a b Grusho, A. A. (1973). "Limit distributions of certain characteristics of random automaton graphs". Mathematical Notes of the Academy of Sciences of the USSR. 4: 633â637. doi:10.1007/BF01095785. S2CIDÂ 121723743.

^ Jump up to: a b Cai, Xing Shi; Devroye, Luc (October 2017). "The graph structure of a deterministic automaton chosen at random". Random Structures & Algorithms. 51 (3): 428â458. arXiv:1504.06238. doi:10.1002/rsa.20707. S2CIDÂ 13013344.

^ Carayol, Arnaud; Nicaud, Cyril (February 2012). Distribution of the number of accessible states in a random deterministic automaton. STACS'12 (29th Symposium on Theoretical Aspects of Computer Science). Vol.Â 14. Paris, France. pp.Â 194â205.

^ Sakarovitch (2009) p.105

^ Lawson (2004) p.63

^ https://www7.in.tum.de/um/courses/auto/ws1718/slides1718/04-Implementations_sets.pdf

^ Lawson (2004) p.46

^ Jump up to: a b Gold, E. M. (1978). "Complexity of Automaton Identification from Given Data". Information and Control. 37 (3): 302â320. doi:10.1016/S0019-9958(78)90562-4.

^ De Vries, A. (28 June 2014). Finite Automata: Behavior and Synthesis. ISBNÂ 9781483297293.

^ Lang, Kevin J. (1992). "Random DFA's can be approximately learned from sparse uniform examples". Proceedings of the fifth annual workshop on Computational learning theory - COLT '92. pp.Â 45â52. doi:10.1145/130385.130390. ISBNÂ 089791497X. S2CIDÂ 7480497.

^ Oncina, J.; GarcÃ­a, P. (1992). "Inferring Regular Languages in Polynomial Updated Time". Pattern Recognition and Image Analysis. Series in Machine Perception and Artificial Intelligence. Vol.Â 1. pp.Â 49â61. doi:10.1142/9789812797902_0004. ISBNÂ 978-981-02-0881-3.

^ Lang, Kevin J.; Pearlmutter, Barak A.; Price, Rodney A. (1998). "Results of the Abbadingo one DFA learning competition and a new evidence-driven state merging algorithm". Grammatical Inference (PDF). Lecture Notes in Computer Science. Vol.Â 1433. pp.Â 1â12. doi:10.1007/BFb0054059. ISBNÂ 978-3-540-64776-8.

^ "Beyond EDSM | Proceedings of the 6th International Colloquium on Grammatical Inference: Algorithms and Applications".

^ Lucas, S.M.; Reynolds, T.J. (2005). "Learning deterministic finite automata with a smart state labeling evolutionary algorithm". IEEE Transactions on Pattern Analysis and Machine Intelligence. 27 (7): 1063â1074. doi:10.1109/TPAMI.2005.143. PMIDÂ 16013754. S2CIDÂ 14062047.

^ Heule, M. J. H. (2010). Exact DFA Identification Using SAT Solvers. Grammatical Inference: Theoretical Results and Applications. ICGI 2010. Lecture Notes in Computer Science. Vol.Â 6339. pp.Â 66â79. doi:10.1007/978-3-642-15488-1_7.

^ Heule, Marijn J. H.; Verwer, Sicco (2013). "Software model synthesis using satisfiability solvers". Empirical Software Engineering. 18 (4): 825â856. doi:10.1007/s10664-012-9222-z. hdl:2066/103766. S2CIDÂ 17865020.

^ Ulyantsev, Vladimir; Zakirzyanov, Ilya; Shalyto, Anatoly (2015). "BFS-Based Symmetry Breaking Predicates for DFA Identification". Language and Automata Theory and Applications. Lecture Notes in Computer Science. Vol.Â 8977. pp.Â 611â622. doi:10.1007/978-3-319-15579-1_48. ISBNÂ 978-3-319-15578-4.


References[edit]
Hopcroft, John E.; Motwani, Rajeev; Ullman, Jeffrey D. (2001). Introduction to Automata Theory, Languages, and Computation (2Â ed.). Addison Wesley. ISBNÂ 0-201-44124-1. Retrieved 19 November 2012.
Lawson, Mark V. (2004). Finite automata. Chapman and Hall/CRC. ISBNÂ 1-58488-255-7. ZblÂ 1086.68074.
McCulloch, W. S.; Pitts, W. (1943). "A Logical Calculus of the Ideas Immanent in Nervous Activity". Bulletin of Mathematical Biophysics. 5 (4): 115â133. doi:10.1007/BF02478259. PMIDÂ 2185863.
Rabin, M. O.; Scott, D. (1959). "Finite automata and their decision problems". IBM J. Res. Dev. 3 (2): 114â125. doi:10.1147/rd.32.0114.
Sakarovitch, Jacques (2009). Elements of automata theory. Translated from the French by Reuben Thomas. Cambridge: Cambridge University Press. ISBNÂ 978-0-521-84425-3. ZblÂ 1188.68177.
Sipser, Michael (1997). Introduction to the Theory of Computation. Boston: PWS. ISBNÂ 0-534-94728-X.. Section 1.1: Finite Automata, pp.Â 31â47. Subsection "Decidable Problems Concerning Regular Languages" of section 4.1: Decidable Languages, pp.Â 152â155.4.4 DFA can accept only regular language
.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}show.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vteAutomata theory: formal languages and formal grammarsChomsky hierarchyGrammarsLanguagesAbstract machines
Type-0
â
Type-1
â
â
â
â
â
Type-2
â
â
Type-3
â
â

Unrestricted
(no common name)
Context-sensitive
Positive range concatenation
Indexed
â
Linear context-free rewriting systems
Tree-adjoining
Context-free
Deterministic context-free
Visibly pushdown
Regular
â
Non-recursive

Recursively enumerable
Decidable
Context-sensitive
Positive range concatenation*
Indexed*
â
Linear context-free rewriting language
Tree-adjoining
Context-free
Deterministic context-free
Visibly pushdown
Regular
Star-free
Finite

Turing machine
Decider
Linear-bounded
PTIME Turing Machine
Nested stack
Thread automaton
restricted Tree stack automaton
Embedded pushdown
Nondeterministic pushdown
Deterministic pushdown
Visibly pushdown
Finite
Counter-free (with aperiodic finite monoid)
Acyclic finite
Each category of languages, except those marked by a *, is a proper subset of the category directly above it. Any language in each category is generated by a grammar and by an automaton in the category in the same line.




<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=Deterministic_finite_automaton&oldid=1068662056"
		Categories: Finite automataHidden categories: Use dmy dates from April 2020All articles with unsourced statementsArticles with unsourced statements from January 2015
	
