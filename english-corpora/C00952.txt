
Title:
Computational complexity of matrix multiplication
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		It has been suggested that Matrix multiplication algorithm be merged into this article. (Discuss) Proposed since September 2021.
algorithmic runtime requirements for matrix multiplication
.mw-parser-output .unsolved{margin:0 1em 1em;border:#ccc solid;padding:0.35em 0.35em 0.35em 2.2em;background-color:#eee;background-image:url("https://upload.wikimedia.org/wikipedia/commons/2/26/Question%2C_Web_Fundamentals.svg");background-position:top 50%left 0.35em;background-size:1.5em;background-repeat:no-repeat}@media(min-width:720px){.mw-parser-output .unsolved{float:right;max-width:25%}}.mw-parser-output .unsolved-label{font-weight:bold}.mw-parser-output .unsolved-body{margin:0.35em;font-style:italic}.mw-parser-output .unsolved-more{font-size:smaller}

Unsolved problem in computer science:
What is the fastest algorithm for matrix multiplication?
(more unsolved problems in computer science)

In theoretical computer science, the computational complexity of matrix multiplication dictates how quickly the operation of matrix multiplication can be performed. Matrix multiplication algorithms are a central subroutine in theoretical and numerical algorithms for numerical linear algebra and optimization, so finding the right amount of time it should take is of major practical relevance.
Directly applying the mathematical definition of matrix multiplication gives an algorithm that requires n3 field operations to multiply two n Ã n matrices over that field (Î(n3) in big O notation). Surprisingly, algorithms exist that provide better running times than this straightforward "schoolbook algorithm". The first to be discovered was Strassen's algorithm, devised by Volker Strassen in 1969 and often referred to as "fast matrix multiplication".[1] The optimal number of field operations needed to multiply two square n Ã n matrices up to constant factors is still unknown. This is a major open question in theoretical computer science.
As of DecemberÂ 2020[update], the matrix multiplication algorithm with best asymptotic complexity runs in O(n2.3728596) time, given by Josh Alman and Virginia Vassilevska Williams.[2][3] However, this and similar improvements to Strassen are not used in practice, because they are galactic algorithms: the constant coefficient hidden by the Big O notation is so large that they are only worthwhile for matrices that are too large to handle on present-day computers.[4][5]

Contents

1 Simple algorithms

1.1 Schoolbook algorithm
1.2 Strassen's algorithm


2 Matrix multiplication exponent

2.1 Group theory reformulation of matrix multiplication algorithms
2.2 Lower bounds for Ï
2.3 Rectangular matrix multiplication


3 Related complexities

3.1 Matrix inversion, determinant and Gaussian elimination


4 See also
5 References



Simple algorithms[edit]
If A, B are n Ã n matrices over a field, then their product AB is also an n Ã n matrix over that field, defined entrywise as


  
    
      
        (
        A
        B
        
          )
          
            i
            j
          
        
        =
        
          â
          
            k
            =
            1
          
          
            n
          
        
        
          A
          
            i
            k
          
        
        
          B
          
            k
            j
          
        
        .
      
    
    {\displaystyle (AB)_{ij}=\sum _{k=1}^{n}A_{ik}B_{kj}.}
  

Schoolbook algorithm[edit]
.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}For implementation techniques (in particular parallel and distributed algorithms), see Matrix multiplication algorithm.
The simplest approach to computing the product of two n Ã n matrices A and B is to compute the arithmetic expressions coming from the definition of matrix multiplication. In pseudocode:

input A and B, both n by n matrices
initialize C to be an n by n matrix of all zeros
for i from 1 to n:
    for j from 1 to n:
        for k from 1 to n:
            C[i][j] = C[i][j] + A[i][k]*B[k][j]
output C (as A*B)

This algorithm requires, in the worst case, 
  
    
      
        
          n
          
            3
          
        
      
    
    {\displaystyle n^{3}}
  
 multiplications of scalars and 
  
    
      
        
          n
          
            3
          
        
        â
        
          n
          
            2
          
        
      
    
    {\displaystyle n^{3}-n^{2}}
  
 additions for computing the product of two square nÃn matrices. Its computational complexity is therefore 
  
    
      
        O
        (
        
          n
          
            3
          
        
        )
      
    
    {\displaystyle O(n^{3})}
  
, in a model of computation where field operations (addition and multiplication) take constant time (in practice, this is the case for floating point numbers, but not necessarily for integers).

Strassen's algorithm[edit]
Main article: Strassen algorithm
Strassen's algorithm improves on naive matrix multiplication through a divide-and-conquer approach. The key observation is that multiplying two 2 Ã 2 matrices can be done with only 7 multiplications, instead of the usual 8 (at the expense of several additional addition and subtraction operations). This means that, treating the input nÃn matrices as block 2 Ã 2 matrices, the task of multiplying nÃn matrices can be reduced to 7 subproblems of multiplying n/2Ãn/2 matrices. Applying this recursively gives an algorithm needing 
  
    
      
        O
        (
        
          n
          
            
              log
              
                2
              
            
            â¡
            7
          
        
        )
        â
        O
        (
        
          n
          
            2.807
          
        
        )
      
    
    {\displaystyle O(n^{\log _{2}7})\approx O(n^{2.807})}
  
 field operations.
Unlike algorithms with faster asymptotic complexity, Strassen's algorithm is used in practice. The numerical stability is reduced compared to the naive algorithm,[6] but it is faster in cases where n > 100 or so[7] and appears in several libraries, such as BLAS.[8] It is very useful for large matrices over exact domains such as finite fields, where numerical stability is not an issue.

Matrix multiplication exponent[edit]
  Improvement of estimates of exponent Ï over time for the computational complexity of matrix multiplication 
  
    
      
        O
        (
        
          n
          
            Ï
          
        
        )
      
    
    {\displaystyle O(n^{\omega })}
  
.

Timeline of matrix multiplication exponent


Year
Bound on omega
Authors


1969
2.8074
Strassen[1]


1978
2.796
Pan[9]


1979
2.780
Bini, Capovani, Romani[10]


1981
2.522
SchÃ¶nhage[11]


1981
2.517
Romani[12]


1981
2.496
Coppersmith, Winograd[13]


1986
2.479
Strassen[14]


1990
2.3755
Coppersmith, Winograd[15]


2010
2.3737
Stothers[16]


2013
2.3729
Williams[17][18]


2014
2.3728639
Le Gall[19]


2020
2.3728596
Alman, Williams[2]

The matrix multiplication exponent, usually denoted Ï, is the smallest real number for which any 
  
    
      
        n
        Ã
        n
      
    
    {\displaystyle n\times n}
  
 matrix over a field can be multiplied together using 
  
    
      
        
          n
          
            Ï
            +
            o
            (
            1
            )
          
        
      
    
    {\displaystyle n^{\omega +o(1)}}
  
 field operations. This notation is commonly used in algorithms research, so that algorithms using matrix multiplication as a subroutine have meaningful bounds on running time regardless of the true value of Ï.
Using a naive lower bound and schoolbook matrix multiplication for the upper bound, one can straightforwardly conclude that 2 â¤ Ï â¤ 3. Whether Ï = 2 is a major open question in theoretical computer science, and there is a line of research developing matrix multiplication algorithms to get improved bounds on Ï.
The current best bound on Ï is Ï < 2.3728596, by Josh Alman and Virginia Vassilevska Williams.[2] This algorithm, like all other recent algorithms in this line of research, uses the laser method, a generalization of the CoppersmithâWinograd algorithm, which was given by Don Coppersmith and Shmuel Winograd in 1990 and was the best matrix multiplication algorithm until 2010.[20] The conceptual idea of these algorithms are similar to Strassen's algorithm: a way is devised for multiplying two k Ã k-matrices with fewer than k3 multiplications, and this technique is applied recursively. The laser method has limitations to its power, and cannot be used to show that Ï < 2.3725.[21]

Group theory reformulation of matrix multiplication algorithms[edit]
Henry Cohn, Robert Kleinberg, BalÃ¡zs Szegedy and Chris Umans put methods such as the Strassen and CoppersmithâWinograd algorithms in an entirely different group-theoretic context, by utilising triples of subsets of finite groups which satisfy a disjointness property called the triple product property (TPP). They also give conjectures that, if true, would imply that there are matrix multiplication algorithms with essentially quadratic complexity. This implies that the optimal exponent of matrix multiplication is 2, which most researchers believe is indeed the case.[5] One such conjecture is that families of wreath products of Abelian groups with symmetric groups realise families of subset triples with a simultaneous version of the TPP.[22][23] Several of their conjectures have since been disproven by Blasiak, Cohn, Church, Grochow, Naslund, Sawin, and Umans using the Slice Rank method.[24] Further, Alon, Shpilka and Chris Umans have recently shown that some of these conjectures implying fast matrix multiplication are incompatible with another plausible conjecture, the sunflower conjecture.[25]

Lower bounds for Ï[edit]
There is a trivial lower bound of 
  
    
      
        Ï
        â¥
        2
      
    
    {\displaystyle \omega \geq 2}
  
. Since any algorithm for multiplying two n Ã n-matrices has to process all 2n2 entries, there is a trivial asymptotic lower bound of Î©(n2) operations for any matrix multiplication algorithm. Thus 
  
    
      
        2
        â¤
        Ï
        <
        2.373
      
    
    {\displaystyle 2\leq \omega <2.373}
  
. It is unknown whether 
  
    
      
        Ï
        >
        2
      
    
    {\displaystyle \omega >2}
  
. The best known lower bound for matrix-multiplication complexity is Î©(n2 log(n)), for bounded coefficient arithmetic circuits over the real or complex numbers, and is due to Ran Raz.[26]

Rectangular matrix multiplication[edit]
Similar techniques also apply to rectangular matrix multiplication. The central object of study is 
  
    
      
        Ï
        (
        k
        )
      
    
    {\displaystyle \omega (k)}
  
, which is the smallest 
  
    
      
        c
      
    
    {\displaystyle c}
  
 such that one can multiply a matrix of size 
  
    
      
        n
        Ã
        â
        
          n
          
            k
          
        
        â
      
    
    {\displaystyle n\times \lceil n^{k}\rceil }
  
 with a matrix of size 
  
    
      
        â
        
          n
          
            k
          
        
        â
        Ã
        n
      
    
    {\displaystyle \lceil n^{k}\rceil \times n}
  
 with 
  
    
      
        O
        (
        
          n
          
            c
            +
            o
            (
            1
            )
          
        
        )
      
    
    {\displaystyle O(n^{c+o(1)})}
  
 arithmetic operations. A result in algebraic complexity states that multiplying matrices of size 
  
    
      
        n
        Ã
        â
        
          n
          
            k
          
        
        â
      
    
    {\displaystyle n\times \lceil n^{k}\rceil }
  
 and 
  
    
      
        â
        
          n
          
            k
          
        
        â
        Ã
        n
      
    
    {\displaystyle \lceil n^{k}\rceil \times n}
  
 requires the same number of arithmetic operations as multiplying matrices of size 
  
    
      
        n
        Ã
        â
        
          n
          
            k
          
        
        â
      
    
    {\displaystyle n\times \lceil n^{k}\rceil }
  
 and 
  
    
      
        n
        Ã
        n
      
    
    {\displaystyle n\times n}
  
 and of size 
  
    
      
        n
        Ã
        n
      
    
    {\displaystyle n\times n}
  
 and 
  
    
      
        n
        Ã
        â
        
          n
          
            k
          
        
        â
      
    
    {\displaystyle n\times \lceil n^{k}\rceil }
  
, so this encompasses the complexity of rectangular matrix multiplication.[27] This generalizes the square matrix multiplication exponent, since 
  
    
      
        Ï
        (
        1
        )
        =
        Ï
      
    
    {\displaystyle \omega (1)=\omega }
  
.
Of interest is proving that, for values of k between 0 and 1, that 
  
    
      
        Ï
        (
        k
        )
        â¤
        2
      
    
    {\displaystyle \omega (k)\leq 2}
  
. 
Since the output of the matrix multiplication problem is size 
  
    
      
        
          n
          
            2
          
        
      
    
    {\displaystyle n^{2}}
  
, 
  
    
      
        Ï
        (
        k
        )
        â¥
        2
      
    
    {\displaystyle \omega (k)\geq 2}
  
 always, so these results show that 
  
    
      
        Ï
        (
        k
        )
        =
        2
      
    
    {\displaystyle \omega (k)=2}
  
 exactly. The largest k such that 
  
    
      
        Ï
        (
        k
        )
        =
        2
      
    
    {\displaystyle \omega (k)=2}
  
 is known as the dual matrix multiplication exponent, usually denoted Î±. Î± is referred to as the "dual" because showing that 
  
    
      
        Î±
        =
        1
      
    
    {\displaystyle \alpha =1}
  
 is equivalent to showing that 
  
    
      
        Ï
        =
        2
      
    
    {\displaystyle \omega =2}
  
. Like the matrix multiplication exponent, the dual matrix multiplication exponent sometimes appears in the complexity of algorithms in numerical linear algebra and optimization.[28]
The first bound on Î± is by Coppersmith in 1982, who showed that 
  
    
      
        Î±
        >
        0.17227
      
    
    {\displaystyle \alpha >0.17227}
  
.[29] The current best bound on Î± is 
  
    
      
        Î±
        >
        0.31389
      
    
    {\displaystyle \alpha >0.31389}
  
, given by Le Gall and Urrutia.[30] This paper also contains bounds on 
  
    
      
        Ï
        (
        k
        )
      
    
    {\displaystyle \omega (k)}
  
.

Related complexities[edit]
Further information: Computational complexity of mathematical operations Â§Â Matrix algebra
Problems that have the same asymptotic complexity as matrix multiplication include determinant, matrix inversion, Gaussian elimination (see next section). Problems with complexity that is expressible in terms of 
  
    
      
        Ï
      
    
    {\displaystyle \omega }
  
 include characteristic polynomial, eigenvalues (but not eigenvectors), Hermite normal form, and Smith normal form.[citation needed]

Matrix inversion, determinant and Gaussian elimination[edit]
In his 1969 paper, where he proved the complexity 
  
    
      
        O
        (
        
          n
          
            2.807
          
        
        )
      
    
    {\displaystyle O(n^{2.807})}
  
 for matrix computation, Strassen proved also that matrix inversion, determinant and Gaussian elimination have, up to a multiplicative constant, the same computational complexity as matrix multiplication. The proof does not make any assumptions on matrix multiplication that is used, except that its complexity is 
  
    
      
        O
        (
        
          n
          
            Ï
          
        
        )
      
    
    {\displaystyle O(n^{\omega })}
  
 for some 
  
    
      
        Ï
        â¥
        2
      
    
    {\displaystyle \omega \geq 2}
  

The starting point of Strassen's proof is using block matrix multiplication. Specifically, a matrix of even dimension 2nÃ2n may be partitioned in four nÃn blocks


  
    
      
        
          
            [
            
              
                
                  
                    A
                  
                
                
                  
                    B
                  
                
              
              
                
                  
                    C
                  
                
                
                  
                    D
                  
                
              
            
            ]
          
        
        .
      
    
    {\displaystyle {\begin{bmatrix}{A}&{B}\\{C}&{D}\end{bmatrix}}.}
  

Under this form, its inverse is 


  
    
      
        
          
            
              [
              
                
                  
                    
                      A
                    
                  
                  
                    
                      B
                    
                  
                
                
                  
                    
                      C
                    
                  
                  
                    
                      D
                    
                  
                
              
              ]
            
          
          
            â
            1
          
        
        =
        
          
            [
            
              
                
                  
                    
                      A
                    
                    
                      â
                      1
                    
                  
                  +
                  
                    
                      A
                    
                    
                      â
                      1
                    
                  
                  
                    B
                  
                  (
                  
                    D
                  
                  â
                  
                    
                      C
                      A
                    
                    
                      â
                      1
                    
                  
                  
                    B
                  
                  
                    )
                    
                      â
                      1
                    
                  
                  
                    
                      C
                      A
                    
                    
                      â
                      1
                    
                  
                
                
                  â
                  
                    
                      A
                    
                    
                      â
                      1
                    
                  
                  
                    B
                  
                  (
                  
                    D
                  
                  â
                  
                    
                      C
                      A
                    
                    
                      â
                      1
                    
                  
                  
                    B
                  
                  
                    )
                    
                      â
                      1
                    
                  
                
              
              
                
                  â
                  (
                  
                    D
                  
                  â
                  
                    
                      C
                      A
                    
                    
                      â
                      1
                    
                  
                  
                    B
                  
                  
                    )
                    
                      â
                      1
                    
                  
                  
                    
                      C
                      A
                    
                    
                      â
                      1
                    
                  
                
                
                  (
                  
                    D
                  
                  â
                  
                    
                      C
                      A
                    
                    
                      â
                      1
                    
                  
                  
                    B
                  
                  
                    )
                    
                      â
                      1
                    
                  
                
              
            
            ]
          
        
        ,
      
    
    {\displaystyle {\begin{bmatrix}{A}&{B}\\{C}&{D}\end{bmatrix}}^{-1}={\begin{bmatrix}{A}^{-1}+{A}^{-1}{B}({D}-{CA}^{-1}{B})^{-1}{CA}^{-1}&-{A}^{-1}{B}({D}-{CA}^{-1}{B})^{-1}\\-({D}-{CA}^{-1}{B})^{-1}{CA}^{-1}&({D}-{CA}^{-1}{B})^{-1}\end{bmatrix}},}
  

provided that A and 
  
    
      
        
          D
        
        â
        
          
            C
            A
          
          
            â
            1
          
        
        
          B
        
      
    
    {\displaystyle {D}-{CA}^{-1}{B}}
  
 are invertible.
Thus, the inverse of a 2nÃ2n matrix may be computed with two inversions, six multiplications and four additions or additive inverses of nÃn matrices. It follows that, denoting respectively by I(n), M(n) and A(n) = n2 the number of operations needed for inverting, multiplying and adding nÃn matrices, one has 


  
    
      
        I
        (
        2
        n
        )
        â¤
        2
        I
        (
        n
        )
        +
        6
        M
        (
        n
        )
        +
        4
        A
        (
        n
        )
        .
      
    
    {\displaystyle I(2n)\leq 2I(n)+6M(n)+4A(n).}
  

If 
  
    
      
        n
        =
        
          2
          
            k
          
        
        ,
      
    
    {\displaystyle n=2^{k},}
  
 one may apply this formula recursively:


  
    
      
        
          
            
              
                I
                (
                
                  2
                  
                    k
                  
                
                )
              
              
                
                â¤
                2
                I
                (
                
                  2
                  
                    k
                    â
                    1
                  
                
                )
                +
                6
                M
                (
                
                  2
                  
                    k
                    â
                    1
                  
                
                )
                +
                4
                A
                (
                
                  2
                  
                    k
                    â
                    1
                  
                
                )
              
            
            
              
              
                
                â¤
                
                  2
                  
                    2
                  
                
                I
                (
                
                  2
                  
                    k
                    â
                    2
                  
                
                )
                +
                6
                (
                M
                (
                
                  2
                  
                    k
                    â
                    1
                  
                
                )
                +
                2
                M
                (
                
                  2
                  
                    k
                    â
                    2
                  
                
                )
                )
                +
                4
                (
                A
                (
                
                  2
                  
                    k
                    â
                    1
                  
                
                )
                +
                2
                A
                (
                
                  2
                  
                    k
                    â
                    2
                  
                
                )
                )
              
            
            
              
              
                
                
                
                
                â®
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}I(2^{k})&\leq 2I(2^{k-1})+6M(2^{k-1})+4A(2^{k-1})\\&\leq 2^{2}I(2^{k-2})+6(M(2^{k-1})+2M(2^{k-2}))+4(A(2^{k-1})+2A(2^{k-2}))\\&\,\,\,\vdots \end{aligned}}}
  

If 
  
    
      
        M
        (
        n
        )
        â¤
        c
        
          n
          
            Ï
          
        
        ,
      
    
    {\displaystyle M(n)\leq cn^{\omega },}
  
 and 
  
    
      
        Î±
        =
        
          2
          
            Ï
          
        
        â¥
        4
        ,
      
    
    {\displaystyle \alpha =2^{\omega }\geq 4,}
  
 one gets eventually 


  
    
      
        
          
            
              
                I
                (
                
                  2
                  
                    k
                  
                
                )
              
              
                
                â¤
                
                  2
                  
                    k
                  
                
                I
                (
                1
                )
                +
                6
                c
                (
                
                  Î±
                  
                    k
                    â
                    1
                  
                
                +
                2
                
                  Î±
                  
                    k
                    â
                    2
                  
                
                +
                â¯
                +
                
                  2
                  
                    k
                    â
                    1
                  
                
                
                  Î±
                  
                    0
                  
                
                )
                +
                k
                
                  2
                  
                    k
                    +
                    1
                  
                
              
            
            
              
              
                
                â¤
                
                  2
                  
                    k
                  
                
                +
                6
                c
                
                  
                    
                      
                        Î±
                        
                          k
                        
                      
                      â
                      
                        2
                        
                          k
                        
                      
                    
                    
                      Î±
                      â
                      2
                    
                  
                
                +
                k
                
                  2
                  
                    k
                    +
                    1
                  
                
              
            
            
              
              
                
                â¤
                d
                (
                
                  2
                  
                    k
                  
                
                
                  )
                  
                    Ï
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}I(2^{k})&\leq 2^{k}I(1)+6c(\alpha ^{k-1}+2\alpha ^{k-2}+\cdots +2^{k-1}\alpha ^{0})+k2^{k+1}\\&\leq 2^{k}+6c{\frac {\alpha ^{k}-2^{k}}{\alpha -2}}+k2^{k+1}\\&\leq d(2^{k})^{\omega }.\end{aligned}}}
  

for some constant d.
For matrices whose dimension is not a power of two, the same complexity is reached by increasing the dimension of the matrix to a power of two, by padding the matrix with rows and columns whose entries are 1 on the diagonal and 0 elsewhere.
This proves the asserted complexity for matrices such that all submatrices that have to be inverted are indeed invertible. This complexity is thus proved for almost all matrices, as a matrix with randomly chosen entries is invertible with probability one.
The same argument applies to LU decomposition, as, if the matrix A is invertible, the equality


  
    
      
        
          
            [
            
              
                
                  
                    A
                  
                
                
                  
                    B
                  
                
              
              
                
                  
                    C
                  
                
                
                  
                    D
                  
                
              
            
            ]
          
        
        =
        
          
            [
            
              
                
                  I
                
                
                  0
                
              
              
                
                  C
                  
                    A
                    
                      â
                      1
                    
                  
                
                
                  I
                
              
            
            ]
          
        
        
        
          
            [
            
              
                
                  A
                
                
                  B
                
              
              
                
                  0
                
                
                  D
                  â
                  C
                  
                    A
                    
                      â
                      1
                    
                  
                  B
                
              
            
            ]
          
        
      
    
    {\displaystyle {\begin{bmatrix}{A}&{B}\\{C}&{D}\end{bmatrix}}={\begin{bmatrix}I&0\\CA^{-1}&I\end{bmatrix}}\,{\begin{bmatrix}A&B\\0&D-CA^{-1}B\end{bmatrix}}}
  

defines a block LU decomposition that may be applied recursively to 
  
    
      
        A
      
    
    {\displaystyle A}
  
 and 
  
    
      
        D
        â
        C
        
          A
          
            â
            1
          
        
        B
        ,
      
    
    {\displaystyle D-CA^{-1}B,}
  
 for getting eventually a true LU decomposition of the original matrix.
The argument applies also for the determinant, since it results from the block LU decomposition that 


  
    
      
        det
        
          
            [
            
              
                
                  
                    A
                  
                
                
                  
                    B
                  
                
              
              
                
                  
                    C
                  
                
                
                  
                    D
                  
                
              
            
            ]
          
        
        =
        det
        (
        A
        )
        det
        (
        D
        â
        C
        
          A
          
            â
            1
          
        
        B
        )
        .
      
    
    {\displaystyle \det {\begin{bmatrix}{A}&{B}\\{C}&{D}\end{bmatrix}}=\det(A)\det(D-CA^{-1}B).}
  

See also[edit]
Computational complexity of mathematical operations
CYK algorithm, Â§Valiant's algorithm
Freivalds' algorithm, a simple Monte Carlo algorithm that, given matrices A, B and C, verifies in Î(n2) time if AB = C.
Matrix chain multiplication
Matrix multiplication, for abstract definitions
Matrix multiplication algorithm, for practical implementation details
Sparse matrix-vector multiplication
References[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ Jump up to: a b 
.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Volker Strassen (Aug 1969). "Gaussian elimination is not optimal". Numerische Mathematik. 13 (4): 354â356. doi:10.1007/BF02165411. S2CIDÂ 121656251.

^ Jump up to: a b c 
Alman, Josh; Williams, Virginia Vassilevska (2020), "A Refined Laser Method and Faster Matrix Multiplication", 32nd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2021), arXiv:2010.05846

^ Hartnett, Kevin. "Matrix Multiplication Inches Closer to Mythic Goal". Quanta Magazine. Retrieved 2021-04-01.

^ Iliopoulos, Costas S. (1989), "Worst-case complexity bounds on algorithms for computing the canonical structure of finite abelian groups and the Hermite and Smith normal forms of an integer matrix" (PDF), SIAM Journal on Computing, 18 (4): 658â669, CiteSeerXÂ 10.1.1.531.9309, doi:10.1137/0218045, MRÂ 1004789, archived from the original (PDF) on 2014-03-05, retrieved 2015-01-16, The CoppersmithâWinograd algorithm is not practical, due to the very large hidden constant in the upper bound on the number of multiplications required.

^ Jump up to: a b Robinson, Sara (November 2005), "Toward an Optimal Algorithm for Matrix Multiplication" (PDF), SIAM News, 38 (9), Even if someone manages to prove one of the conjecturesâthereby demonstrating that Ï = 2âthe wreath product approach is unlikely to be applicable to the large matrix problems that arise in practice. [...] the input matrices must be astronomically large for the difference in time to be apparent.

^ Miller, Webb (1975), "Computational complexity and numerical stability", SIAM News, 4 (2): 97â107, CiteSeerXÂ 10.1.1.148.9947, doi:10.1137/0204009

^ Skiena, Steven (2008). "Sorting and Searching". The Algorithm Design Manual. Springer. pp.Â 45â46, 401â403. doi:10.1007/978-1-84800-070-4_4. ISBNÂ 978-1-84800-069-8.

^ Press, William H.; Flannery, Brian P.; Teukolsky, Saul A.; Vetterling, William T. (2007). Numerical Recipes: The Art of Scientific Computing (3rdÂ ed.). Cambridge University Press. p.Â 108. ISBNÂ 978-0-521-88068-8.

^ 
Victor Yakovlevich Pan (Oct 1978). "Strassen's Algorithm is not Optimal: Trilinear Technique of Aggregating, Uniting and Canceling for Constructing Fast Algorithms for Matrix Operations". Proc. 19th FOCS. pp.Â 166â176. doi:10.1109/SFCS.1978.34. S2CIDÂ 14348408.

^ 
Dario Andrea Bini; Milvio Capovani; Francesco Romani; Grazia Lotti (Jun 1979). "
  
    
      
        O
        (
        
          n
          
            2.7799
          
        
        )
      
    
    {\displaystyle O(n^{2.7799})}
  
 complexity for 
  
    
      
        n
        Ã
        n
      
    
    {\displaystyle n\times n}
  
 approximate matrix multiplication". Information Processing Letters. 8 (5): 234â235. doi:10.1016/0020-0190(79)90113-3.

^ 
A. SchÃ¶nhage (1981). "Partial and total matrix multiplication". SIAM Journal on Computing. 10 (3): 434â455. doi:10.1137/0210032.

^ 
Francesco Romani (1982). "Some properties of disjoint sums of tensors related to matrix multiplication". SIAM Journal on Computing. 11 (2): 263â267. doi:10.1137/0211020.

^ 
D. Coppersmith; S. Winograd (1981). "On the asymptotic complexity of matrix multiplication". Proc. 22nd Annual Symposium on Foundations of Computer Science (FOCS). pp.Â 82â90. doi:10.1109/SFCS.1981.27. S2CIDÂ 206558664.

^ 
Volker Strassen (Oct 1986). "The asymptotic spectrum of tensors and the exponent of matrix multiplication". Proc. 27th Ann. Symp. on Foundation of Computer Science (FOCS). pp.Â 49â54. doi:10.1109/SFCS.1986.52. S2CIDÂ 15077423.

^ 
D. Coppersmith; S. Winograd (Mar 1990). "Matrix multiplication via arithmetic progressions". Journal of Symbolic Computation. 9 (3): 251â280. doi:10.1016/S0747-7171(08)80013-2.

^ 
Stothers, Andrew James (2010). On the complexity of matrix multiplication (Ph.D. thesis). University of Edinburgh.

^ 
Virginia Vassilevska Williams (2012). "Multiplying Matrices Faster than Coppersmith-Winograd".  In Howard J. Karloff; Toniann Pitassi (eds.). Proc. 44th Symposium on Theory of Computing (STOC). ACM. pp.Â 887â898. doi:10.1145/2213977.2214056.

^ 
Williams, Virginia Vassilevska. Multiplying matrices in 
  
    
      
        O
        (
        
          n
          
            2.373
          
        
        )
      
    
    {\displaystyle O(n^{2.373})}
  
 time (PDF) (Technical Report). Stanford University.

^ 
Le Gall, FranÃ§ois (2014), "Powers of tensors and fast matrix multiplication",  in Katsusuke Nabeshima (ed.), Proceedings of the 39th International Symposium on Symbolic and Algebraic Computation (ISSAC), pp.Â 296â303, arXiv:1401.7714, Bibcode:2014arXiv1401.7714L, ISBNÂ 978-1-4503-2501-1

^ Coppersmith, Don; Winograd, Shmuel (1990), "Matrix multiplication via arithmetic progressions" (PDF), Journal of Symbolic Computation, 9 (3): 251, doi:10.1016/S0747-7171(08)80013-2

^ Ambainis, Andris; Filmus, Yuval; Le Gall, FranÃ§ois (2015-06-14). "Fast Matrix Multiplication: Limitations of the Coppersmith-Winograd Method". Proceedings of the forty-seventh annual ACM symposium on Theory of Computing. STOC '15. Portland, Oregon, USA: Association for Computing Machinery: 585â593. doi:10.1145/2746539.2746554. ISBNÂ 978-1-4503-3536-2.

^ Cohn, H.; Kleinberg, R.; Szegedy, B.; Umans, C. (2005). "Group-theoretic Algorithms for Matrix Multiplication". 46th Annual IEEE Symposium on Foundations of Computer Science (FOCS'05). p.Â 379. doi:10.1109/SFCS.2005.39. ISBNÂ 0-7695-2468-0. S2CIDÂ 41278294.

^ Henry Cohn, Chris Umans. A Group-theoretic Approach to Fast Matrix Multiplication. arXiv:math.GR/0307321. Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science, 11â14 October 2003, Cambridge, MA, IEEE Computer Society, pp.Â 438â449.

^ Blasiak, J.; Cohn, H.; Church, T.; Grochow, J.; Naslund, E.; Sawin, W.; Umans, C. (2017). "On cap sets and the group-theoretic approach to matrix multiplication". Discrete Analysis. doi:10.19086/da.1245. S2CIDÂ 9687868.

^ Alon, Shpilka, Umans, On Sunflowers and Matrix Multiplication

^ Raz, Ran (2002). "On the complexity of matrix product". Proceedings of the Thirty-fourth Annual ACM Symposium on Theory of Computing: 144. doi:10.1145/509907.509932. ISBNÂ 1581134959. S2CIDÂ 9582328.

^ Gall, Francois Le; Urrutia, Florent (2018-01-01), "Improved Rectangular Matrix Multiplication using Powers of the Coppersmith-Winograd Tensor", Proceedings of the 2018 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), Proceedings, Society for Industrial and Applied Mathematics, pp.Â 1029â1046, arXiv:1708.05622, doi:10.1137/1.9781611975031.67, retrieved 2021-05-23

^ Cohen, Michael B.; Lee, Yin Tat; Song, Zhao (2021-01-05). "Solving Linear Programs in the Current Matrix Multiplication Time". Journal of the ACM. 68 (1): 3:1â3:39. arXiv:1810.07896. doi:10.1145/3424305. ISSNÂ 0004-5411.

^ Coppersmith, D. (1982-08-01). "Rapid Multiplication of Rectangular Matrices". SIAM Journal on Computing. 11 (3): 467â471. doi:10.1137/0211037. ISSNÂ 0097-5397.

^ Le Gall, Francois; Urrutia, Florent (2018-01-01), "Improved Rectangular Matrix Multiplication using Powers of the Coppersmith-Winograd Tensor", Proceedings of the 2018 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), Proceedings, Society for Industrial and Applied Mathematics, pp.Â 1029â1046, arXiv:1708.05622, doi:10.1137/1.9781611975031.67, retrieved 2021-05-23






<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=Computational_complexity_of_matrix_multiplication&oldid=1062495705"
		Categories: Computer arithmetic algorithmsComputational complexity theoryMatrix theoryUnsolved problems in computer scienceHidden categories: Articles to be merged from September 2021All articles to be mergedArticles with short descriptionPages with lower-case short descriptionShort description matches WikidataArticles containing potentially dated statements from December 2020All articles containing potentially dated statementsAll articles with unsourced statementsArticles with unsourced statements from March 2018
	
