
Title:
Restricted Boltzmann machine
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Class of artificial neural network
.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}Part of a series onMachine learningand data mining
showProblems
Classification
Clustering
Regression
Anomaly detection
Data Cleaning
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction

showSupervised learning.mw-parser-output .nobold{font-weight:normal}(classificationÂ â¢ regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)

showClustering
BIRCH
CURE
Hierarchical
k-means
Expectationâmaximization (EM)
DBSCAN
OPTICS
Mean shift

showDimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t-SNE

showStructured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

showAnomaly detection
k-NN
Local outlier factor

hideArtificial neural network
Autoencoder
Cognitive computing
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
ESN
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net
Transformer
Vision
Spiking neural network
Memtransistor
Electrochemical RAM (ECRAM)

showReinforcement learning
Q-learning
SARSA
Temporal difference (TD)

showTheory
Kernel machines
Biasâvariance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory

showMachine-learning venues
NeurIPS
ICML
ML
JMLR
ArXiv:cs.LG

showRelated articles
Glossary of artificial intelligence
List of datasets for machine-learning research
Outline of machine learning
.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vte
  Diagram of a restricted Boltzmann machine with three visible units and four hidden units (no bias units).
A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.
RBMs were initially invented under the name Harmonium by Paul Smolensky in 1986,[1]
and rose to prominence after Geoffrey Hinton and collaborators invented fast learning algorithms for them in the mid-2000. RBMs have found applications in dimensionality reduction,[2]
classification,[3]
collaborative filtering,[4]  feature learning,[5]
topic modelling[6]
and even many body quantum mechanics.[7][8] They can be trained in either supervised or unsupervised ways, depending on the task.
As their name implies, RBMs are a variant of Boltzmann machines, with the restriction that their neurons must form a bipartite graph: 
a pair of nodes from each of the two groups of units (commonly referred to as the "visible" and "hidden" units respectively) may have a symmetric connection between them; and there are no connections between nodes within a group. By contrast, "unrestricted" Boltzmann machines may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of Boltzmann machines, in particular the gradient-based contrastive divergence algorithm.[9]
Restricted Boltzmann machines can also be used in deep learning networks. In particular, deep belief networks can be formed by "stacking" RBMs and optionally fine-tuning the resulting deep network with gradient descent and backpropagation.[10]

Contents

1 Structure

1.1 Relation to other models


2 Training algorithm
3 Literature
4 See also
5 References
6 External links



Structure[edit]
The standard type of RBM has binary-valued (Boolean) hidden and visible units, and consists of a matrix of weights 
  
    
      
        W
      
    
    {\displaystyle W}
  
 of size 
  
    
      
        m
        Ã
        n
      
    
    {\displaystyle m\times n}
  
. Each weight element 
  
    
      
        (
        
          w
          
            i
            ,
            j
          
        
        )
      
    
    {\displaystyle (w_{i,j})}
  
 of the matrix is associated with the connection between the visible (input) unit 
  
    
      
        
          v
          
            i
          
        
      
    
    {\displaystyle v_{i}}
  
 and the hidden unit 
  
    
      
        
          h
          
            j
          
        
      
    
    {\displaystyle h_{j}}
  
. In addition, there are bias weights (offsets) 
  
    
      
        
          a
          
            i
          
        
      
    
    {\displaystyle a_{i}}
  
 for 
  
    
      
        
          v
          
            i
          
        
      
    
    {\displaystyle v_{i}}
  
 and 
  
    
      
        
          b
          
            j
          
        
      
    
    {\displaystyle b_{j}}
  
 for 
  
    
      
        
          h
          
            j
          
        
      
    
    {\displaystyle h_{j}}
  
. Given the weights and biases, the energy of a configuration (pair of boolean vectors) (v,h) is defined as


  
    
      
        E
        (
        v
        ,
        h
        )
        =
        â
        
          â
          
            i
          
        
        
          a
          
            i
          
        
        
          v
          
            i
          
        
        â
        
          â
          
            j
          
        
        
          b
          
            j
          
        
        
          h
          
            j
          
        
        â
        
          â
          
            i
          
        
        
          â
          
            j
          
        
        
          v
          
            i
          
        
        
          w
          
            i
            ,
            j
          
        
        
          h
          
            j
          
        
      
    
    {\displaystyle E(v,h)=-\sum _{i}a_{i}v_{i}-\sum _{j}b_{j}h_{j}-\sum _{i}\sum _{j}v_{i}w_{i,j}h_{j}}
  

or, in matrix notation,


  
    
      
        E
        (
        v
        ,
        h
        )
        =
        â
        
          a
          
            
              T
            
          
        
        v
        â
        
          b
          
            
              T
            
          
        
        h
        â
        
          v
          
            
              T
            
          
        
        W
        h
        .
      
    
    {\displaystyle E(v,h)=-a^{\mathrm {T} }v-b^{\mathrm {T} }h-v^{\mathrm {T} }Wh.}
  

This energy function is analogous to that of a Hopfield network. As with general Boltzmann machines, the joint probability distribution for the visible and hidden vectors is defined in terms of the energy function as follows,[11]


  
    
      
        P
        (
        v
        ,
        h
        )
        =
        
          
            1
            Z
          
        
        
          e
          
            â
            E
            (
            v
            ,
            h
            )
          
        
      
    
    {\displaystyle P(v,h)={\frac {1}{Z}}e^{-E(v,h)}}
  

where 
  
    
      
        Z
      
    
    {\displaystyle Z}
  
 is a partition function defined as the sum of 
  
    
      
        
          e
          
            â
            E
            (
            v
            ,
            h
            )
          
        
      
    
    {\displaystyle e^{-E(v,h)}}
  
 over all possible configurations, which can be interpreted as a normalizing constant to ensure that the probabilities sum to 1. The marginal probability of a visible vector is the sum of 
  
    
      
        P
        (
        v
        ,
        h
        )
      
    
    {\displaystyle P(v,h)}
  
 over all possible hidden layer configurations,[11]


  
    
      
        P
        (
        v
        )
        =
        
          
            1
            Z
          
        
        
          â
          
            {
            h
            }
          
        
        
          e
          
            â
            E
            (
            v
            ,
            h
            )
          
        
      
    
    {\displaystyle P(v)={\frac {1}{Z}}\sum _{\{h\}}e^{-E(v,h)}}
  
,
and vice versa. Since the underlying graph structure of the RBM is bipartite (meaning there is no intra-layer connections), the hidden unit activations are mutually independent given the visible unit activations. Conversely, the visible unit activations are mutually independent given the hidden unit activations.[9] That is, for m visible units and n hidden units, the conditional probability of a configuration of the visible units v, given a configuration of the hidden units h, is


  
    
      
        P
        (
        v
        
          |
        
        h
        )
        =
        
          â
          
            i
            =
            1
          
          
            m
          
        
        P
        (
        
          v
          
            i
          
        
        
          |
        
        h
        )
      
    
    {\displaystyle P(v|h)=\prod _{i=1}^{m}P(v_{i}|h)}
  
.
Conversely, the conditional probability of h given v is


  
    
      
        P
        (
        h
        
          |
        
        v
        )
        =
        
          â
          
            j
            =
            1
          
          
            n
          
        
        P
        (
        
          h
          
            j
          
        
        
          |
        
        v
        )
      
    
    {\displaystyle P(h|v)=\prod _{j=1}^{n}P(h_{j}|v)}
  
.
The individual activation probabilities are given by


  
    
      
        P
        (
        
          h
          
            j
          
        
        =
        1
        
          |
        
        v
        )
        =
        Ï
        
          (
          
            
              b
              
                j
              
            
            +
            
              â
              
                i
                =
                1
              
              
                m
              
            
            
              w
              
                i
                ,
                j
              
            
            
              v
              
                i
              
            
          
          )
        
      
    
    {\displaystyle P(h_{j}=1|v)=\sigma \left(b_{j}+\sum _{i=1}^{m}w_{i,j}v_{i}\right)}
  
 and 
  
    
      
        
        P
        (
        
          v
          
            i
          
        
        =
        1
        
          |
        
        h
        )
        =
        Ï
        
          (
          
            
              a
              
                i
              
            
            +
            
              â
              
                j
                =
                1
              
              
                n
              
            
            
              w
              
                i
                ,
                j
              
            
            
              h
              
                j
              
            
          
          )
        
      
    
    {\displaystyle \,P(v_{i}=1|h)=\sigma \left(a_{i}+\sum _{j=1}^{n}w_{i,j}h_{j}\right)}
  

where 
  
    
      
        Ï
      
    
    {\displaystyle \sigma }
  
 denotes the logistic sigmoid.
The visible units of Restricted Boltzmann Machine can be multinomial, although the hidden units are Bernoulli.[clarification needed] In this case, the logistic function for visible units is replaced by the softmax function


  
    
      
        P
        (
        
          v
          
            i
          
          
            k
          
        
        =
        1
        
          |
        
        h
        )
        =
        
          
            
              exp
              â¡
              (
              
                a
                
                  i
                
                
                  k
                
              
              +
              
                Î£
                
                  j
                
              
              
                W
                
                  i
                  j
                
                
                  k
                
              
              
                h
                
                  j
                
              
              )
            
            
              
                Î£
                
                  
                    k
                    â²
                  
                  =
                  1
                
                
                  K
                
              
              exp
              â¡
              (
              
                a
                
                  i
                
                
                  
                    k
                    â²
                  
                
              
              +
              
                Î£
                
                  j
                
              
              
                W
                
                  i
                  j
                
                
                  
                    k
                    â²
                  
                
              
              
                h
                
                  j
                
              
              )
            
          
        
      
    
    {\displaystyle P(v_{i}^{k}=1|h)={\frac {\exp(a_{i}^{k}+\Sigma _{j}W_{ij}^{k}h_{j})}{\Sigma _{k'=1}^{K}\exp(a_{i}^{k'}+\Sigma _{j}W_{ij}^{k'}h_{j})}}}
  

where K is the number of discrete values that the visible values have. They are applied in topic modeling,[6] and recommender systems.[4]

Relation to other models[edit]
Restricted Boltzmann machines are a special case of Boltzmann machines and Markov random fields.[12][13]
Their graphical model corresponds to that of factor analysis.[14]

Training algorithm[edit]
Restricted Boltzmann machines are trained to maximize the product of probabilities assigned to some training set 
  
    
      
        V
      
    
    {\displaystyle V}
  
 (a matrix, each row of which is treated as a visible vector 
  
    
      
        v
      
    
    {\displaystyle v}
  
),


  
    
      
        arg
        â¡
        
          max
          
            W
          
        
        
          â
          
            v
            â
            V
          
        
        P
        (
        v
        )
      
    
    {\displaystyle \arg \max _{W}\prod _{v\in V}P(v)}
  

or equivalently, to maximize the expected log probability of a training sample 
  
    
      
        v
      
    
    {\displaystyle v}
  
 selected randomly from 
  
    
      
        V
      
    
    {\displaystyle V}
  
:[12][13]


  
    
      
        arg
        â¡
        
          max
          
            W
          
        
        
          E
        
        
          [
          
            log
            â¡
            P
            (
            v
            )
          
          ]
        
      
    
    {\displaystyle \arg \max _{W}\mathbb {E} \left[\log P(v)\right]}
  

The algorithm most often used to train RBMs, that is, to optimize the weight matrix 
  
    
      
        W
      
    
    {\displaystyle W}
  
, is the contrastive divergence (CD) algorithm due to Hinton, originally developed to train PoE (product of experts) models.[15][16]
The algorithm performs Gibbs sampling and is used inside a gradient descent procedure (similar to the way backpropagation is used inside such a procedure when training feedforward neural nets) to compute weight update.
The basic, single-step contrastive divergence (CD-1) procedure for a single sample can be summarized as follows:

Take a training sample v, compute the probabilities of the hidden units and sample a hidden activation vector h from this probability distribution.
Compute the outer product of v and h and call this the positive gradient.
From h, sample a reconstruction v' of the visible units, then resample the hidden activations h' from this. (Gibbs sampling step)
Compute the outer product of v' and h' and call this the negative gradient.
Let the update to the weight matrix 
  
    
      
        W
      
    
    {\displaystyle W}
  
 be the positive gradient minus the negative gradient, times some learning rate: 
  
    
      
        Î
        W
        =
        Ïµ
        (
        v
        
          h
          
            
              T
            
          
        
        â
        
          v
          â²
        
        
          h
          
            â²
            
              
                T
              
            
          
        
        )
      
    
    {\displaystyle \Delta W=\epsilon (vh^{\mathsf {T}}-v'h'^{\mathsf {T}})}
  
.
Update the biases a and b analogously: 
  
    
      
        Î
        a
        =
        Ïµ
        (
        v
        â
        
          v
          â²
        
        )
      
    
    {\displaystyle \Delta a=\epsilon (v-v')}
  
, 
  
    
      
        Î
        b
        =
        Ïµ
        (
        h
        â
        
          h
          â²
        
        )
      
    
    {\displaystyle \Delta b=\epsilon (h-h')}
  
.
A Practical Guide to Training RBMs written by Hinton can be found on his homepage.[11]

Literature[edit]
.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Fischer, Asja; Igel, Christian (2012), "An Introduction to Restricted Boltzmann Machines", Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications, Berlin, Heidelberg: Springer Berlin Heidelberg, pp.Â 14â36, retrieved 2021-09-19
See also[edit]
Autoencoder
Helmholtz machine
References[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ Smolensky, Paul (1986). "Chapter 6: Information Processing in Dynamical Systems: Foundations of Harmony Theory" (PDF).  In Rumelhart, David E.; McLelland, James L. (eds.). Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations. MIT Press. pp.Â 194â281. ISBNÂ 0-262-68053-X.

^ Hinton, G. E.; Salakhutdinov, R. R. (2006). "Reducing the Dimensionality of Data with Neural Networks" (PDF). Science. 313 (5786): 504â507. Bibcode:2006Sci...313..504H. doi:10.1126/science.1127647. PMIDÂ 16873662. S2CIDÂ 1658773.

^ Larochelle, H.; Bengio, Y. (2008). Classification using discriminative restricted Boltzmann machines (PDF). Proceedings of the 25th international conference on Machine learning - ICML '08. p.Â 536. doi:10.1145/1390156.1390224. ISBNÂ 9781605582054.

^ Jump up to: a b Salakhutdinov, R.; Mnih, A.; Hinton, G. (2007). Restricted Boltzmann machines for collaborative filtering. Proceedings of the 24th international conference on Machine learning - ICML '07. p.Â 791. doi:10.1145/1273496.1273596. ISBNÂ 9781595937933.

^ Coates, Adam; Lee, Honglak; Ng, Andrew Y. (2011). An analysis of single-layer networks in unsupervised feature learning (PDF). International Conference on Artificial Intelligence and Statistics (AISTATS).

^ Jump up to: a b Ruslan Salakhutdinov and Geoffrey Hinton (2010). Replicated softmax: an undirected topic model. Neural Information Processing Systems 23.

^ Carleo, Giuseppe; Troyer, Matthias (2017-02-10). "Solving the quantum many-body problem with artificial neural networks". Science. 355 (6325): 602â606. arXiv:1606.02318. Bibcode:2017Sci...355..602C. doi:10.1126/science.aag2302. ISSNÂ 0036-8075. PMIDÂ 28183973. S2CIDÂ 206651104.

^ Melko, Roger G.; Carleo, Giuseppe; Carrasquilla, Juan; Cirac, J. Ignacio (September 2019). "Restricted Boltzmann machines in quantum physics". Nature Physics. 15 (9): 887â892. Bibcode:2019NatPh..15..887M. doi:10.1038/s41567-019-0545-1. ISSNÂ 1745-2481.

^ Jump up to: a b Miguel Ã. Carreira-PerpiÃ±Ã¡n and Geoffrey Hinton (2005). On contrastive divergence learning. Artificial Intelligence and Statistics.

^ Hinton, G. (2009). "Deep belief networks". Scholarpedia. 4 (5): 5947. Bibcode:2009SchpJ...4.5947H. doi:10.4249/scholarpedia.5947.

^ Jump up to: a b c Geoffrey Hinton (2010). A Practical Guide to Training Restricted Boltzmann Machines. UTML TR 2010â003, University of Toronto.

^ Jump up to: a b Sutskever, Ilya; Tieleman, Tijmen (2010). "On the convergence properties of contrastive divergence" (PDF). Proc. 13th Int'l Conf. On AI and Statistics (AISTATS). Archived from the original (PDF) on 2015-06-10.

^ Jump up to: a b Asja Fischer and Christian Igel. Training Restricted Boltzmann Machines: An Introduction Archived 2015-06-10 at the Wayback Machine. Pattern Recognition 47, pp. 25-39, 2014

^ MarÃ­a AngÃ©lica Cueto; Jason Morton; Bernd Sturmfels (2010). "Geometry of the restricted Boltzmann machine". Algebraic Methods in Statistics and Probability. American Mathematical Society. 516. arXiv:0908.4425. Bibcode:2009arXiv0908.4425A.

^ Geoffrey Hinton (1999). Products of Experts. ICANN 1999.

^ Hinton, G. E. (2002). "Training Products of Experts by Minimizing Contrastive Divergence" (PDF). Neural Computation. 14 (8): 1771â1800. doi:10.1162/089976602760128018. PMIDÂ 12180402. S2CIDÂ 207596505.


External links[edit]
Introduction to Restricted Boltzmann Machines. Edwin Chen's blog, July 18, 2011.
"A Beginner's Guide to Restricted Boltzmann Machines". Archived from the original on February 11, 2017. Retrieved November 15, 2018.{{cite web}}:  CS1 maint: bot: original URL status unknown (link). Deeplearning4j Documentation
"Understanding RBMs". Archived from the original on September 20, 2016. Retrieved December 29, 2014.. Deeplearning4j Documentation
Python implementation of Bernoulli RBM and tutorial
SimpleRBM is a very small RBM code (24kB) useful for you to learn about how RBMs learn and work.
Julia implementation of Restricted Boltzmann machines, with different kinds of layers: https://github.com/cossio/RestrictedBoltzmannMachines.jl




<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=Restricted_Boltzmann_machine&oldid=1057456034"
		Categories: Artificial neural networksStochastic modelsSupervised learningUnsupervised learningHidden categories: Webarchive template wayback linksArticles with short descriptionShort description matches WikidataWikipedia articles needing clarification from April 2021CS1 maint: bot: original URL status unknown
	
