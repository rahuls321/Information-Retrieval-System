
Title:
VapnikâChervonenkis dimension
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Notion in supervised machine learningIn VapnikâChervonenkis theory, the VapnikâChervonenkis (VC) dimension is a measure of the capacity (complexity, expressive power, richness, or flexibility) of a set of functions that can be learned by a statistical binary classification algorithm. It is defined as the cardinality of the largest set of points that the algorithm can shatter, which means the algorithm can always learn a perfect classifier for any labeling of at least one configuration of those data points. It was originally defined by Vladimir Vapnik and Alexey Chervonenkis.[1]
Informally, the capacity of a classification model is related to how complicated it can be. For example, consider the thresholding of a high-degree polynomial: if the polynomial evaluates above zero, that point is classified as positive, otherwise as negative. A high-degree polynomial can be wiggly, so it can fit a given set of training points well. But one can expect that the classifier will make errors on other points, because it is too wiggly. Such a polynomial has a high capacity. A much simpler alternative is to threshold a linear function. This function may not fit the training set well, because it has a low capacity. This notion of capacity is made rigorous below.

Contents

1 Definitions

1.1 VC dimension of a set-family
1.2 VC dimension of a classification model


2 Examples
3 Uses

3.1 In statistical learning theory
3.2 In computational geometry


4 Bounds
5 VC dimension of a finite projective plane
6 VC dimension of a boosting classifier
7 VC dimension of a neural network
8 Generalizations
9 See also
10 Footnotes
11 References



Definitions[edit]
VC dimension of a set-family[edit]
Let 
  
    
      
        H
      
    
    {\displaystyle H}
  
 be a set family (a set of sets) and 
  
    
      
        C
      
    
    {\displaystyle C}
  
 a set. Their intersection is defined as the following set family:


  
    
      
        H
        â©
        C
        :=
        {
        h
        â©
        C
        â£
        h
        â
        H
        }
        .
      
    
    {\displaystyle H\cap C:=\{h\cap C\mid h\in H\}.}
  

We say that a set 
  
    
      
        C
      
    
    {\displaystyle C}
  
 is shattered by 
  
    
      
        H
      
    
    {\displaystyle H}
  
 if 
  
    
      
        H
        â©
        C
      
    
    {\displaystyle H\cap C}
  
 contains all the subsets of 
  
    
      
        C
      
    
    {\displaystyle C}
  
, i.e.:


  
    
      
        
          |
        
        H
        â©
        C
        
          |
        
        =
        
          2
          
            
              |
            
            C
            
              |
            
          
        
        .
      
    
    {\displaystyle |H\cap C|=2^{|C|}.}
  

The VC dimension 
  
    
      
        D
      
    
    {\displaystyle D}
  
 of 
  
    
      
        H
      
    
    {\displaystyle H}
  
 is the largest cardinality of sets shattered by 
  
    
      
        H
      
    
    {\displaystyle H}
  
. If arbitrarily large subsets can be shattered, the VC dimension is 
  
    
      
        â
      
    
    {\displaystyle \infty }
  
.

VC dimension of a classification model[edit]
A binary classification model 
  
    
      
        f
      
    
    {\displaystyle f}
  
 with some parameter vector 
  
    
      
        Î¸
      
    
    {\displaystyle \theta }
  
 is said to shatter a set of data points 
  
    
      
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        â¦
        ,
        
          x
          
            n
          
        
        )
      
    
    {\displaystyle (x_{1},x_{2},\ldots ,x_{n})}
  
 if, for all assignments of labels to those points, there exists a 
  
    
      
        Î¸
      
    
    {\displaystyle \theta }
  
 such that the model 
  
    
      
        f
      
    
    {\displaystyle f}
  
 makes no errors when evaluating that set of data points[citation needed].
The VC dimension of a model 
  
    
      
        f
      
    
    {\displaystyle f}
  
 is the maximum number of points that can be arranged so that 
  
    
      
        f
      
    
    {\displaystyle f}
  
 shatters them.  More formally, it is the maximum cardinal 
  
    
      
        D
      
    
    {\displaystyle D}
  
 such that some data point set of cardinality 
  
    
      
        D
      
    
    {\displaystyle D}
  
 can be shattered by 
  
    
      
        f
      
    
    {\displaystyle f}
  
.

Examples[edit]
1. 
  
    
      
        f
      
    
    {\displaystyle f}
  
 is a constant classifier (with no parameters). Its VC dimension is 0 since it cannot shatter even a single point. In general, the VC dimension of a finite classification model, which can return at most 
  
    
      
        
          2
          
            d
          
        
      
    
    {\displaystyle 2^{d}}
  
 different classifiers, is at most 
  
    
      
        d
      
    
    {\displaystyle d}
  
 (this is an upper bound on the VC dimension; the SauerâShelah lemma gives a lower bound on the dimension).
2. 
  
    
      
        f
      
    
    {\displaystyle f}
  
 is a single-parametric threshold classifier on real numbers; i.e, for a certain threshold 
  
    
      
        Î¸
      
    
    {\displaystyle \theta }
  
, the classifier 
  
    
      
        
          f
          
            Î¸
          
        
      
    
    {\displaystyle f_{\theta }}
  
 returns 1 if the input number is larger than 
  
    
      
        Î¸
      
    
    {\displaystyle \theta }
  
 and 0 otherwise. The VC dimension of 
  
    
      
        f
      
    
    {\displaystyle f}
  
  is 1 because: (a) It can shatter a single point. For every point 
  
    
      
        x
      
    
    {\displaystyle x}
  
, a classifier 
  
    
      
        
          f
          
            Î¸
          
        
      
    
    {\displaystyle f_{\theta }}
  
 labels it as 0 if 
  
    
      
        Î¸
        >
        x
      
    
    {\displaystyle \theta >x}
  
 and labels it as 1 if 
  
    
      
        Î¸
        <
        x
      
    
    {\displaystyle \theta <x}
  
. (b) It cannot shatter any set of two points. For every set of two numbers, if the smaller is labeled 1, then the larger must also be labeled 1, so not all labelings are possible.
3. 
  
    
      
        f
      
    
    {\displaystyle f}
  
 is a single-parametric interval classifier on real numbers; i.e, for a certain parameter 
  
    
      
        Î¸
      
    
    {\displaystyle \theta }
  
, the classifier 
  
    
      
        
          f
          
            Î¸
          
        
      
    
    {\displaystyle f_{\theta }}
  
 returns 1 if the input number is in the interval 
  
    
      
        [
        Î¸
        ,
        Î¸
        +
        4
        ]
      
    
    {\displaystyle [\theta ,\theta +4]}
  
 and 0 otherwise. The VC dimension of 
  
    
      
        f
      
    
    {\displaystyle f}
  
 is 2 because: (a) It can shatter some sets of two points. E.g, for every set 
  
    
      
        {
        x
        ,
        x
        +
        2
        }
      
    
    {\displaystyle \{x,x+2\}}
  
, a classifier 
  
    
      
        
          f
          
            Î¸
          
        
      
    
    {\displaystyle f_{\theta }}
  
 labels it as (0,0) if 
  
    
      
        Î¸
        <
        x
        â
        4
      
    
    {\displaystyle \theta <x-4}
  
 or if 
  
    
      
        Î¸
        >
        x
        +
        2
      
    
    {\displaystyle \theta >x+2}
  
, as (1,0) if 
  
    
      
        Î¸
        â
        [
        x
        â
        4
        ,
        x
        â
        2
        )
      
    
    {\displaystyle \theta \in [x-4,x-2)}
  
, as (1,1) if 
  
    
      
        Î¸
        â
        [
        x
        â
        2
        ,
        x
        ]
      
    
    {\displaystyle \theta \in [x-2,x]}
  
, and as (0,1) if 
  
    
      
        Î¸
        â
        (
        x
        ,
        x
        +
        2
        ]
      
    
    {\displaystyle \theta \in (x,x+2]}
  
.
(b) It cannot shatter any set of three points. For every set of three numbers, if the smallest and the largest are labeled 1, then the middle one must also be labeled 1, so not all labelings are possible.
4. 
  
    
      
        f
      
    
    {\displaystyle f}
  
 is a straight line as a classification model on points in a two-dimensional plane (this is the model used by a perceptron). The line should separate positive data points from negative data points. There exist sets of 3 points that can indeed be shattered using this model (any 3 points that are not collinear can be shattered). However, no set of 4 points can be shattered: by Radon's theorem, any four points can be partitioned into two subsets with intersecting convex hulls, so it is not possible to separate one of these two subsets from the other. Thus, the VC dimension of this particular classifier is 3. It is important to remember that while one can choose any arrangement of points, the arrangement of those points cannot change when attempting to shatter for some label assignment. Note, only 3 of the 23Â =Â 8 possible label assignments are shown for the three points.













3 points shattered

4 points impossible

5. 
  
    
      
        f
      
    
    {\displaystyle f}
  
 is a single-parametric sine classifier,  i.e, for a certain parameter 
  
    
      
        Î¸
      
    
    {\displaystyle \theta }
  
, the classifier 
  
    
      
        
          f
          
            Î¸
          
        
      
    
    {\displaystyle f_{\theta }}
  
 returns 1 if the input number 
  
    
      
        x
      
    
    {\displaystyle x}
  
 has 
  
    
      
        sin
        â¡
        (
        Î¸
        x
        )
        >
        0
      
    
    {\displaystyle \sin(\theta x)>0}
  
 and 0 otherwise. The VC dimension of 
  
    
      
        f
      
    
    {\displaystyle f}
  
 is infinite, since it can shatter any finite subset of the set 
  
    
      
        {
        
          2
          
            â
            m
          
        
        â£
        m
        â
        
          N
        
        }
      
    
    {\displaystyle \{2^{-m}\mid m\in \mathbb {N} \}}
  
.[2]:â57â

Uses[edit]
In statistical learning theory[edit]
The VC dimension can predict a probabilistic upper bound on the test error of a classification model. Vapnik[3] proved that the probability of the test error (i.e., risk with 0-1 loss function) distancing from an upper bound (on data that is drawn i.i.d. from the same distribution as the training set) is given by:


  
    
      
        Pr
        
          (
          
            
              test error
            
            â©½
            
              training error
            
            +
            
              
                
                  
                    1
                    N
                  
                
                
                  [
                  
                    D
                    
                      (
                      
                        log
                        â¡
                        
                          (
                          
                            
                              
                                
                                  2
                                  N
                                
                                D
                              
                            
                          
                          )
                        
                        +
                        1
                      
                      )
                    
                    â
                    log
                    â¡
                    
                      (
                      
                        
                          
                            Î·
                            4
                          
                        
                      
                      )
                    
                  
                  ]
                
              
            
            
          
          )
        
        =
        1
        â
        Î·
        ,
      
    
    {\displaystyle \Pr \left({\text{test error}}\leqslant {\text{training error}}+{\sqrt {{\frac {1}{N}}\left[D\left(\log \left({\tfrac {2N}{D}}\right)+1\right)-\log \left({\tfrac {\eta }{4}}\right)\right]}}\,\right)=1-\eta ,}
  

where 
  
    
      
        D
      
    
    {\displaystyle D}
  
 is the VC dimension of the classification model, 
  
    
      
        0
        <
        Î·
        â©½
        1
      
    
    {\displaystyle 0<\eta \leqslant 1}
  
, and 
  
    
      
        N
      
    
    {\displaystyle N}
  
 is the size of the training set (restriction: this formula is valid when 
  
    
      
        D
        âª
        N
      
    
    {\displaystyle D\ll N}
  
. When 
  
    
      
        D
      
    
    {\displaystyle D}
  
 is larger, the test-error may be much higher than the training-error. This is due to overfitting).
The VC dimension also appears in sample-complexity bounds. A space of binary functions with VC dimension 
  
    
      
        D
      
    
    {\displaystyle D}
  
 can be learned with:


  
    
      
        N
        =
        Î
        
          (
          
            
              
                D
                +
                ln
                â¡
                
                  
                    1
                    Î´
                  
                
              
              Îµ
            
          
          )
        
      
    
    {\displaystyle N=\Theta \left({\frac {D+\ln {1 \over \delta }}{\varepsilon }}\right)}
  

samples, where 
  
    
      
        Îµ
      
    
    {\displaystyle \varepsilon }
  
 is the learning error and 
  
    
      
        Î´
      
    
    {\displaystyle \delta }
  
 is the failure probability. Thus, the sample-complexity is a linear function of the VC dimension of the hypothesis space.

In computational geometry[edit]
The VC dimension is one of the critical parameters in the size of Îµ-nets, which determines the complexity of approximation algorithms based on them; range sets without finite VC dimension may not have finite Îµ-nets at all.

Bounds[edit]
0. The VC dimension of the dual set-family of 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
  
 is strictly less than 
  
    
      
        
          2
          
            vc
            â¡
            (
            
              
                F
              
            
            )
            +
            1
          
        
      
    
    {\displaystyle 2^{\operatorname {vc} ({\mathcal {F}})+1}}
  
, and this is best possible.
1. The VC dimension of a finite set-family 
  
    
      
        H
      
    
    {\displaystyle H}
  
 is at most 
  
    
      
        
          log
          
            2
          
        
        â¡
        
          |
        
        H
        
          |
        
      
    
    {\displaystyle \log _{2}|H|}
  
.[2]:â56â This is because 
  
    
      
        
          |
        
        H
        â©
        C
        
          |
        
        â¤
        
          |
        
        H
        
          |
        
      
    
    {\displaystyle |H\cap C|\leq |H|}
  
 by definition.
2. Given a set-family 
  
    
      
        H
      
    
    {\displaystyle H}
  
, define 
  
    
      
        
          H
          
            s
          
        
      
    
    {\displaystyle H_{s}}
  
 as a set-family that contains all intersections of 
  
    
      
        s
      
    
    {\displaystyle s}
  
 elements of 
  
    
      
        H
      
    
    {\displaystyle H}
  
. Then:[2]:â57â


  
    
      
        VCDim
        â¡
        (
        
          H
          
            s
          
        
        )
        â¤
        VCDim
        â¡
        (
        H
        )
        â
        (
        2
        s
        
          log
          
            2
          
        
        â¡
        (
        3
        s
        )
        )
      
    
    {\displaystyle \operatorname {VCDim} (H_{s})\leq \operatorname {VCDim} (H)\cdot (2s\log _{2}(3s))}
  

3. Given a set-family 
  
    
      
        H
      
    
    {\displaystyle H}
  
 and an element 
  
    
      
        
          h
          
            0
          
        
        â
        H
      
    
    {\displaystyle h_{0}\in H}
  
, define 
  
    
      
        H
        
        Î
        
          h
          
            0
          
        
        :=
        {
        h
        
        Î
        
          h
          
            0
          
        
        â£
        h
        â
        H
        }
      
    
    {\displaystyle H\,\Delta h_{0}:=\{h\,\Delta h_{0}\mid h\in H\}}
  
 where 
  
    
      
        Î
      
    
    {\displaystyle \Delta }
  
 denotes symmetric set difference. Then:[2]:â58â


  
    
      
        VCDim
        â¡
        (
        H
        
        Î
        
          h
          
            0
          
        
        )
        =
        VCDim
        â¡
        (
        H
        )
      
    
    {\displaystyle \operatorname {VCDim} (H\,\Delta h_{0})=\operatorname {VCDim} (H)}
  

VC dimension of a finite projective plane[edit]
A finite projective plane of order n is a collection of n2 + n + 1 sets (called "lines") over n2 + n + 1 elements (called "points"), for which:

Each line contains exactly n + 1 points.
Each line intersects every other line in exactly one point.
Each point is contained in exactly n + 1 lines.
Each point is in exactly one line in common with every other point.
At least four points do not lie in a common line.
The VC dimension of a finite projective plane isÂ 2.[4]
Proof: (a) For each pair of distinct points, there is one line that contains both of them, lines that contain only one of them, and lines that contain none of them, so every set of size 2 is shattered. (b) For any triple of three distinct points, if there is a line x that contain all three, then there is no line y that contains exactly two (since then x and y would intersect in two points, which is contrary to the definition of a projective plane). Hence, no set of size 3 is shattered.

VC dimension of a boosting classifier[edit]
Suppose we have a base class 
  
    
      
        B
      
    
    {\displaystyle B}
  
 of simple classifiers, whose VC dimension is 
  
    
      
        D
      
    
    {\displaystyle D}
  
.
We can construct a more powerful classifier by combining several different classifiers from 
  
    
      
        B
      
    
    {\displaystyle B}
  
; this technique is called boosting. Formally, given 
  
    
      
        T
      
    
    {\displaystyle T}
  
 classifiers 
  
    
      
        
          h
          
            1
          
        
        ,
        â¦
        ,
        
          h
          
            T
          
        
        â
        B
      
    
    {\displaystyle h_{1},\ldots ,h_{T}\in B}
  
 and a weight vector 
  
    
      
        w
        â
        
          
            R
          
          
            T
          
        
      
    
    {\displaystyle w\in \mathbb {R} ^{T}}
  
, we can define the following classifier:


  
    
      
        f
        (
        x
        )
        =
        sign
        â¡
        
          (
          
            
              â
              
                t
                =
                1
              
              
                T
              
            
            
              w
              
                t
              
            
            â
            
              h
              
                t
              
            
            (
            x
            )
          
          )
        
      
    
    {\displaystyle f(x)=\operatorname {sign} \left(\sum _{t=1}^{T}w_{t}\cdot h_{t}(x)\right)}
  

The VC dimension of the set of all such classifiers (for all selections of 
  
    
      
        T
      
    
    {\displaystyle T}
  
 classifiers from 
  
    
      
        B
      
    
    {\displaystyle B}
  
 and a weight-vector from 
  
    
      
        
          
            R
          
          
            T
          
        
      
    
    {\displaystyle \mathbb {R} ^{T}}
  
), assuming 
  
    
      
        T
        ,
        D
        â¥
        3
      
    
    {\displaystyle T,D\geq 3}
  
, is at most:[5]:â108â109â


  
    
      
        T
        â
        (
        D
        +
        1
        )
        â
        (
        3
        log
        â¡
        (
        T
        â
        (
        D
        +
        1
        )
        )
        +
        2
        )
      
    
    {\displaystyle T\cdot (D+1)\cdot (3\log(T\cdot (D+1))+2)}
  

VC dimension of a neural network[edit]
This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.  (May 2021) (Learn how and when to remove this template message)
A neural network is described by a directed acyclic graph G(V,E), where:

V is the set of nodes. Each node is a simple computation cell.
E is the set of edges, Each edge has a weight.
The input to the network is represented by the sources of the graph â the nodes with no incoming edges.
The output of the network is represented by the sinks of the graph â the nodes with no outgoing edges.
Each intermediate node gets as input a weighted sum of the outputs of the nodes at its incoming edges, where the weights are the weights on the edges.
Each intermediate node outputs a certain increasing function of its input, such as the sign function or the sigmoid function. This function is called the activation function.
The VC dimension of a neural network is bounded as follows:[5]:â234â235â

If the activation function is the sign function and the weights are general, then the VC dimension is at most 
  
    
      
        O
        (
        
          |
        
        E
        
          |
        
        â
        log
        â¡
        (
        
          |
        
        E
        
          |
        
        )
        )
      
    
    {\displaystyle O(|E|\cdot \log(|E|))}
  
.
If the activation function is the sigmoid function and the weights are general, then the VC dimension is at least 
  
    
      
        Î©
        (
        
          |
        
        E
        
          
            |
          
          
            2
          
        
        )
      
    
    {\displaystyle \Omega (|E|^{2})}
  
 and at most 
  
    
      
        O
        (
        
          |
        
        E
        
          
            |
          
          
            2
          
        
        â
        
          |
        
        V
        
          
            |
          
          
            2
          
        
        )
      
    
    {\displaystyle O(|E|^{2}\cdot |V|^{2})}
  
.
If the weights come from a finite family (e.g. the weights are real numbers that can be represented by at most 32 bits in a computer), then, for both activation functions, the VC dimension is at most 
  
    
      
        O
        (
        
          |
        
        E
        
          |
        
        )
      
    
    {\displaystyle O(|E|)}
  
.
Generalizations[edit]
The VC dimension is defined for spaces of binary functions (functions to {0,1}). Several generalizations have been suggested for spaces of non-binary functions.

For multi-valued functions (functions to {0,...,n}), the Natarajan dimension[6] can be used. Ben David et al[7] present a generalization of this concept.
For real-valued functions (e.g. functions to a real interval, [0,1]), Pollard's pseudo-dimension[8][9][10] can be used.
The Rademacher complexity provides similar bounds to the VC, and can sometimes provide more insight than VC dimension calculations into such statistical methods such as those using kernels[citation needed].
See also[edit]



Wikimedia Commons has media related to Vapnik-Chervonenkis dimension.

Growth function
SauerâShelah lemma, a bound on the number of sets in a set system in terms of the VC dimension.
KarpinskiâMacintyre theorem,[11] a bound on the VC dimension of general Pfaffian formulas.
Footnotes[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Vapnik, V. N.; Chervonenkis, A. Ya. (1971). "On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities". Theory of Probability & Its Applications. 16 (2): 264. doi:10.1137/1116025.

This is an English translation, by B. Seckler, of the Russian paper: "On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities". Dokl. Akad. Nauk. 181 (4): 781. 1968.

The translation was reproduced as:
Vapnik, V. N.; Chervonenkis, A. Ya. (2015). "On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities". Measures of Complexity. p.Â 11. doi:10.1007/978-3-319-21852-6_3. ISBNÂ 978-3-319-21851-9.

^ Jump up to: a b c d Mohri, Mehryar; Rostamizadeh, Afshin; Talwalkar, Ameet (2012). Foundations of Machine Learning. USA, Massachusetts: MIT Press. ISBNÂ 9780262018258.

^ Vapnik 2000.

^ Alon, N.; Haussler, D.; Welzl, E. (1987). "Partitioning and geometric embedding of range spaces of finite Vapnik-Chervonenkis dimension". Proceedings of the third annual symposium on Computational geometry â SCG '87. p.Â 331. doi:10.1145/41958.41994. ISBNÂ 978-0897912310. S2CIDÂ 7394360.

^ Jump up to: a b Shalev-Shwartz, Shai; Ben-David, Shai (2014). Understanding Machine Learning â from Theory to Algorithms. Cambridge University Press. ISBNÂ 9781107057135.

^ Natarajan 1989.

^ Ben-David, Cesa-Bianchi & Long 1992.

^ Pollard 1984.

^ Anthony & Bartlett 2009.

^ Morgenstern & Roughgarden 2015.

^ Karpinski & Macintyre 1997.


References[edit]
Moore, Andrew. "VC dimension tutorial".
Vapnik, Vladimir (2000). The nature of statistical learning theory. Springer.
Blumer, A.; Ehrenfeucht, A.; Haussler, D.; Warmuth, M. K. (1989). "Learnability and the VapnikâChervonenkis dimension" (PDF). Journal of the ACM. 36 (4): 929â865. doi:10.1145/76359.76371. S2CIDÂ 1138467.
Burges, Christopher. "Tutorial on SVMs for Pattern Recognition" (PDF). (containing information also for VC dimension)
Chazelle, Bernard. "The Discrepancy Method".
Natarajan, B.K. (1989). "On Learning sets and functions". Machine Learning. 4: 67â97. doi:10.1007/BF00114804.
Ben-David, Shai; Cesa-Bianchi, NicolÃ²; Long, Philip M. (1992). "Characterizations of learnability for classes of {O, â¦, n}-valued functions". Proceedings of the fifth annual workshop on Computational learning theory â COLT '92. p.Â 333. doi:10.1145/130385.130423. ISBNÂ 089791497X.
Pollard, D. (1984). Convergence of Stochastic Processes. Springer. ISBNÂ 9781461252542.
Anthony, Martin; Bartlett, Peter L. (2009). Neural Network Learning: Theoretical Foundations. ISBNÂ 9780521118620.
Morgenstern, Jamie H.; Roughgarden, Tim (2015). On the Pseudo-Dimension of Nearly Optimal Auctions. NIPS. arXiv:1506.03684. Bibcode:2015arXiv150603684M.
Karpinski, Marek; Macintyre, Angus (February 1997). "Polynomial Bounds for VC Dimension of Sigmoidal and General Pfaffian Neural Networks". Journal of Computer and System Sciences. 54 (1): 169â176. doi:10.1006/jcss.1997.1477.




<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=VapnikâChervonenkis_dimension&oldid=1057846532"
		Categories: DimensionStatistical classificationComputational learning theoryMeasures of complexityHidden categories: Articles with short descriptionShort description is different from WikidataAll articles with unsourced statementsArticles with unsourced statements from November 2021Articles needing additional references from May 2021All articles needing additional referencesArticles with unsourced statements from February 2020Commons category link is on Wikidata
	
