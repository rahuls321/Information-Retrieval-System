
Title:
Learning rate
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Tuning parameter (hyperparameter) in optimization
.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}Part of a series onMachine learningand data mining
Problems
Classification
Clustering
Regression
Anomaly detection
Data Cleaning
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction

Supervised learning.mw-parser-output .nobold{font-weight:normal}(classificationÂ â¢ regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)

Clustering
BIRCH
CURE
Hierarchical
k-means
Expectationâmaximization (EM)
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t-SNE

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
k-NN
Local outlier factor

Artificial neural network
Autoencoder
Cognitive computing
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
ESN
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net
Transformer
Vision
Spiking neural network
Memtransistor
Electrochemical RAM (ECRAM)

Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)

Theory
Kernel machines
Biasâvariance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory

Machine-learning venues
NeurIPS
ICML
ML
JMLR
ArXiv:cs.LG

Related articles
Glossary of artificial intelligence
List of datasets for machine-learning research
Outline of machine learning
.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vte
In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.[1] Since it influences to what extent newly acquired information overrides old information, it metaphorically represents the speed at which a machine learning model "learns". In the adaptive control literature, the learning rate is commonly referred to as gain.[2]
In setting a learning rate, there is a trade-off between the rate of convergence and overshooting. While the descent direction is usually determined from the gradient of the loss function, the learning rate determines how big a step is taken in that direction. A too high learning rate will make the learning jump over minima but a too low learning rate will either take too long to converge or get stuck in an undesirable local minimum.[3]
In order to achieve faster convergence, prevent oscillations and getting stuck in undesirable local minima the learning rate is often varied during training either in accordance to a learning rate schedule or by using an adaptive learning rate.[4] The learning rate and its adjustments may also differ per parameter, in which case it is a diagonal matrix that can be interpreted as an approximation to the inverse of the Hessian matrix in Newton's method.[5] The learning rate is related to the step length determined by inexact line search in quasi-Newton methods and related optimization algorithms.[6][7] 
When conducting line searches, mini-batch sub-sampling (MBSS) affect the characteristics of the loss function along which the learning rate needs to be resolved.[8] Static MBSS keeps the mini-batch fixed along a search direction, resulting in a smooth loss function along the search direction. Dynamic MBSS updates the mini-batch at every function evaluation, resulting in a point-wise discontinuous loss function along the search direction. Line searches that adaptively resolve learning rates for static MBSS loss functions include the parabolic approximation line (PAL) search.[9] Line searches that adaptively resolve learning rates for dynamic MBSS loss functions include probabilistic line searches,[10] gradient-only line searches (GOLS)[11] and quadratic approximations.[12]

Contents

1 Learning rate schedule
2 Adaptive learning rate
3 See also
4 References
5 Further reading
6 External links



Learning rate schedule[edit]
Initial rate can be left as system default or can be selected using a range of techniques.[13] A learning rate schedule changes the learning rate during learning and is most often changed between epochs/iterations. This is mainly done with two parameters: decay and momentum . There are many different learning rate schedules but the most common are time-based, step-based and exponential.[4]
Decay serves to settle the learning in a nice place and avoid oscillations, a situation that may arise when a too high constant learning rate makes the learning jump back and forth over a minimum, and is controlled by a hyperparameter.
Momentum is analogous to a ball rolling down a hill; we want the ball to settle at the lowest point of the hill (corresponding to the lowest error). Momentum both speeds up the learning (increasing the learning rate) when the error cost gradient is heading in the same direction for a long time and also avoids local minima by 'rolling over' small bumps. Momentum is controlled by a hyper parameter analogous to a ball's mass which must be chosen manuallyâtoo high and the ball will roll over minima which we wish to find, too low and it will not fulfil its purpose. The formula for factoring in the momentum is more complex than for decay but is most often built in with deep learning libraries such as Keras.
Time-based learning schedules alter the learning rate depending on the learning rate of the previous time iteration. Factoring in the decay the mathematical formula for the learning rate is:

  
    
      
        
          Î·
          
            n
            +
            1
          
        
        =
        
          
            
              Î·
              
                n
              
            
            
              1
              +
              d
              n
            
          
        
      
    
    {\displaystyle \eta _{n+1}={\frac {\eta _{n}}{1+dn}}}
  

where 
  
    
      
        Î·
      
    
    {\displaystyle \eta }
  
 is the learning rate, 
  
    
      
        d
      
    
    {\displaystyle d}
  
 is a decay parameter and 
  
    
      
        n
      
    
    {\displaystyle n}
  
 is the iteration step.
Step-based learning schedules changes the learning rate according to some pre defined steps. The decay application formula is here defined as:

  
    
      
        
          Î·
          
            n
          
        
        =
        
          Î·
          
            0
          
        
        
          d
          
            
              â
              
                
                  
                    1
                    +
                    n
                  
                  r
                
              
              â
            
          
        
      
    
    {\displaystyle \eta _{n}=\eta _{0}d^{\left\lfloor {\frac {1+n}{r}}\right\rfloor }}
  

where 
  
    
      
        
          Î·
          
            n
          
        
      
    
    {\displaystyle \eta _{n}}
  
 is the learning rate at iteration 
  
    
      
        n
      
    
    {\displaystyle n}
  
, 
  
    
      
        
          Î·
          
            0
          
        
      
    
    {\displaystyle \eta _{0}}
  
 is the initial learning rate, 
  
    
      
        d
      
    
    {\displaystyle d}
  
 is how much the learning rate should change at each drop (0.5 corresponds to a halving) and 
  
    
      
        r
      
    
    {\displaystyle r}
  
 corresponds to the droprate, or how often the rate should be dropped (10 corresponds to a drop every 10 iterations). The floor function (
  
    
      
        â
        â¦
        â
      
    
    {\displaystyle \lfloor \dots \rfloor }
  
) here drops the value of its input to 0 for all values smaller than 1.
Exponential learning schedules are similar to step-based but instead of steps a decreasing exponential function is used. The mathematical formula for factoring in the decay is:

  
    
      
        
          Î·
          
            n
          
        
        =
        
          Î·
          
            0
          
        
        
          e
          
            â
            d
            n
          
        
      
    
    {\displaystyle \eta _{n}=\eta _{0}e^{-dn}}
  

where 
  
    
      
        d
      
    
    {\displaystyle d}
  
 is a decay parameter.

Adaptive learning rate[edit]
The issue with learning rate schedules is that they all depend on hyperparameters that must be manually chosen for each given learning session and may vary greatly depending on the problem at hand or the model used. To combat this there are many different types of adaptive gradient descent algorithms such as Adagrad, Adadelta, RMSprop, and Adam[14] which are generally built into deep learning libraries such as Keras.[15]

See also[edit]
.mw-parser-output .div-col{margin-top:0.3em;column-width:30em}.mw-parser-output .div-col-small{font-size:90%}.mw-parser-output .div-col-rules{column-rule:1px solid #aaa}.mw-parser-output .div-col dl,.mw-parser-output .div-col ol,.mw-parser-output .div-col ul{margin-top:0}.mw-parser-output .div-col li,.mw-parser-output .div-col dd{page-break-inside:avoid;break-inside:avoid-column}
Hyperparameter (machine learning)
Hyperparameter optimization
Stochastic gradient descent
Variable metric methods
Overfitting
Backpropagation
AutoML
Model selection
Self-tuning

References[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Murphy, Kevin P. (2012). Machine Learning: A Probabilistic Perspective. Cambridge: MIT Press. p.Â 247. ISBNÂ 978-0-262-01802-9.

^ Delyon, Bernard (2000). "Stochastic Approximation with Decreasing Gain: Convergence and Asymptotic Theory". Unpublished Lecture Notes. UniversitÃ© de Rennes. CiteSeerXÂ 10.1.1.29.4428.

^ Buduma, Nikhil; Locascio, Nicholas (2017). Fundamentals of Deep LearningÂ : Designing Next-Generation Machine Intelligence Algorithms. O'Reilly. p.Â 21. ISBNÂ 978-1-4919-2558-4.

^ Jump up to: a b Patterson, Josh; Gibson, Adam (2017). "Understanding Learning Rates". Deep LearningÂ : A Practitioner's Approach. O'Reilly. pp.Â 258â263. ISBNÂ 978-1-4919-1425-0.

^ Ruder, Sebastian (2017). "An Overview of Gradient Descent Optimization Algorithms". arXiv:1609.04747 [cs.LG].

^ Nesterov, Y. (2004). Introductory Lectures on Convex Optimization: A Basic Course. Boston: Kluwer. p.Â 25. ISBNÂ 1-4020-7553-7.

^ Dixon, L. C. W. (1972). "The Choice of Step Length, a Crucial Factor in the Performance of Variable Metric Algorithms". Numerical Methods for Non-linear Optimization. London: Academic Press. pp.Â 149â170. ISBNÂ 0-12-455650-7.

^ Kafka, Dominic; Wilke, Daniel N. (2021). "An empirical study into finding optima in stochastic optimization of neural networks". Information Sciences. 560: 235â255. arXiv:1903.08552. doi:10.1016/j.ins.2021.01.005. S2CIDÂ 233313117.

^ Mutschler, Maximus; Zell, Andreas (2019). "Parabolic Approximation Line Search for DNNs". arXiv:1903.11991 [cs.LG].

^ Mahsereci, Maren; Hennig, Phillip (2016). "Probabilistic Line Searches for Stochastic Optimization". arXiv:1502.02846v4 [cs.LG].

^ Kafka, Dominic; Wilke, Daniel N. (2021). "Resolving learning rates adaptively by locating stochastic non-negative associated gradient projection points using line searches". Journal of Global Optimization. 79: 111â152. arXiv:2001.05113. doi:10.1007/s10898-020-00921-z. S2CIDÂ 210181099.

^ Chae, Younghwan; Wilke, Daniel N. (2019). "Empirical study towards understanding line search approximations for training neural networks". arXiv:1909.06893 [stat.ML].

^ Smith, Leslie N. (4 April 2017). "Cyclical Learning Rates for Training Neural Networks". arXiv:1506.01186 [cs.CV].

^ Murphy, Kevin (2021). Probabilistic Machine Learning: An Introduction. Probabilistic Machine Learning: An Introduction. MIT Press. Retrieved 10 April 2021.

^ Brownlee, Jason (22 January 2019). "How to Configure the Learning Rate When Training Deep Learning Neural Networks". Machine Learning Mastery. Retrieved 4 January 2021.


Further reading[edit]
GÃ©ron, AurÃ©lien (2017). "Gradient Descent". Hands-On Machine Learning with Scikit-Learn and TensorFlow. O'Reilly. pp.Â 113â124. ISBNÂ 978-1-4919-6229-9.
Plagianakos, V. P.; Magoulas, G. D.; Vrahatis, M. N. (2001). "Learning Rate Adaptation in Stochastic Gradient Descent". Advances in Convex Analysis and Global Optimization. Kluwer. pp.Â 433â444. ISBNÂ 0-7923-6942-4.
External links[edit]
de Freitas, Nando (February 12, 2015). "Optimization". Deep Learning Lecture 6. University of Oxford â via YouTube.




<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=Learning_rate&oldid=1068900578"
		Categories: Machine learningModel selectionOptimization algorithms and methodsHidden categories: Articles with short descriptionShort description matches Wikidata
	
