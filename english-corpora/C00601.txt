
Title:
String-searching algorithm
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Searches for patterns within strings
In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.
A basic example of string searching is when the pattern and the searched text are arrays of elements of an alphabet (finite set) Î£.  Î£ may be a human language alphabet, for example, the letters A through Z and other applications may use a binary alphabet (Î£ = {0,1}) or a DNA alphabet (Î£ = {A,C,G,T}) in bioinformatics.
In practice, the method of feasible string-search algorithm may be affected by the string encoding. In particular, if a variable-width encoding is in use, then it may be slower to find the Nth character, perhaps requiring time proportional to N. This may significantly slow some search algorithms. One of many possible solutions is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.[citation needed]

Contents

1 Overview
2 Examples of search algorithms

2.1 Naive string search
2.2 Finite-state-automaton-based search
2.3 Stubs
2.4 Index methods
2.5 Other variants


3 Classification of search algorithms

3.1 Classification by a number of patterns

3.1.1 Single-pattern algorithms
3.1.2 Algorithms using a finite set of patterns
3.1.3 Algorithms using an infinite number of patterns


3.2 Classification by the use of preprocessing programs
3.3 Classification by matching strategies


4 See also
5 References
6 External links



Overview[edit]
The most basic case of string searching involves one (often very long) string, sometimes called the haystack, and one (often very short) string, sometimes called the needle. The goal is to find one or more occurrences of the needle within the haystack.  For example, one might search for to within:

   Some books are to be tasted, others to be swallowed, and some few to be chewed and digested.

One might request the first occurrence of "to", which is the fourth word; or all occurrences, of which there are 3; or the last, which is the fifth word from the end.
Very commonly, however, various constraints are added. For example, one might want to match the "needle" only where it consists of one (or more) complete wordsâperhaps defined as not having other letters immediately adjacent on either side. In that case a search for "hew" or "low" should fail for the example sentence above, even though those literal strings do occur.
Another common example involves "normalization". For many purposes, a search for a phrase such as "to be" should succeed even in places where there is something else intervening between the "to" and the "be":

More than one space
Other "whitespace" characters such as tabs, non-breaking spaces, line-breaks, etc.
Less commonly, a hyphen or soft hyphen
In structured texts, tags or even arbitrarily large but "parenthetical" things such as footnotes, list-numbers or other markers, embedded images, and so on.
Many symbol systems include characters that are synonymous (at least for some purposes):

Latin-based alphabets distinguish lower-case from upper-case, but for many purposes string search is expected to ignore the distinction.
Many languages include ligatures, where one composite character is equivalent to two or more other characters.
Many writing systems involve diacritical marks such as accents or vowel points, which may vary in their usage, or be of varying importance in matching.
DNA sequences can involve non-coding segments which may be ignored for some purposes, or polymorphisms that lead to no change in the encoded proteins, which may not count as a true difference for some other purposes.
Some languages have rules where a different character or form of character must be used at the start, middle, or end of words.
Finally, for strings that represent natural language, aspects of the language itself become involved. For example, one might wish to find all occurrences of a "word" despite it having alternate spellings, prefixes or suffixes, etc.
Another more complex type of search is regular expression searching, where the user constructs a pattern of characters or other symbols, and any match to the pattern should fulfill the search. For example, to catch both the American English word "color" and the British equivalent "colour", instead of searching for two different literal strings, one might use a regular expression such as:

   colou?r

where the "?" conventionally makes the preceding character ("u") optional.
This article mainly discusses algorithms for the simpler kinds of string searching.
A similar problem introduced in the field of bioinformatics and genomics is the maximal exact matching (MEM).[1] Given two strings, MEMs are common substrings that cannot be extended left or right without causing a mismatch.[2]

Examples of search algorithms[edit]
Naive string search[edit]
A simple and inefficient way to see where one string occurs inside another is to check at each index, one by one. First, we see if there's a copy of the needle starting at the first character of the haystack; if not, we look to see if there's a copy of the needle starting at the second character of the haystack, and so forth. In the normal case, we only have to look at one or two characters for each wrong position to see that it is a wrong position, so in the average case, this takes O(n + m) steps, where n is the length of the haystack and m is the length of the needle; but in the worst case, searching for a string like "aaaab" in a string like "aaaaaaaaab", it takes O(nm)

Finite-state-automaton-based search[edit]

In this approach, backtracking is avoided by constructing a deterministic finite automaton (DFA) that recognizes stored search string. These are expensive to constructâthey are usually created using the powerset constructionâbut are very quick to use. For example, the DFA shown to the right recognizes the word "MOMMY". This approach is frequently generalized in practice to search for arbitrary regular expressions.

Stubs[edit]
KnuthâMorrisâPratt computes a DFA that recognizes inputs with the string to search for as a suffix, BoyerâMoore starts searching from the end of the needle, so it can usually jump ahead a whole needle-length at each step. BaezaâYates keeps track of whether the previous j characters were a prefix of the search string, and is therefore adaptable to fuzzy string searching. The bitap algorithm is an application of BaezaâYates' approach.

Index methods[edit]
Faster search algorithms preprocess the text. After building a substring index, for example a suffix tree or suffix array, the occurrences of a pattern can be found quickly. As an example, a suffix tree can be built in 
  
    
      
        Î
        (
        n
        )
      
    
    {\displaystyle \Theta (n)}
  
 time, and all 
  
    
      
        z
      
    
    {\displaystyle z}
  
 occurrences of a pattern can be found in 
  
    
      
        O
        (
        m
        )
      
    
    {\displaystyle O(m)}
  
 time under the assumption that the alphabet has a constant size and all inner nodes in the suffix tree know what leaves are underneath them. The latter can be accomplished by running a DFS algorithm from the root of the suffix tree.

Other variants[edit]
Some search methods, for instance trigram search, are intended to find a "closeness" score between the search string and the text rather than a "match/non-match". These are sometimes called "fuzzy" searches.

Classification of search algorithms[edit]
Classification by a number of patterns[edit]
The various algorithms can be classified by the number of patterns each uses. 

Single-pattern algorithms[edit]
In the following compilation, m is the length of the pattern, n the length of the searchable text, k = |Î£| is the size of the alphabet, and f is a constant introduced by SIMD operations.




Algorithm

Preprocessing time

Matching time[1]

Space


NaÃ¯ve string-search algorithm

none

Î(mn)

none


Optimized NaÃ¯ve string-search algorithm (libc++ and libstdc++ string::find)[3][4]

none

Î(mn/f)

none


RabinâKarp algorithm

Î(m)

average Î(n + m),worst Î((nâm)m)

O(1)


KnuthâMorrisâPratt algorithm

Î(m)

Î(n)

Î(m)


BoyerâMoore string-search algorithm

Î(m + k)

best Î©(n/m),worst O(mn)

Î(k)


Bitap algorithm (shift-or, shift-and, BaezaâYatesâGonnet; fuzzy; agrep)

Î(m + k)

O(mn)




Two-way string-matching algorithm (glibc memmem/strstr)[5]

Î(m)

O(n+m)

O(1)


BNDM (Backward Non-Deterministic DAWG Matching) (fuzzy + regex; nrgrep)[6]

O(m)

O(n)




BOM (Backward Oracle Matching)[7]

O(m)

O(mn)




FM-index

O(n)

O(m)

O(n)

1..mw-parser-output .citation{word-wrap:break-word}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}^ Asymptotic times are expressed using O, Î©, and Î notation.
The BoyerâMoore string-search algorithm has been the standard benchmark for the practical string-search literature.[8]

Algorithms using a finite set of patterns[edit]
AhoâCorasick string matching algorithm (extension of Knuth-Morris-Pratt)
Commentz-Walter algorithm (extension of Boyer-Moore)
Set-BOM (extension of Backward Oracle Matching)
RabinâKarp string search algorithm
Algorithms using an infinite number of patterns[edit]
Naturally, the patterns can not be enumerated finitely in this case. They are represented usually by a regular grammar or regular expression.

Classification by the use of preprocessing programs[edit]
Other classification approaches are possible. One of the most common uses preprocessing as main criteria.


Classes of string searching algorithms[9]




Text not preprocessed

Text preprocessed


Patterns not preprocessed

Elementary algorithms

Index methods


Patterns preprocessed

Constructed search engines

Signature methodsÂ :[10]

Classification by matching strategies[edit]
Another one classifies the algorithms by their matching strategy:[11]

Match the prefix first (KnuthâMorrisâPratt, Shift-And, AhoâCorasick)
Match the suffix first (BoyerâMoore and variants, Commentz-Walter)
Match the best factor first (BNDM, BOM, Set-BOM)
Other strategy (NaÃ¯ve, RabinâKarp)
See also[edit]
Sequence alignment
Graph matching
Pattern matching
Compressed pattern matching
Matching wildcards
Full-text search
References[edit]

^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Kurtz, Stefan; Phillippy, Adam; Delcher, Arthur L; Smoot, Michael; Shumway, Martin; Antonescu, Corina; Salzberg, Steven L (2004). "Versatile and open software for comparing large genomes". Genome Biology. 5 (2): R12. doi:10.1186/gb-2004-5-2-r12. ISSNÂ 1465-6906. PMCÂ 395750. PMIDÂ 14759262.

^ Khan, Zia; Bloom, Joshua S.; Kruglyak, Leonid; Singh, Mona (2009-07-01). "A practical algorithm for finding maximal exact matches in large sequence datasets using sparse suffix arrays". Bioinformatics. 25 (13): 1609â1616. doi:10.1093/bioinformatics/btp275. PMCÂ 2732316. PMIDÂ 19389736.

^ Kumar, Aditya. "libc++: Improve string::find algorithm". {{cite journal}}: Cite journal requires |journal= (help)

^ Kumar, Aditya. "libstdc++: Improve string::find algorithm". {{cite journal}}: Cite journal requires |journal= (help)

^ Crochemore, Maxime; Perrin, Dominique (1 July 1991). "Two-way string-matching" (PDF). Journal of the ACM. 38 (3): 650â674. doi:10.1145/116825.116845. S2CIDÂ 15055316.

^ Navarro, Gonzalo; Raffinot, Mathieu (1998). "A bit-parallel approach to suffix automata: Fast extended string matching" (PDF). Combinatorial Pattern Matching. Lecture Notes in Computer Science. Springer Berlin Heidelberg. 1448: 14â33. doi:10.1007/bfb0030778. ISBNÂ 978-3-540-64739-3.

^ Fan, H.; Yao, N.; Ma, H. (December 2009). "Fast Variants of the Backward-Oracle-Marching Algorithm" (PDF). 2009 Fourth International Conference on Internet Computing for Science and Engineering: 56â59. doi:10.1109/ICICSE.2009.53. ISBNÂ 978-1-4244-6754-9. S2CIDÂ 6073627.

^ Hume; Sunday (1991). "Fast String Searching". Software: Practice and Experience. 21 (11): 1221â1248. doi:10.1002/spe.4380211105. S2CIDÂ 5902579.

^ Melichar, Borivoj, Jan Holub, and J. Polcar. Text Searching Algorithms. Volume I: Forward String Matching. Vol. 1. 2 vols., 2005. http://stringology.org/athens/TextSearchingAlgorithms/.

^ Riad Mokadem; Witold Litwin http://www.cse.scu.edu/~tschwarz/Papers/vldb07_final.pdf (2007), Fast nGramBased String Search Over Data Encoded Using Algebraic Signatures, 33rd International Conference on Very Large Data Bases (VLDB) {{citation}}: External link in |surname2= (help)

^ Gonzalo Navarro; Mathieu Raffinot (2008), Flexible Pattern Matching Strings: Practical On-Line Search Algorithms for Texts and Biological Sequences, ISBNÂ 978-0-521-03993-2


R. S. Boyer and J. S. Moore, A fast string searching algorithm, Carom. ACM 20, (10), 262â272(1977).
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Third Edition. MIT Press and McGraw-Hill, 2009. ISBNÂ 0-262-03293-7. Chapter 32: String Matching, pp.Â 985â1013.
External links[edit]



Wikimedia Commons has media related to String-searching algorithm.

Huge list of pattern matching links  Last updated: 12/27/2008 20:18:38
Large (maintained) list of string-matching algorithms
NIST list of string-matching algorithms
StringSearch â high-performance pattern matching algorithms in Java â Implementations of many String-Matching-Algorithms in Java (BNDM, Boyer-Moore-Horspool, Boyer-Moore-Horspool-Raita, Shift-Or)
StringsAndChars â Implementations of many String-Matching-Algorithms (for single and multiple patterns) in Java
Exact String Matching Algorithms â Animation in Java, Detailed description and C implementation of many algorithms.
(PDF) Improved Single and Multiple Approximate String Matching
Kalign2: high-performance multiple alignment of protein and nucleotide sequences allowing external features
NyoTengu â high-performance pattern matching algorithm in C â Implementations of Vector and Scalar String-Matching-Algorithms in C
.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vteStringsString metric
Approximate string matching
Bitap algorithm
DamerauâLevenshtein distance
Edit distance
Gestalt Pattern Matching
Hamming distance
JaroâWinkler distance
Lee distance
Levenshtein automaton
Levenshtein distance
WagnerâFischer algorithm 
String-searching algorithm
ApostolicoâGiancarlo algorithm
BoyerâMoore string-search algorithm
BoyerâMooreâHorspool algorithm
KnuthâMorrisâPratt algorithm
RabinâKarp algorithm
Multiple string searching
AhoâCorasick
Commentz-Walter algorithm
Regular expression
Comparison of regular-expression engines
Regular grammar
Thompson's construction
Nondeterministic finite automaton
Sequence alignment
Hirschberg's algorithm
NeedlemanâWunsch algorithm
SmithâWaterman algorithm
Data structure
DAFSA
Suffix array
Suffix automaton
Suffix tree
Generalized suffix tree
Rope
Ternary search tree
Trie
Other
Parsing
Pattern matching
Compressed pattern matching
Longest common subsequence
Longest common substring
Sequential pattern mining
Sorting





<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=String-searching_algorithm&oldid=1069591822"
		Categories: String matching algorithmsHidden categories: CS1 errors: missing periodicalCS1 errors: external linksArticles with short descriptionShort description is different from WikidataAll articles with unsourced statementsArticles with unsourced statements from August 2017Commons category link from Wikidata
	
