
Title:
A* search algorithm
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Algorithm used for pathfinding and graph traversal
.mw-parser-output .infobox-subbox{padding:0;border:none;margin:-3px;width:auto;min-width:100%;font-size:100%;clear:none;float:none;background-color:transparent}.mw-parser-output .infobox-3cols-child{margin:auto}.mw-parser-output .infobox .navbar{font-size:100%}body.skin-minerva .mw-parser-output .infobox-header,body.skin-minerva .mw-parser-output .infobox-subheader,body.skin-minerva .mw-parser-output .infobox-above,body.skin-minerva .mw-parser-output .infobox-title,body.skin-minerva .mw-parser-output .infobox-image,body.skin-minerva .mw-parser-output .infobox-full-data,body.skin-minerva .mw-parser-output .infobox-below{text-align:center}ClassSearch algorithmData structureGraphWorst-case performance
  
    
      
        O
        (
        
          |
        
        E
        
          |
        
        )
        =
        O
        (
        
          b
          
            d
          
        
        )
      
    
    {\displaystyle O(|E|)=O(b^{d})}
  
Worst-case space complexity
  
    
      
        O
        (
        
          |
        
        V
        
          |
        
        )
        =
        O
        (
        
          b
          
            d
          
        
        )
      
    
    {\displaystyle O(|V|)=O(b^{d})}
  

.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}Graph and treesearch algorithms
Î±âÎ²
A*
B*
Backtracking
Beam
BellmanâFord
Best-first
Bidirectional
BorÅ¯vka
Branch & bound
BFS
British Museum
D*
DFS
Dijkstra
Edmonds
FloydâWarshall
Fringe search
Hill climbing
IDA*
Iterative deepening
Johnson
Jump point
Kruskal
Lexicographic BFS
LPA*
Prim
SMA*

Listings
Graph algorithms
Search algorithms
List of graph algorithms

Related topics
Dynamic programming
Graph traversal
Tree traversal
Search games
Graph coloring
.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vte
A* (pronounced "A-star") is a graph traversal and path search algorithm, which is often used in many fields of computer science due to its completeness, optimality, and optimal efficiency.[1] One major practical drawback is its 
  
    
      
        O
        (
        
          b
          
            d
          
        
        )
      
    
    {\displaystyle O(b^{d})}
  
 space complexity, as it stores all generated nodes in memory. Thus, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance,[2] as well as memory-bounded approaches; however, A* is still the best solution in many cases.[3]
Peter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute (now SRI International) first published the algorithm in 1968.[4] It can be seen as an extension of Dijkstra's algorithm. A* achieves better performance by using heuristics to guide its search.

Contents

1 History
2 Description

2.1 Pseudocode
2.2 Example
2.3 Implementation details
2.4 Special cases


3 Properties

3.1 Termination and Completeness
3.2 Admissibility
3.3 Optimality and Consistency


4 Bounded relaxation
5 Complexity
6 Applications
7 Relations to other algorithms
8 Variants
9 See also
10 Notes
11 References
12 Further reading
13 External links



History[edit]
  A* was invented by researchers working on Shakey the Robot's path planning.
A* was created as part of the Shakey project, which had the aim of building a mobile robot that could plan its own actions. Nils Nilsson originally proposed using the Graph Traverser algorithm[5] for Shakey's path planning.[6] Graph Traverser is guided by a heuristic function h(n), the estimated distance from node n to the goal node: it entirely ignores g(n), the distance from the start node to n. Bertram Raphael suggested using the sum, g(n) + h(n).[6] Peter Hart invented the concepts we now call admissibility and consistency of heuristic functions. A* was originally designed for finding least-cost paths when the cost of a path is the sum of its  costs, but it has been shown that A* can be used to find optimal paths for any problem satisfying the conditions of a cost algebra.[7]
The original 1968 A* paper[4] contained a theorem stating that no A*-like algorithm[a] could expand fewer nodes than A* if the heuristic function is consistent and A*'s tie-breaking rule is suitably chosen. A â³correctionâ³ was published a few years later[8] claiming that consistency was not required, but this was shown to be false in Dechter and Pearl's definitive study of A*'s optimality (now called optimal efficiency), which gave an example of A* with a heuristic that was admissible but not consistent expanding arbitrarily more nodes than an alternative A*-like algorithm.[9]

Description[edit]
A* is an informed search algorithm, or a best-first search, meaning that it is formulated in terms of weighted graphs: starting from a specific starting node of a graph, it aims to find a path to the given goal node having the smallest cost (least distance travelled, shortest time, etc.).  It does this by maintaining a tree of paths originating at the start node and extending those paths one edge at a time until its termination criterion is satisfied.
At each iteration of its main loop, A* needs to determine which of its paths to extend. It does so based on the cost of the path and an estimate of the cost required to extend the path all the way to the goal. Specifically, A* selects the path that minimizes


  
    
      
        f
        (
        n
        )
        =
        g
        (
        n
        )
        +
        h
        (
        n
        )
      
    
    {\displaystyle f(n)=g(n)+h(n)}
  

where n is the next node on the path, g(n) is the cost of the path from the start node to n, and h(n) is a heuristic function that estimates the cost of the cheapest path from n to the goal. A* terminates when the path it chooses to extend is a path from start to goal or if there are no paths eligible to be extended. The heuristic function is problem-specific. If the heuristic function is admissible, meaning that it never overestimates the actual cost to get to the goal, A* is guaranteed to return a least-cost path from start to goal.
Typical implementations of A* use a priority queue to perform the repeated selection of minimum (estimated) cost nodes to expand. This priority queue is known as the open set or fringe. At each step of the algorithm, the node with the lowest f(x) value is removed from the queue, the f and g values of its neighbors are updated accordingly, and these neighbors are added to the queue. The algorithm continues until a removed node (thus the node with the lowest f value out of all fringe nodes) is a goal node.[b] The f value of that goal is then also the cost of the shortest path, since h at the goal is zero in an admissible heuristic.
The algorithm described so far gives us only the length of the shortest path. To find the actual sequence of steps, the algorithm can be easily revised so that each node on the path keeps track of its predecessor. After this algorithm is run, the ending node will point to its predecessor, and so on, until some node's predecessor is the start node.
As an example, when searching for the shortest route on a map, h(x) might represent the straight-line distance to the goal, since that is physically the smallest possible distance between any two points. For a grid map from a video game, using the Manhattan distance or the octile distance becomes better depending on the set of movements available (4-way or 8-way).

  A* pathfinding algorithm navigating around a randomly-generated maze
Play media  Illustration of A* search for finding path between two point on a graph.
If the heuristic h satisfies the additional condition h(x) â¤ d(x, y) + h(y) for every edge (x, y) of the graph (where d denotes the length of that edge), then h is called monotone, or consistent. With a consistent heuristic, A* is guaranteed to find an optimal path without processing any node more than once and A* is equivalent to running Dijkstra's algorithm with the reduced cost d'(x, y) = d(x, y) + h(y) â h(x).

Pseudocode[edit]
The following pseudocode describes the algorithm:

function reconstruct_path(cameFrom, current)
    total_path := {current}
    while current in cameFrom.Keys:
        current := cameFrom[current]
        total_path.prepend(current)
    return total_path

// A* finds a path from start to goal.
// h is the heuristic function. h(n) estimates the cost to reach goal from node n.
function A_Star(start, goal, h)
    // The set of discovered nodes that may need to be (re-)expanded.
    // Initially, only the start node is known.
    // This is usually implemented as a min-heap or priority queue rather than a hash-set.
    openSet := {start}

    // For node n, cameFrom[n] is the node immediately preceding it on the cheapest path from start
    // to n currently known.
    cameFrom := an empty map

    // For node n, gScore[n] is the cost of the cheapest path from start to n currently known.
    gScore := map with default value of Infinity
    gScore[start] := 0

    // For node n, fScore[n]Â := gScore[n] + h(n). fScore[n] represents our current best guess as to
    // how short a path from start to finish can be if it goes through n.
    fScore := map with default value of Infinity
    fScore[start] := h(start)

    while openSet is not empty
        // This operation can occur in O(1) time if openSet is a min-heap or a priority queue
        current := the node in openSet having the lowest fScore[] value
        if current = goal
            return reconstruct_path(cameFrom, current)

        openSet.Remove(current)
        for each neighbor of current
            // d(current,neighbor) is the weight of the edge from current to neighbor
            // tentative_gScore is the distance from start to the neighbor through current
            tentative_gScore := gScore[current] + d(current, neighbor)
            if tentative_gScore < gScore[neighbor]
                // This path to neighbor is better than any previous one. Record it!
                cameFrom[neighbor] := current
                gScore[neighbor] := tentative_gScore
                fScore[neighbor] := tentative_gScore + h(neighbor)
                if neighbor not in openSet
                    openSet.add(neighbor)

    // Open set is empty but goal was never reached
    return failure

Remark: In this pseudocode, if a node is reached by one path, removed from openSet, and subsequently reached by a cheaper path, it will be added to openSet again. This is essential to guarantee that the path returned is optimal if the heuristic function is admissible but not  consistent.   If the heuristic is consistent, when a node is removed from openSet the path to it is guaranteed to be optimal so the test âtentative_gScore < gScore[neighbor]â will always fail if the node is reached again.

  Illustration of A* search for finding path from a start node to a goal node in a robot motion planning problem. The empty circles represent the nodes in the open set, i.e., those that remain to be explored, and the filled ones are in the closed set. Color on each closed node indicates the distance from the goal: the greener, the closer. One can first see the A* moving in a straight line in the direction of the goal, then when hitting the obstacle, it explores alternative routes through the nodes from the open set. .mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}See also: Dijkstra's algorithm


Example[edit]
An example of an A* algorithm in action where nodes are cities connected with roads and h(x) is the     straight-line distance to target point:

Key: green: start; blue: goal; orange: visited
The A* algorithm also has real-world applications. In this example, edges are railroads and h(x) is the great-circle distance (the shortest possible distance on a sphere) to the target. The algorithm is searching for a path between Washington, D.C. and Los Angeles.


Implementation details[edit]
There are a number of simple optimizations or implementation details that can significantly affect the performance of an A* implementation.  The first detail to note is that the way the priority queue handles ties can have a significant effect on performance in some situations.  If ties are broken so the queue behaves in a LIFO manner, A* will behave like depth-first search among equal cost paths (avoiding exploring more than one equally optimal solution).
When a path is required at the end of the search, it is common to keep with each node a reference to that node's parent.  At the end of the search these references can be used to recover the optimal path.  If these references are being kept then it can be important that the same node doesn't appear in the priority queue more than once (each entry corresponding to a different path to the node, and each with a different cost).  A standard approach here is to check if a node about to be added already appears in the priority queue.  If it does, then the priority and parent pointers are changed to correspond to the lower cost path. A standard binary heap based priority queue does not directly support the operation of searching for one of its elements, but it can be augmented with a hash table that maps elements to their position in the heap, allowing this decrease-priority operation to be performed in logarithmic time. Alternatively, a Fibonacci heap can perform the same decrease-priority operations in constant amortized time.

Special cases[edit]
Dijkstra's algorithm, as another example of a uniform-cost search algorithm, can be viewed as a special case of A* where 
  
    
      
        h
        (
        x
        )
        =
        0
      
    
    {\displaystyle h(x)=0}
  
 for all x.[10][11] General depth-first search can be implemented using A* by considering that there is a global counter C initialized with a very large value. Every time we process a node we assign C to all of its newly discovered neighbors. After each single assignment, we decrease the counter C by one. Thus the earlier a node is discovered, the higher its 
  
    
      
        h
        (
        x
        )
      
    
    {\displaystyle h(x)}
  
 value. Both Dijkstra's algorithm and depth-first search can be implemented more efficiently without including an 
  
    
      
        h
        (
        x
        )
      
    
    {\displaystyle h(x)}
  
 value at each node.

Properties[edit]
Termination and Completeness[edit]
On finite graphs with non-negative edge weights A* is guaranteed to terminate and is complete, i.e. it will always find a solution (a path from start to goal) if one exists. On infinite graphs with a finite branching factor and edge costs that are bounded away from zero (
  
    
      
        d
        (
        x
        ,
        y
        )
        >
        Îµ
        >
        0
      
    
    {\textstyle d(x,y)>\varepsilon >0}
  
 for some fixed 
  
    
      
        Îµ
      
    
    {\displaystyle \varepsilon }
  
), A* is guaranteed to terminate only if there exists a solution.[1]

Admissibility[edit]
A search algorithm is said to be admissible if it is guaranteed to return an optimal solution. If the heuristic function used by A* is admissible, then A* is admissible. An intuitive â³proofâ³ of this  is as follows:
When A* terminates its search, it has found a path from start to goal whose actual cost is lower than the estimated cost of any path from start to goal through any open node (the node's 
  
    
      
        f
      
    
    {\displaystyle f}
  
 value). When the heuristic is admissible, those estimates are optimistic (not quiteâsee the next paragraph), so A* can safely ignore those nodes because they cannot possibly lead to a cheaper solution than the one it already has. In other words, A* will never overlook the possibility of a lower-cost path from start to goal and so it will continue to search until no such possibilities exist.
The actual proof is a bit more involved because the 
  
    
      
        f
      
    
    {\displaystyle f}
  
 values of open nodes are not guaranteed to be optimistic even if the heuristic is admissible. This is because the 
  
    
      
        g
      
    
    {\displaystyle g}
  
 values of open nodes are not guaranteed to be optimal, so the sum 
  
    
      
        g
        +
        h
      
    
    {\displaystyle g+h}
  
 is not guaranteed to be optimistic.

Optimality and Consistency[edit]
Algorithm A is optimally efficient with respect to a set of alternative algorithms Alts on a set of problems P if for every problem P in P and every algorithm Aâ² in Alts, the set of nodes expanded by A in solving P is a subset (possibly equal) of the set of nodes expanded by Aâ² in solving P. The definitive study of the optimal efficiency of A* is due to Rina Dechter and Judea Pearl.[9]
They considered a variety of  definitions of Alts and P  in combination with A*'s heuristic being merely admissible or being both consistent and admissible.  The most interesting positive result they proved is that A*, with a consistent heuristic, is optimally efficient with respect to all admissible A*-like search algorithms on all â³non-pathologicalâ³ search problems.  Roughly speaking, their notion of non-pathological problem is what we now mean by â³up to tie-breakingâ³.  This result does not hold if A*'s heuristic is admissible but not consistent. In that case, Dechter and Pearl showed there exist admissible A*-like algorithms that can expand arbitrarily fewer nodes than A* on some non-pathological problems.
Optimal efficiency is about the set of nodes expanded, not the number of node expansions (the number of iterations of A*'s main loop).  When the heuristic being used is admissible but not consistent, it is possible for a node to be expanded by A* many times, an exponential number of times in the worst case.[12]
In such circumstances Dijkstra's algorithm could outperform A* by a large margin. However, more recent research found that this pathological case only occurs in certain contrived situations where the edge weight of the search graph is exponential in the size of the graph, and that certain inconsistent (but admissible) heuristics can lead to a reduced number of node expansions in A* searches.[13][14]

Bounded relaxation[edit]
  A* search that uses a heuristic that is 5.0(=Îµ) times a consistent heuristic, and obtains a suboptimal path.
While the admissibility criterion guarantees an optimal solution path, it also means that A* must examine all equally meritorious paths to find the optimal path. To compute approximate shortest paths, it is possible to speed up the search at the expense of optimality by relaxing the admissibility criterion. Oftentimes we want to bound this relaxation, so that we can guarantee that the solution path is no worse than (1 + Îµ) times the optimal solution path. This new guarantee is referred to as Îµ-admissible.
There are a number of Îµ-admissible algorithms:

Weighted A*/Static Weighting's.[15] If ha(n) is an admissible heuristic function, in the weighted version of the A* search one uses hw(n) = Îµ ha(n), Îµ > 1 as the heuristic function, and perform the A* search as usual (which eventually happens faster than using ha since fewer nodes are expanded). The path hence found by the search algorithm can have a cost of at most Îµ times that of the least cost path in the graph.[16]
Dynamic Weighting[17] uses the cost function 
  
    
      
        f
        (
        n
        )
        =
        g
        (
        n
        )
        +
        (
        1
        +
        Îµ
        w
        (
        n
        )
        )
        h
        (
        n
        )
      
    
    {\displaystyle f(n)=g(n)+(1+\varepsilon w(n))h(n)}
  
, where 
  
    
      
        w
        (
        n
        )
        =
        
          
            {
            
              
                
                  1
                  â
                  
                    
                      
                        d
                        (
                        n
                        )
                      
                      N
                    
                  
                
                
                  d
                  (
                  n
                  )
                  â¤
                  N
                
              
              
                
                  0
                
                
                  
                    otherwise
                  
                
              
            
            
          
        
      
    
    {\displaystyle w(n)={\begin{cases}1-{\frac {d(n)}{N}}&d(n)\leq N\\0&{\text{otherwise}}\end{cases}}}
  
, and where 
  
    
      
        d
        (
        n
        )
      
    
    {\displaystyle d(n)}
  
 is the depth of the search and N is the anticipated length of the solution path.
Sampled Dynamic Weighting[18] uses sampling of nodes to better estimate and debias the heuristic error.

  
    
      
        
          A
          
            Îµ
          
          
            â
          
        
      
    
    {\displaystyle A_{\varepsilon }^{*}}
  
.[19] uses two heuristic functions. The first is the FOCAL list, which is used to select candidate nodes, and the second hF is used to select the most promising node from the FOCAL list.
AÎµ[20] selects nodes with the function 
  
    
      
        A
        f
        (
        n
        )
        +
        B
        
          h
          
            F
          
        
        (
        n
        )
      
    
    {\displaystyle Af(n)+Bh_{F}(n)}
  
, where A and B are constants. If no nodes can be selected, the algorithm will backtrack with the function 
  
    
      
        C
        f
        (
        n
        )
        +
        D
        
          h
          
            F
          
        
        (
        n
        )
      
    
    {\displaystyle Cf(n)+Dh_{F}(n)}
  
, where C and D are constants.
AlphA*[21] attempts to promote depth-first exploitation by preferring recently expanded nodes. AlphA* uses the cost function 
  
    
      
        
          f
          
            Î±
          
        
        (
        n
        )
        =
        (
        1
        +
        
          w
          
            Î±
          
        
        (
        n
        )
        )
        f
        (
        n
        )
      
    
    {\displaystyle f_{\alpha }(n)=(1+w_{\alpha }(n))f(n)}
  
, where 
  
    
      
        
          w
          
            Î±
          
        
        (
        n
        )
        =
        
          
            {
            
              
                
                  Î»
                
                
                  g
                  (
                  Ï
                  (
                  n
                  )
                  )
                  â¤
                  g
                  (
                  
                    
                      
                        n
                        ~
                      
                    
                  
                  )
                
              
              
                
                  Î
                
                
                  
                    otherwise
                  
                
              
            
            
          
        
      
    
    {\displaystyle w_{\alpha }(n)={\begin{cases}\lambda &g(\pi (n))\leq g({\tilde {n}})\\\Lambda &{\text{otherwise}}\end{cases}}}
  
, where Î» and Î are constants with 
  
    
      
        Î»
        â¤
        Î
      
    
    {\displaystyle \lambda \leq \Lambda }
  
, Ï(n) is the parent of n, and Ã± is the most recently expanded node.
Complexity[edit]
The time complexity of A* depends on the heuristic. In the worst case of an unbounded search space, the number of nodes expanded is exponential in the depth of the solution (the shortest path) d: O(bd), where b is the branching factor (the average number of successors per state).[22] This assumes that a goal state exists at all, and is reachable from the start state; if it is not, and the state space is infinite, the algorithm will not terminate.
The heuristic function has a major effect on the practical performance of A* search, since a good heuristic allows A* to prune away many of the bd nodes that an uninformed search would expand. Its quality can be expressed in terms of the effective branching factor b*, which can be determined empirically for a problem instance by measuring the number of nodes generated by expansion, N, and the depth of the solution, then solving[23]


  
    
      
        N
        +
        1
        =
        1
        +
        
          b
          
            â
          
        
        +
        (
        
          b
          
            â
          
        
        
          )
          
            2
          
        
        +
        â¯
        +
        (
        
          b
          
            â
          
        
        
          )
          
            d
          
        
        .
      
    
    {\displaystyle N+1=1+b^{*}+(b^{*})^{2}+\dots +(b^{*})^{d}.}
  

Good heuristics are those with low effective branching factor (the optimal being b* = 1).
The time complexity is polynomial when the search space is a tree, there is a single goal state, and the heuristic function h meets the following condition:


  
    
      
        
          |
        
        h
        (
        x
        )
        â
        
          h
          
            â
          
        
        (
        x
        )
        
          |
        
        =
        O
        (
        log
        â¡
        
          h
          
            â
          
        
        (
        x
        )
        )
      
    
    {\displaystyle |h(x)-h^{*}(x)|=O(\log h^{*}(x))}
  

where h* is the optimal heuristic, the exact cost to get from x to the goal. In other words, the error of h will not grow faster than the logarithm of the "perfect heuristic" h* that returns the true distance from x to the goal.[16][22]
The space complexity of A* is roughly the same as that of all other graph search algorithms, as it keeps all generated nodes in memory.[1] In practice, this turns out to be the biggest drawback of A* search, leading to the development of memory-bounded heuristic searches, such as Iterative deepening A*, memory bounded A*, and SMA*.

Applications[edit]
A* is often used for the common pathfinding problem in applications such as video games, but was originally designed as a general graph traversal algorithm.[4]
It finds applications in diverse problems, including the problem of parsing using stochastic grammars in NLP.[24]
Other cases include an Informational search with online learning.[25]

Relations to other algorithms[edit]
What sets A* apart from a greedy best-first search algorithm is that it takes the cost/distance already traveled, g(n), into account.
Some common variants of Dijkstra's algorithm can be viewed as a special case of A* where the heuristic 
  
    
      
        h
        (
        n
        )
        =
        0
      
    
    {\displaystyle h(n)=0}
  
 for all nodes;[10][11] in turn, both Dijkstra and A* are special cases of dynamic programming.[26]
A* itself is a special case of a generalization of branch and bound.[27]

Variants[edit]
Anytime A*[28]
Block A*
D*
Field D*
Fringe
Fringe Saving A* (FSA*)
Generalized Adaptive A* (GAA*)
Incremental heuristic search
Reduced A*[29]
Iterative deepening A* (IDA*)
Jump point search
Lifelong Planning A* (LPA*)
New Bidirectional A* (NBA*)[30]
Simplified Memory bounded A* (SMA*)
Theta*
A* can also be adapted to a bidirectional search algorithm. Special care needs to be taken for the stopping criterion.[31]

See also[edit]
Breadth-first search
Depth-first search
Any-angle path planning, search for paths that are not limited to move along graph edges but rather can take on any angle
Notes[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ âA*-likeâ means the algorithm searches by extending paths originating at the start node one edge at a time, just as A* does. This excludes, for example, algorithms that search backward from the goal or in both directions simultaneously. In addition, the algorithms covered by this theorem must be admissible and ânot more informedâ than A*.

^ Goal nodes may be passed over multiple times if there remain other nodes with lower f values, as they may lead to a shorter path to a goal.


References[edit]


^ Jump up to: a b c .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Russell, Stuart J. (2018). Artificial intelligence a modern approach. Norvig, Peter (4thÂ ed.). Boston: Pearson. ISBNÂ 978-0134610993. OCLCÂ 1021874142.

^ Delling, D.; Sanders, P.; Schultes, D.; Wagner, D. (2009). "Engineering Route Planning Algorithms". Algorithmics of Large and Complex Networks: Design, Analysis, and Simulation. Lecture Notes in Computer Science. Vol.Â 5515. Springer. pp.Â 11ä¸ª$7â139. CiteSeerXÂ 10.1.1.164.8916. doi:10.1007/978-3-642-02094-0_7. ISBNÂ 978-3-642-02093-3.

^ Zeng, W.; Church, R. L. (2009). "Finding shortest paths on real road networks: the case for A*". International Journal of Geographical Information Science. 23 (4): 531â543. doi:10.1080/13658810801949850. S2CIDÂ 14833639.

^ Jump up to: a b c Hart, P. E.; Nilsson, N. J.; Raphael, B. (1968). "A Formal Basis for the Heuristic Determination of Minimum Cost Paths". IEEE Transactions on Systems Science and Cybernetics. 4 (2): 100â107. doi:10.1109/TSSC.1968.300136.

^ Doran, J. E.; Michie, D. (1966-09-20). "Experiments with the Graph Traverser program". Proc. R. Soc. Lond. A. 294 (1437): 235â259. Bibcode:1966RSPSA.294..235D. doi:10.1098/rspa.1966.0205. ISSNÂ 0080-4630. S2CIDÂ 21698093.

^ Jump up to: a b Nilsson, Nils J. (2009-10-30). The Quest for Artificial Intelligence (PDF). Cambridge: Cambridge University Press. ISBNÂ 9780521122931.

^ Edelkamp, Stefan; Jabbar, Shahid; Lluch-Lafuente, Alberto (2005). "Cost-Algebraic Heuristic Search" (PDF). Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI): 1362â1367.

^ Hart, Peter E.; Nilsson, Nils J.; Raphael, Bertram (1972-12-01). "Correction to 'A Formal Basis for the Heuristic Determination of Minimum Cost Paths'" (PDF). ACM SIGART Bulletin (37): 28â29. doi:10.1145/1056777.1056779. ISSNÂ 0163-5719. S2CIDÂ 6386648.

^ Jump up to: a b Dechter, Rina; Judea Pearl (1985). "Generalized best-first search strategies and the optimality of A*". Journal of the ACM. 32 (3): 505â536. doi:10.1145/3828.3830. S2CIDÂ 2092415.

^ Jump up to: a b De Smith, Michael John; Goodchild, Michael F.; Longley, Paul (2007), Geospatial Analysis: A Comprehensive Guide to Principles, Techniques and Software Tools, Troubadour Publishing Ltd, p.Â 344, ISBNÂ 9781905886609.

^ Jump up to: a b Hetland, Magnus Lie (2010), Python Algorithms: Mastering Basic Algorithms in the Python Language, Apress, p.Â 214, ISBNÂ 9781430232377.

^ Martelli, Alberto (1977). "On the Complexity of Admissible Search Algorithms". Artificial Intelligence. 8 (1): 1â13. doi:10.1016/0004-3702(77)90002-9.

^ Felner, Ariel; Uzi Zahavi (2011). "Inconsistent heuristics in theory and practice". Artificial Intelligence. 175 (9â10): 1570â1603. doi:10.1016/j.artint.2011.02.001.

^ Zhang, Zhifu; N. R. Sturtevant (2009). Using Inconsistent Heuristics on A* Search. Twenty-First International Joint Conference on Artificial Intelligence.

^ Pohl, Ira (1970). "First results on the effect of error in heuristic search". Machine Intelligence. 5: 219â236.

^ Jump up to: a b Pearl, Judea (1984). Heuristics: Intelligent Search Strategies for Computer Problem Solving. Addison-Wesley. ISBNÂ 978-0-201-05594-8.

^ Pohl, Ira (August 1973). "The avoidance of (relative) catastrophe, heuristic competence, genuine dynamic weighting and computational issues in heuristic problem solving" (PDF). Proceedings of the Third International Joint Conference on Artificial Intelligence (IJCAI-73). Vol.Â 3. California, USA. pp.Â 11â17.

^ KÃ¶ll, Andreas; Hermann Kaindl (August 1992). "A new approach to dynamic weighting". Proceedings of the Tenth European Conference on Artificial Intelligence (ECAI-92). Vienna, Austria. pp.Â 16â17.

^ Pearl, Judea; Jin H. Kim (1982). "Studies in semi-admissible heuristics". IEEE Transactions on Pattern Analysis and Machine Intelligence. 4 (4): 392â399. doi:10.1109/TPAMI.1982.4767270. PMIDÂ 21869053. S2CIDÂ 3176931.

^ Ghallab, Malik; Dennis Allard (August 1983). "AÎµ â an efficient near admissible heuristic search algorithm" (PDF). Proceedings of the Eighth International Joint Conference on Artificial Intelligence (IJCAI-83). Vol.Â 2. Karlsruhe, Germany. pp.Â 789â791. Archived from the original (PDF) on 2014-08-06.

^ Reese, BjÃ¸rn (1999). "AlphA*: An Îµ-admissible heuristic search algorithm". Archived from the original on 2016-01-31. Retrieved 2014-11-05. {{cite journal}}: Cite journal requires |journal= (help)

^ Jump up to: a b Russell, Stuart; Norvig, Peter (2003) [1995]. Artificial Intelligence: A Modern Approach (2ndÂ ed.). Prentice Hall. pp.Â 97â104. ISBNÂ 978-0137903955.

^ Russell, Stuart; Norvig, Peter (2009) [1995]. Artificial Intelligence: A Modern Approach (3rdÂ ed.). Prentice Hall. p.Â 103. ISBNÂ 978-0-13-604259-4.

^ Klein, Dan; Manning, Christopher D. (2003). A* parsing: fast exact Viterbi parse selection. Proc. NAACL-HLT.

^ Kagan E.; Ben-Gal I. (2014). "A Group-Testing Algorithm with Online Informational Learning" (PDF). IIE Transactions. 46 (2): 164â184. doi:10.1080/0740817X.2013.803639. S2CIDÂ 18588494.

^ Ferguson, Dave; Likhachev, Maxim; Stentz, Anthony (2005). A Guide to Heuristic-based Path Planning (PDF). Proc. ICAPS Workshop on Planning under Uncertainty for Autonomous Systems.

^ Nau, Dana S.; Kumar, Vipin; Kanal, Laveen (1984). "General branch and bound, and its relation to Aâ and AOâ" (PDF). Artificial Intelligence. 23 (1): 29â58. doi:10.1016/0004-3702(84)90004-3.

^ Hansen, Eric A., and Rong Zhou. "Anytime Heuristic Search. Archived 2016-11-05 at the Wayback Machine" J. Artif. Intell. Res.(JAIR) 28 (2007): 267-297.

^  Fareh, Raouf; Baziyad, Mohammed; Rahman, Mohammad H.; Rabie, Tamer; Bettayeb, Maamar (2019-05-14). "Investigating Reduced Path Planning Strategy for Differential Wheeled Mobile Robot". Robotica. 38 (2): 235â255. doi:10.1017/S0263574719000572. ISSNÂ 0263-5747. S2CIDÂ 181849209.

^ Pijls, Wim; Post, Henk "Yet another bidirectional algorithm for shortest paths" In Econometric Institute Report EI 2009-10/Econometric Institute, Erasmus University Rotterdam. Erasmus School of Economics.

^ "Efficient Point-to-Point Shortest Path Algorithms" (PDF). {{cite journal}}: Cite journal requires |journal= (help) from Princeton University


Further reading[edit]
Nilsson, N. J. (1980). Principles of Artificial Intelligence. Palo Alto, California: Tioga Publishing Company. ISBNÂ 978-0-935382-01-3.
External links[edit]
Clear visual A* explanation, with advice and thoughts on path-finding
Variation on A* called Hierarchical Path-Finding A* (HPA*)
Brian Grinstead. "A* Search Algorithm in JavaScript (Updated)". Archived from the original on 15 February 2020. Retrieved 8 February 2021.




<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=A*_search_algorithm&oldid=1060160033"
		Categories: Graph algorithmsRouting algorithmsSearch algorithmsCombinatorial optimizationGame artificial intelligenceGreedy algorithmsGraph distanceHidden categories: CS1 errors: missing periodicalWebarchive template wayback linksArticles with short descriptionShort description is different from WikidataArticles with example pseudocode
	
