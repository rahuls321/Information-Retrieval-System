
Title:
Chernoff bound
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		exponentially decreasing bounds on tail distributions of sums of independent random variables
In probability theory, the Chernoff bound gives exponentially decreasing bounds on tail distributions of sums of independent random variables. Despite being named after Herman Chernoff, the author of the paper it first appeared in,[1] the result is due to Herman Rubin.[2]  It is a sharper bound than the known first- or second-moment-based tail bounds such as Markov's inequality or Chebyshev's inequality, which only yield power-law bounds on tail decay. However, the Chernoff bound requires that the variates be independent â a condition that neither Markov's inequality nor Chebyshev's inequality require, although Chebyshev's inequality does require the variates to be pairwise independent.
It is related to the (historically prior) Bernstein inequalities and to Hoeffding's inequality.

Contents

1 The generic bound
2 Multiplicative form (relative error)
3 Additive form (absolute error)
4 Sums of independent bounded random variables
5 Applications
6 Matrix bound

6.1 Theorem without the dependency on the dimensions
6.2 Theorem with matrices that are not completely random


7 Sampling variant
8 Proofs

8.1 Multiplicative form
8.2 ChernoffâHoeffding theorem (additive form)


9 See also
10 References
11 Further reading



The generic bound[edit]
The generic Chernoff bound for a random variable X is attained by applying Markov's inequality to etX. This gives a bound in terms of the moment-generating function of X. For every 
  
    
      
        t
        â¥
        0
      
    
    {\displaystyle t\geq 0}
  
:


  
    
      
        Pr
        (
        X
        â¥
        a
        )
        =
        Pr
        (
        
          e
          
            t
            â
            X
          
        
        â¥
        
          e
          
            t
            â
            a
          
        
        )
        â¤
        
          
            
              
                E
              
              
                [
                
                  e
                  
                    t
                    â
                    X
                  
                
                ]
              
            
            
              e
              
                t
                â
                a
              
            
          
        
        .
      
    
    {\displaystyle \Pr(X\geq a)=\Pr(e^{t\cdot X}\geq e^{t\cdot a})\leq {\frac {\mathrm {E} \left[e^{t\cdot X}\right]}{e^{t\cdot a}}}.}
  

Since this bound is true for every 
  
    
      
        t
      
    
    {\displaystyle t}
  
, we have:


  
    
      
        Pr
        (
        X
        â¥
        a
        )
        â¤
        
          inf
          
            t
            â¥
            0
          
        
        
          
            
              
                E
              
              
                [
                
                  exp
                  â¡
                  (
                  t
                  â
                  X
                  )
                
                ]
              
            
            
              exp
              â¡
              (
              t
              â
              a
              )
            
          
        
        .
      
    
    {\displaystyle \Pr(X\geq a)\leq \inf _{t\geq 0}{\frac {\mathrm {E} \left[\exp(t\cdot X)\right]}{\exp(t\cdot a)}}.}
  

The Chernoff bound sometimes refers to the above inequality,[3] which was first applied by Sergei Bernstein to prove the related Bernstein inequalities.[citation needed] It is also used to prove Hoeffding's inequality, Bennett's inequality, and McDiarmid's inequality.
This inequality can be applied generally to various classes of distributions, including sub-gaussian distributions,[4] sub-gamma distributions, and sums of independent random variables.[3] Chernoff bounds commonly refer to the case where 
  
    
      
        X
      
    
    {\displaystyle X}
  
 is the sum of independent Bernoulli random variables.[5][6]
When X is the sum of n independent random variables X1, ..., Xn, the moment generating function of X is the product of the individual moment generating functions, giving that





  
    
      
        Pr
        (
        X
        â¥
        a
        )
        â¤
        
          inf
          
            t
            â¥
            0
          
        
        
          
            
              
                E
              
              
                [
                
                  
                    â
                    
                      i
                    
                  
                  
                    e
                    
                      t
                      â
                      
                        X
                        
                          i
                        
                      
                    
                  
                
                ]
              
            
            
              e
              
                t
                â
                a
              
            
          
        
        =
        
          inf
          
            t
            â¥
            0
          
        
        
          e
          
            â
            t
            â
            a
          
        
        
          â
          
            i
          
        
        
          E
        
        
          [
          
            e
            
              t
              â
              
                X
                
                  i
                
              
            
          
          ]
        
        .
      
    
    {\displaystyle \Pr(X\geq a)\leq \inf _{t\geq 0}{\frac {\mathrm {E} \left[\prod _{i}e^{t\cdot X_{i}}\right]}{e^{t\cdot a}}}=\inf _{t\geq 0}e^{-t\cdot a}\prod _{i}\mathrm {E} \left[e^{t\cdot X_{i}}\right].}
  






Â 

Â 

Â 



Â 



(1)

By performing the same analysis on the random variable -X, one can get the same bound in the other direction.


  
    
      
        Pr
        (
        X
        â¤
        a
        )
        â¤
        
          inf
          
            t
            â¥
            0
          
        
        
          e
          
            t
            a
          
        
        
          â
          
            i
          
        
        
          E
        
        
          [
          
            e
            
              â
              t
              
                X
                
                  i
                
              
            
          
          ]
        
      
    
    {\displaystyle \Pr(X\leq a)\leq \inf _{t\geq 0}e^{ta}\prod _{i}\mathrm {E} \left[e^{-tX_{i}}\right]}
  

Specific Chernoff bounds are attained by calculating the moment-generating function 
  
    
      
        
          E
        
        
          [
          
            e
            
              â
              t
              â
              
                X
                
                  i
                
              
            
          
          ]
        
      
    
    {\displaystyle \mathrm {E} \left[e^{-t\cdot X_{i}}\right]}
  
 for specific instances of the random variables 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
  
. The bounds in the following sections for Bernoulli random variables are derived by using that, for a Bernoulli random variable 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
  
 with probability p of being equal to 1,


  
    
      
        
          E
        
        
          [
          
            e
            
              t
              â
              
                X
                
                  i
                
              
            
          
          ]
        
        =
        (
        1
        â
        p
        )
        
          e
          
            0
          
        
        +
        p
        
          e
          
            t
          
        
        =
        1
        +
        p
        (
        
          e
          
            t
          
        
        â
        1
        )
        â¤
        
          e
          
            p
            (
            
              e
              
                t
              
            
            â
            1
            )
          
        
        .
      
    
    {\displaystyle \mathrm {E} \left[e^{t\cdot X_{i}}\right]=(1-p)e^{0}+pe^{t}=1+p(e^{t}-1)\leq e^{p(e^{t}-1)}.}
  

One can encounter many flavors of Chernoff bounds: the original additive form (which gives a bound on the absolute error) or the more practical multiplicative form (which bounds the error relative to the mean).

Multiplicative form (relative error)[edit]
Multiplicative Chernoff bound. Suppose X1, ..., Xn are independent random variables taking values in {0, 1}. Let X denote their sum and let Î¼ = E[X] denote the sum's expected value. Then for any Î´ > 0,


  
    
      
        Pr
        (
        X
        >
        (
        1
        +
        Î´
        )
        Î¼
        )
        <
        
          
            (
            
              
                
                  e
                  
                    Î´
                  
                
                
                  (
                  1
                  +
                  Î´
                  
                    )
                    
                      1
                      +
                      Î´
                    
                  
                
              
            
            )
          
          
            Î¼
          
        
        .
      
    
    {\displaystyle \Pr(X>(1+\delta )\mu )<\left({\frac {e^{\delta }}{(1+\delta )^{1+\delta }}}\right)^{\mu }.}
  

A similar proof strategy can be used to show that


  
    
      
        Pr
        (
        X
        <
        (
        1
        â
        Î´
        )
        Î¼
        )
        <
        
          
            (
            
              
                
                  e
                  
                    â
                    Î´
                  
                
                
                  (
                  1
                  â
                  Î´
                  
                    )
                    
                      1
                      â
                      Î´
                    
                  
                
              
            
            )
          
          
            Î¼
          
        
        .
      
    
    {\displaystyle \Pr(X<(1-\delta )\mu )<\left({\frac {e^{-\delta }}{(1-\delta )^{1-\delta }}}\right)^{\mu }.}
  

The above formula is often unwieldy in practice, so the following looser but more convenient bounds[7] are often used, which follow from the inequality 
  
    
      
        
          
            
              
                2
                Î´
              
              
                2
                +
                Î´
              
            
          
          â¤
          log
          â¡
          (
          1
          +
          Î´
          )
        
      
    
    {\displaystyle \textstyle {\frac {2\delta }{2+\delta }}\leq \log(1+\delta )}
  
 from the list of logarithmic inequalities:


  
    
      
        Pr
        (
        X
        â¤
        (
        1
        â
        Î´
        )
        Î¼
        )
        â¤
        
          e
          
            â
            
              
                
                  
                    Î´
                    
                      2
                    
                  
                  Î¼
                
                2
              
            
          
        
        ,
        
        0
        â¤
        Î´
        â¤
        1
        ,
      
    
    {\displaystyle \Pr(X\leq (1-\delta )\mu )\leq e^{-{\frac {\delta ^{2}\mu }{2}}},\qquad 0\leq \delta \leq 1,}
  


  
    
      
        Pr
        (
        X
        â¥
        (
        1
        +
        Î´
        )
        Î¼
        )
        â¤
        
          e
          
            â
            
              
                
                  
                    Î´
                    
                      2
                    
                  
                  Î¼
                
                
                  2
                  +
                  Î´
                
              
            
          
        
        ,
        
        0
        â¤
        Î´
        ,
      
    
    {\displaystyle \Pr(X\geq (1+\delta )\mu )\leq e^{-{\frac {\delta ^{2}\mu }{2+\delta }}},\qquad 0\leq \delta ,}
  


  
    
      
        Pr
        (
        
          |
        
        X
        â
        Î¼
        
          |
        
        â¥
        Î´
        Î¼
        )
        â¤
        2
        
          e
          
            â
            
              
                
                  
                    Î´
                    
                      2
                    
                  
                  Î¼
                
                3
              
            
          
        
        ,
        
        0
        â¤
        Î´
        â¤
        1.
      
    
    {\displaystyle \Pr(|X-\mu |\geq \delta \mu )\leq 2e^{-{\frac {\delta ^{2}\mu }{3}}},\qquad 0\leq \delta \leq 1.}
  

Additive form (absolute error)[edit]
The following theorem is due to Wassily Hoeffding[8] and hence is called the ChernoffâHoeffding theorem.

ChernoffâHoeffding theorem. Suppose X1, ..., Xn are i.i.d. random variables, taking values in {0, 1}. Let p = E[X1] and Îµ > 0.

  
    
      
        
          
            
              
                Pr
                
                  (
                  
                    
                      
                        1
                        n
                      
                    
                    â
                    
                      X
                      
                        i
                      
                    
                    â¥
                    p
                    +
                    Îµ
                  
                  )
                
                â¤
                
                  
                    (
                    
                      
                        
                          (
                          
                            
                              p
                              
                                p
                                +
                                Îµ
                              
                            
                          
                          )
                        
                        
                          p
                          +
                          Îµ
                        
                      
                      
                        
                          
                            (
                            
                              
                                
                                  1
                                  â
                                  p
                                
                                
                                  1
                                  â
                                  p
                                  â
                                  Îµ
                                
                              
                            
                            )
                          
                        
                        
                          1
                          â
                          p
                          â
                          Îµ
                        
                      
                    
                    )
                  
                  
                    n
                  
                
              
              
                
                =
                
                  e
                  
                    â
                    D
                    (
                    p
                    +
                    Îµ
                    â¥
                    p
                    )
                    n
                  
                
              
            
            
              
                Pr
                
                  (
                  
                    
                      
                        1
                        n
                      
                    
                    â
                    
                      X
                      
                        i
                      
                    
                    â¤
                    p
                    â
                    Îµ
                  
                  )
                
                â¤
                
                  
                    (
                    
                      
                        
                          (
                          
                            
                              p
                              
                                p
                                â
                                Îµ
                              
                            
                          
                          )
                        
                        
                          p
                          â
                          Îµ
                        
                      
                      
                        
                          
                            (
                            
                              
                                
                                  1
                                  â
                                  p
                                
                                
                                  1
                                  â
                                  p
                                  +
                                  Îµ
                                
                              
                            
                            )
                          
                        
                        
                          1
                          â
                          p
                          +
                          Îµ
                        
                      
                    
                    )
                  
                  
                    n
                  
                
              
              
                
                =
                
                  e
                  
                    â
                    D
                    (
                    p
                    â
                    Îµ
                    â¥
                    p
                    )
                    n
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\Pr \left({\frac {1}{n}}\sum X_{i}\geq p+\varepsilon \right)\leq \left(\left({\frac {p}{p+\varepsilon }}\right)^{p+\varepsilon }{\left({\frac {1-p}{1-p-\varepsilon }}\right)}^{1-p-\varepsilon }\right)^{n}&=e^{-D(p+\varepsilon \parallel p)n}\\\Pr \left({\frac {1}{n}}\sum X_{i}\leq p-\varepsilon \right)\leq \left(\left({\frac {p}{p-\varepsilon }}\right)^{p-\varepsilon }{\left({\frac {1-p}{1-p+\varepsilon }}\right)}^{1-p+\varepsilon }\right)^{n}&=e^{-D(p-\varepsilon \parallel p)n}\end{aligned}}}
  

where

  
    
      
        D
        (
        x
        â¥
        y
        )
        =
        x
        ln
        â¡
        
          
            x
            y
          
        
        +
        (
        1
        â
        x
        )
        ln
        â¡
        
          (
          
            
              
                1
                â
                x
              
              
                1
                â
                y
              
            
          
          )
        
      
    
    {\displaystyle D(x\parallel y)=x\ln {\frac {x}{y}}+(1-x)\ln \left({\frac {1-x}{1-y}}\right)}
  

is the KullbackâLeibler divergence between Bernoulli distributed random variables with parameters x and y respectively. If p â¥ .mw-parser-output .sfrac{white-space:nowrap}.mw-parser-output .sfrac.tion,.mw-parser-output .sfrac .tion{display:inline-block;vertical-align:-0.5em;font-size:85%;text-align:center}.mw-parser-output .sfrac .num,.mw-parser-output .sfrac .den{display:block;line-height:1em;margin:0 0.1em}.mw-parser-output .sfrac .den{border-top:1px solid}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}1/2, then 
  
    
      
        D
        (
        p
        +
        Îµ
        â¥
        p
        )
        â¥
        
          
            
              
                Îµ
                
                  2
                
              
              
                2
                p
                (
                1
                â
                p
                )
              
            
          
        
      
    
    {\displaystyle D(p+\varepsilon \parallel p)\geq {\tfrac {\varepsilon ^{2}}{2p(1-p)}}}
  
 which means

  
    
      
        Pr
        
          (
          
            
              
                1
                n
              
            
            â
            
              X
              
                i
              
            
            >
            p
            +
            x
          
          )
        
        â¤
        exp
        â¡
        
          (
          
            â
            
              
                
                  
                    x
                    
                      2
                    
                  
                  n
                
                
                  2
                  p
                  (
                  1
                  â
                  p
                  )
                
              
            
          
          )
        
        .
      
    
    {\displaystyle \Pr \left({\frac {1}{n}}\sum X_{i}>p+x\right)\leq \exp \left(-{\frac {x^{2}n}{2p(1-p)}}\right).}
  

A simpler bound follows by relaxing the theorem using D(p + Îµ || p) â¥ 2Îµ2, which follows from the convexity of D(p + Îµ || p) and the fact that


  
    
      
        
          
            
              d
              
                2
              
            
            
              d
              
                Îµ
                
                  2
                
              
            
          
        
        D
        (
        p
        +
        Îµ
        â¥
        p
        )
        =
        
          
            1
            
              (
              p
              +
              Îµ
              )
              (
              1
              â
              p
              â
              Îµ
              )
            
          
        
        â¥
        4
        =
        
          
            
              d
              
                2
              
            
            
              d
              
                Îµ
                
                  2
                
              
            
          
        
        (
        2
        
          Îµ
          
            2
          
        
        )
        .
      
    
    {\displaystyle {\frac {d^{2}}{d\varepsilon ^{2}}}D(p+\varepsilon \parallel p)={\frac {1}{(p+\varepsilon )(1-p-\varepsilon )}}\geq 4={\frac {d^{2}}{d\varepsilon ^{2}}}(2\varepsilon ^{2}).}
  

This result is a special case of Hoeffding's inequality. Sometimes, the bounds


  
    
      
        
          
            
              
                D
                (
                (
                1
                +
                x
                )
                p
                â¥
                p
                )
                â¥
                
                  
                    1
                    4
                  
                
                
                  x
                  
                    2
                  
                
                p
                ,
              
              
              
              
                
                  â
                  
                    
                      
                        1
                        2
                      
                    
                  
                
                â¤
                x
                â¤
                
                  
                    
                      1
                      2
                    
                  
                
                ,
              
            
            
              
                D
                (
                x
                â¥
                y
                )
                â¥
                
                  
                    
                      3
                      (
                      x
                      â
                      y
                      
                        )
                        
                          2
                        
                      
                    
                    
                      2
                      (
                      2
                      y
                      +
                      x
                      )
                    
                  
                
                ,
              
            
            
              
                D
                (
                x
                â¥
                y
                )
                â¥
                
                  
                    
                      (
                      x
                      â
                      y
                      
                        )
                        
                          2
                        
                      
                    
                    
                      2
                      y
                    
                  
                
                ,
              
              
              
              
                x
                â¤
                y
                ,
              
            
            
              
                D
                (
                x
                â¥
                y
                )
                â¥
                
                  
                    
                      (
                      x
                      â
                      y
                      
                        )
                        
                          2
                        
                      
                    
                    
                      2
                      x
                    
                  
                
                ,
              
              
              
              
                x
                â¥
                y
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}D((1+x)p\parallel p)\geq {\frac {1}{4}}x^{2}p,&&&{-{\tfrac {1}{2}}}\leq x\leq {\tfrac {1}{2}},\\[6pt]D(x\parallel y)\geq {\frac {3(x-y)^{2}}{2(2y+x)}},\\[6pt]D(x\parallel y)\geq {\frac {(x-y)^{2}}{2y}},&&&x\leq y,\\[6pt]D(x\parallel y)\geq {\frac {(x-y)^{2}}{2x}},&&&x\geq y\end{aligned}}}
  

which are stronger for p < 1/8, are also used.

Sums of independent bounded random variables[edit]
.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}Main article: Hoeffding's inequality
Chernoff bounds may also be applied to general sums of independent, bounded random variables, regardless of their distribution; this is known as Hoeffding's inequality. The proof follows a similar approach to the other Chernoff bounds, but applying Hoeffding's lemma to bound the moment generating functions (see Hoeffding's inequality).  

Hoeffding's inequality. Suppose X1, ..., Xn are independent random variables taking values in [a,b]. Let X denote their sum and let Î¼ = E[X] denote the sum's expected value. Then for any Î´ > 0,

  
    
      
        Pr
        (
        X
        â¤
        (
        1
        â
        Î´
        )
        Î¼
        )
        <
        
          e
          
            â
            
              
                
                  2
                  
                    Î´
                    
                      2
                    
                  
                  
                    Î¼
                    
                      2
                    
                  
                
                
                  n
                  (
                  b
                  â
                  a
                  
                    )
                    
                      2
                    
                  
                
              
            
          
        
        ,
      
    
    {\displaystyle \Pr(X\leq (1-\delta )\mu )<e^{-{\frac {2\delta ^{2}\mu ^{2}}{n(b-a)^{2}}}},}
  


  
    
      
        Pr
        (
        X
        â¥
        (
        1
        +
        Î´
        )
        Î¼
        )
        <
        
          e
          
            â
            
              
                
                  2
                  
                    Î´
                    
                      2
                    
                  
                  
                    Î¼
                    
                      2
                    
                  
                
                
                  n
                  (
                  b
                  â
                  a
                  
                    )
                    
                      2
                    
                  
                
              
            
          
        
        .
      
    
    {\displaystyle \Pr(X\geq (1+\delta )\mu )<e^{-{\frac {2\delta ^{2}\mu ^{2}}{n(b-a)^{2}}}}.}
  

Applications[edit]
Chernoff bounds have very useful applications in set balancing and packet routing in sparse networks.
The set balancing problem arises while designing statistical experiments. Typically while designing a statistical experiment, given the features of each participant in the experiment, we need to know how to divide the participants into 2 disjoint groups such that each feature is roughly as balanced as possible between the two groups.[9]
Chernoff bounds are also used to obtain tight bounds for permutation routing problems which reduce network congestion while routing packets in sparse networks.[9]
Chernoff bounds are used in computational learning theory to prove that a learning algorithm is probably approximately correct, i.e. with high probability the algorithm has small error on a sufficiently large training data set.[10]
Chernoff bounds can be effectively used to evaluate the "robustness level" of an application/algorithm by exploring its perturbation space with randomization.[11]
The use of the Chernoff bound permits one to abandon the strongâand mostly unrealisticâsmall perturbation hypothesis (the perturbation magnitude is small). The robustness level can be, in turn, used either to validate or reject a specific algorithmic choice, a hardware implementation or the appropriateness of a solution whose structural parameters are affected by uncertainties.
A simple and common use of Chernoff bounds is for "boosting" of randomized algorithms. If one has an algorithm that outputs a guess that is the desired answer with probability p > 1/2, then one can get a higher success rate by running the algorithm 
  
    
      
        n
        =
        log
        â¡
        (
        1
        
          /
        
        Î´
        )
        2
        p
        
          /
        
        (
        p
        â
        1
        
          /
        
        2
        
          )
          
            2
          
        
      
    
    {\displaystyle n=\log(1/\delta )2p/(p-1/2)^{2}}
  
 times and outputting a guess that is output by more than n/2 runs of the algorithm. (There cannot be more than one such guess by the pigeonhole principle.) Assuming that these algorithm runs are independent, the probability that more than n/2 of the guesses is correct is equal to the probability that the sum of independent Bernoulli random variables Xk that are 1 with probability p is more than n/2. This can be shown to be at least 
  
    
      
        1
        â
        Î´
      
    
    {\displaystyle 1-\delta }
  
 via the multiplicative Chernoff bound (Corollary 13.3 in Sinclair's class notes, Î¼ = np).[12]:


  
    
      
        Pr
        
          [
          
            X
            >
            
              
                n
                2
              
            
          
          ]
        
        â¥
        1
        â
        
          e
          
            â
            
              
                1
                
                  2
                  p
                
              
            
            n
            
              
                (
                
                  p
                  â
                  
                    
                      1
                      2
                    
                  
                
                )
              
              
                2
              
            
          
        
        â¥
        1
        â
        Î´
      
    
    {\displaystyle \Pr \left[X>{n \over 2}\right]\geq 1-e^{-{\frac {1}{2p}}n\left(p-{\frac {1}{2}}\right)^{2}}\geq 1-\delta }
  

Matrix bound[edit]
Main article: Matrix Chernoff bound
Rudolf Ahlswede and Andreas Winter introduced a Chernoff bound for matrix-valued random variables.[13] The following version of the inequality can be found in the work of Tropp.[14]
Let M1, ..., Mt be independent matrix valued random variables such that 
  
    
      
        
          M
          
            i
          
        
        â
        
          
            C
          
          
            
              d
              
                1
              
            
            Ã
            
              d
              
                2
              
            
          
        
      
    
    {\displaystyle M_{i}\in \mathbb {C} ^{d_{1}\times d_{2}}}
  
 and 
  
    
      
        
          E
        
        [
        
          M
          
            i
          
        
        ]
        =
        0
      
    
    {\displaystyle \mathbb {E} [M_{i}]=0}
  
.
Let us denote by 
  
    
      
        â
        M
        â
      
    
    {\displaystyle \lVert M\rVert }
  
 the operator norm of the matrix 
  
    
      
        M
      
    
    {\displaystyle M}
  
. If 
  
    
      
        â
        
          M
          
            i
          
        
        â
        â¤
        Î³
      
    
    {\displaystyle \lVert M_{i}\rVert \leq \gamma }
  
 holds almost surely for all 
  
    
      
        i
        â
        {
        1
        ,
        â¦
        ,
        t
        }
      
    
    {\displaystyle i\in \{1,\ldots ,t\}}
  
, then for every Îµ > 0


  
    
      
        Pr
        
          (
          
            
              â
              
                
                  
                    1
                    t
                  
                
                
                  â
                  
                    i
                    =
                    1
                  
                  
                    t
                  
                
                
                  M
                  
                    i
                  
                
              
              â
            
            >
            Îµ
          
          )
        
        â¤
        (
        
          d
          
            1
          
        
        +
        
          d
          
            2
          
        
        )
        exp
        â¡
        
          (
          
            â
            
              
                
                  3
                  
                    Îµ
                    
                      2
                    
                  
                  t
                
                
                  8
                  
                    Î³
                    
                      2
                    
                  
                
              
            
          
          )
        
        .
      
    
    {\displaystyle \Pr \left(\left\|{\frac {1}{t}}\sum _{i=1}^{t}M_{i}\right\|>\varepsilon \right)\leq (d_{1}+d_{2})\exp \left(-{\frac {3\varepsilon ^{2}t}{8\gamma ^{2}}}\right).}
  

Notice that in order to conclude that the deviation from 0 is bounded by Îµ with high probability, we need to choose a number of samples 
  
    
      
        t
      
    
    {\displaystyle t}
  
 proportional to the logarithm of 
  
    
      
        
          d
          
            1
          
        
        +
        
          d
          
            2
          
        
      
    
    {\displaystyle d_{1}+d_{2}}
  
. In general, unfortunately, a dependence on  
  
    
      
        log
        â¡
        (
        min
        (
        
          d
          
            1
          
        
        ,
        
          d
          
            2
          
        
        )
        )
      
    
    {\displaystyle \log(\min(d_{1},d_{2}))}
  
 is inevitable: take for example a diagonal random sign matrix of dimension 
  
    
      
        d
        Ã
        d
      
    
    {\displaystyle d\times d}
  
. The operator norm of the sum of t independent samples is precisely the maximum deviation among d independent random walks of length t. In order to achieve a fixed bound on the maximum deviation with constant probability, it is easy to see that t should grow logarithmically with d in this scenario.[15]
The following theorem can be obtained by assuming M has low rank, in order to avoid the dependency on the dimensions.

Theorem without the dependency on the dimensions[edit]
Let 0 < Îµ < 1 and M be a random symmetric real matrix with 
  
    
      
        â
        
          E
        
        [
        M
        ]
        â
        â¤
        1
      
    
    {\displaystyle \|\mathrm {E} [M]\|\leq 1}
  
 and 
  
    
      
        â
        M
        â
        â¤
        Î³
      
    
    {\displaystyle \|M\|\leq \gamma }
  
 almost surely. Assume that each element on the support of M has at most rank r. Set 


  
    
      
        t
        =
        Î©
        
          (
          
            
              
                Î³
                log
                â¡
                (
                Î³
                
                  /
                
                
                  Îµ
                  
                    2
                  
                
                )
              
              
                Îµ
                
                  2
                
              
            
          
          )
        
        .
      
    
    {\displaystyle t=\Omega \left({\frac {\gamma \log(\gamma /\varepsilon ^{2})}{\varepsilon ^{2}}}\right).}
  

If 
  
    
      
        r
        â¤
        t
      
    
    {\displaystyle r\leq t}
  
 holds almost surely, then


  
    
      
        Pr
        
          (
          
            
              â
              
                
                  
                    1
                    t
                  
                
                
                  â
                  
                    i
                    =
                    1
                  
                  
                    t
                  
                
                
                  M
                  
                    i
                  
                
                â
                
                  E
                
                [
                M
                ]
              
              â
            
            >
            Îµ
          
          )
        
        â¤
        
          
            1
            
              
                p
                o
                l
                y
              
              (
              t
              )
            
          
        
      
    
    {\displaystyle \Pr \left(\left\|{\frac {1}{t}}\sum _{i=1}^{t}M_{i}-\mathrm {E} [M]\right\|>\varepsilon \right)\leq {\frac {1}{\mathbf {poly} (t)}}}
  

where M1, ..., Mt are i.i.d. copies of M.

Theorem with matrices that are not completely random[edit]
Garg, Lee, Song and Srivastava [16] proved a Chernoff-type bound for sums of matrix-valued random variables sampled via a random walk on an expander, confirming a conjecture due to Wigderson and Xiao. 
Kyng and Song [17] proved a Chernoff-type bound for sums of Laplacian matrix of random spanning trees.

Sampling variant[edit]
The following variant of Chernoff's bound can be used to bound the probability that a majority in a population will become a minority in a sample, or vice versa.[18]
Suppose there is a general population A and a sub-population BâA. Mark the relative size of the sub-population (|B|/|A|) by r.
Suppose we pick an integer k and a random sample SâA of size k. Mark the relative size of the sub-population in the sample (|Bâ©S|/|S|) by rS.
Then, for every fraction dâ[0,1]:


  
    
      
        
          P
          r
        
        
          (
          
            
              r
              
                S
              
            
            <
            (
            1
            â
            d
            )
            â
            r
          
          )
        
        <
        exp
        â¡
        
          (
          
            â
            r
            â
            
              d
              
                2
              
            
            â
            k
            
              /
            
            2
          
          )
        
      
    
    {\displaystyle \mathrm {Pr} \left(r_{S}<(1-d)\cdot r\right)<\exp \left(-r\cdot d^{2}\cdot k/2\right)}
  

In particular, if B is a majority in A (i.e. r > 0.5) we can bound the probability that B will remain majority in S (rS>0.5) by taking: d = 1 - 1 / (2 r):[19]


  
    
      
        
          P
          r
        
        
          (
          
            
              r
              
                S
              
            
            >
            0.5
          
          )
        
        >
        1
        â
        exp
        â¡
        
          (
          
            â
            r
            â
            
              
                (
                
                  1
                  â
                  
                    
                      1
                      
                        2
                        r
                      
                    
                  
                
                )
              
              
                2
              
            
            â
            k
            
              /
            
            2
          
          )
        
      
    
    {\displaystyle \mathrm {Pr} \left(r_{S}>0.5\right)>1-\exp \left(-r\cdot \left(1-{\frac {1}{2r}}\right)^{2}\cdot k/2\right)}
  

This bound is of course not tight at all. For example, when r=0.5 we get a trivial bound Prob > 0.

Proofs[edit]
Multiplicative form[edit]
Following the conditions of the multiplicative Chernoff bound, let X1, ..., Xn be independent Bernoulli random variables, whose sum is X, each having probability pi of being equal to 1. For a Bernoulli variable:


  
    
      
        
          E
        
        
          [
          
            e
            
              t
              â
              
                X
                
                  i
                
              
            
          
          ]
        
        =
        (
        1
        â
        
          p
          
            i
          
        
        )
        
          e
          
            0
          
        
        +
        
          p
          
            i
          
        
        
          e
          
            t
          
        
        =
        1
        +
        
          p
          
            i
          
        
        (
        
          e
          
            t
          
        
        â
        1
        )
        â¤
        
          e
          
            
              p
              
                i
              
            
            (
            
              e
              
                t
              
            
            â
            1
            )
          
        
      
    
    {\displaystyle \mathrm {E} \left[e^{t\cdot X_{i}}\right]=(1-p_{i})e^{0}+p_{i}e^{t}=1+p_{i}(e^{t}-1)\leq e^{p_{i}(e^{t}-1)}}
  

So, using (1) with 
  
    
      
        a
        =
        (
        1
        +
        Î´
        )
        Î¼
      
    
    {\displaystyle a=(1+\delta )\mu }
  
 for any 
  
    
      
        Î´
        >
        0
      
    
    {\displaystyle \delta >0}
  
 and where 
  
    
      
        Î¼
        =
        
          E
        
        [
        X
        ]
        =
        
          
            â
            
              i
              =
              1
            
            
              n
            
          
          
            p
            
              i
            
          
        
      
    
    {\displaystyle \mu =\mathrm {E} [X]=\textstyle \sum _{i=1}^{n}p_{i}}
  
,


  
    
      
        
          
            
              
                Pr
                (
                X
                >
                (
                1
                +
                Î´
                )
                Î¼
                )
              
              
                
                â¤
                
                  inf
                  
                    t
                    â¥
                    0
                  
                
                exp
                â¡
                (
                â
                t
                (
                1
                +
                Î´
                )
                Î¼
                )
                
                  â
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                E
                â¡
                [
                exp
                â¡
                (
                t
                
                  X
                  
                    i
                  
                
                )
                ]
              
            
            
              
              
                
                â¤
                
                  inf
                  
                    t
                    â¥
                    0
                  
                
                exp
                â¡
                
                  
                    (
                  
                
                â
                t
                (
                1
                +
                Î´
                )
                Î¼
                +
                
                  â
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  p
                  
                    i
                  
                
                (
                
                  e
                  
                    t
                  
                
                â
                1
                )
                
                  
                    )
                  
                
              
            
            
              
              
                
                =
                
                  inf
                  
                    t
                    â¥
                    0
                  
                
                exp
                â¡
                
                  
                    (
                  
                
                â
                t
                (
                1
                +
                Î´
                )
                Î¼
                +
                (
                
                  e
                  
                    t
                  
                
                â
                1
                )
                Î¼
                
                  
                    )
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\Pr(X>(1+\delta )\mu )&\leq \inf _{t\geq 0}\exp(-t(1+\delta )\mu )\prod _{i=1}^{n}\operatorname {E} [\exp(tX_{i})]\\[4pt]&\leq \inf _{t\geq 0}\exp {\Big (}-t(1+\delta )\mu +\sum _{i=1}^{n}p_{i}(e^{t}-1){\Big )}\\[4pt]&=\inf _{t\geq 0}\exp {\Big (}-t(1+\delta )\mu +(e^{t}-1)\mu {\Big )}.\end{aligned}}}
  

If we simply set t = log(1 + Î´) so that t > 0 for Î´ > 0, we can substitute and find


  
    
      
        exp
        â¡
        
          
            (
          
        
        â
        t
        (
        1
        +
        Î´
        )
        Î¼
        +
        (
        
          e
          
            t
          
        
        â
        1
        )
        Î¼
        
          
            )
          
        
        =
        
          
            
              exp
              â¡
              (
              (
              1
              +
              Î´
              â
              1
              )
              Î¼
              )
            
            
              (
              1
              +
              Î´
              
                )
                
                  (
                  1
                  +
                  Î´
                  )
                  Î¼
                
              
            
          
        
        =
        
          
            [
            
              
                
                  e
                  
                    Î´
                  
                
                
                  (
                  1
                  +
                  Î´
                  
                    )
                    
                      (
                      1
                      +
                      Î´
                      )
                    
                  
                
              
            
            ]
          
          
            Î¼
          
        
        .
      
    
    {\displaystyle \exp {\Big (}-t(1+\delta )\mu +(e^{t}-1)\mu {\Big )}={\frac {\exp((1+\delta -1)\mu )}{(1+\delta )^{(1+\delta )\mu }}}=\left[{\frac {e^{\delta }}{(1+\delta )^{(1+\delta )}}}\right]^{\mu }.}
  

This proves the result desired.

ChernoffâHoeffding theorem (additive form)[edit]
Let q = p + Îµ. Taking a = nq in (1), we obtain:


  
    
      
        Pr
        
          (
          
            
              
                1
                n
              
            
            â
            
              X
              
                i
              
            
            â¥
            q
          
          )
        
        â¤
        
          inf
          
            t
            >
            0
          
        
        
          
            
              E
              
                [
                
                  â
                  
                    e
                    
                      t
                      
                        X
                        
                          i
                        
                      
                    
                  
                
                ]
              
            
            
              e
              
                t
                n
                q
              
            
          
        
        =
        
          inf
          
            t
            >
            0
          
        
        
          
            (
            
              
                
                  E
                  
                    [
                    
                      e
                      
                        t
                        
                          X
                          
                            i
                          
                        
                      
                    
                    ]
                  
                
                
                  e
                  
                    t
                    q
                  
                
              
            
            )
          
          
            n
          
        
        .
      
    
    {\displaystyle \Pr \left({\frac {1}{n}}\sum X_{i}\geq q\right)\leq \inf _{t>0}{\frac {E\left[\prod e^{tX_{i}}\right]}{e^{tnq}}}=\inf _{t>0}\left({\frac {E\left[e^{tX_{i}}\right]}{e^{tq}}}\right)^{n}.}
  

Now, knowing that Pr(Xi = 1) = p, Pr(Xi = 0) = 1 â p, we have


  
    
      
        
          
            (
            
              
                
                  
                    E
                  
                  
                    [
                    
                      e
                      
                        t
                        
                          X
                          
                            i
                          
                        
                      
                    
                    ]
                  
                
                
                  e
                  
                    t
                    q
                  
                
              
            
            )
          
          
            n
          
        
        =
        
          
            (
            
              
                
                  p
                  
                    e
                    
                      t
                    
                  
                  +
                  (
                  1
                  â
                  p
                  )
                
                
                  e
                  
                    t
                    q
                  
                
              
            
            )
          
          
            n
          
        
        =
        
          
            (
            
              p
              
                e
                
                  (
                  1
                  â
                  q
                  )
                  t
                
              
              +
              (
              1
              â
              p
              )
              
                e
                
                  â
                  q
                  t
                
              
            
            )
          
          
            n
          
        
        .
      
    
    {\displaystyle \left({\frac {\mathrm {E} \left[e^{tX_{i}}\right]}{e^{tq}}}\right)^{n}=\left({\frac {pe^{t}+(1-p)}{e^{tq}}}\right)^{n}=\left(pe^{(1-q)t}+(1-p)e^{-qt}\right)^{n}.}
  

Therefore, we can easily compute the infimum, using calculus:


  
    
      
        
          
            d
            
              d
              t
            
          
        
        
          (
          
            p
            
              e
              
                (
                1
                â
                q
                )
                t
              
            
            +
            (
            1
            â
            p
            )
            
              e
              
                â
                q
                t
              
            
          
          )
        
        =
        (
        1
        â
        q
        )
        p
        
          e
          
            (
            1
            â
            q
            )
            t
          
        
        â
        q
        (
        1
        â
        p
        )
        
          e
          
            â
            q
            t
          
        
      
    
    {\displaystyle {\frac {d}{dt}}\left(pe^{(1-q)t}+(1-p)e^{-qt}\right)=(1-q)pe^{(1-q)t}-q(1-p)e^{-qt}}
  

Setting the equation to zero and solving, we have


  
    
      
        
          
            
              
                (
                1
                â
                q
                )
                p
                
                  e
                  
                    (
                    1
                    â
                    q
                    )
                    t
                  
                
              
              
                
                =
                q
                (
                1
                â
                p
                )
                
                  e
                  
                    â
                    q
                    t
                  
                
              
            
            
              
                (
                1
                â
                q
                )
                p
                
                  e
                  
                    t
                  
                
              
              
                
                =
                q
                (
                1
                â
                p
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}(1-q)pe^{(1-q)t}&=q(1-p)e^{-qt}\\(1-q)pe^{t}&=q(1-p)\end{aligned}}}
  

so that


  
    
      
        
          e
          
            t
          
        
        =
        
          
            
              (
              1
              â
              p
              )
              q
            
            
              (
              1
              â
              q
              )
              p
            
          
        
        .
      
    
    {\displaystyle e^{t}={\frac {(1-p)q}{(1-q)p}}.}
  

Thus,


  
    
      
        t
        =
        log
        â¡
        
          (
          
            
              
                (
                1
                â
                p
                )
                q
              
              
                (
                1
                â
                q
                )
                p
              
            
          
          )
        
        .
      
    
    {\displaystyle t=\log \left({\frac {(1-p)q}{(1-q)p}}\right).}
  

As q = p + Îµ > p, we see that t > 0, so our bound is satisfied on t. Having solved for t, we can plug back into the equations above to find that


  
    
      
        
          
            
              
                log
                â¡
                
                  (
                  
                    p
                    
                      e
                      
                        (
                        1
                        â
                        q
                        )
                        t
                      
                    
                    +
                    (
                    1
                    â
                    p
                    )
                    
                      e
                      
                        â
                        q
                        t
                      
                    
                  
                  )
                
              
              
                
                =
                log
                â¡
                
                  (
                  
                    
                      e
                      
                        â
                        q
                        t
                      
                    
                    (
                    1
                    â
                    p
                    +
                    p
                    
                      e
                      
                        t
                      
                    
                    )
                  
                  )
                
              
            
            
              
              
                
                =
                log
                â¡
                
                  (
                  
                    e
                    
                      â
                      q
                      log
                      â¡
                      
                        (
                        
                          
                            
                              (
                              1
                              â
                              p
                              )
                              q
                            
                            
                              (
                              1
                              â
                              q
                              )
                              p
                            
                          
                        
                        )
                      
                    
                  
                  )
                
                +
                log
                â¡
                
                  (
                  
                    1
                    â
                    p
                    +
                    p
                    
                      e
                      
                        log
                        â¡
                        
                          (
                          
                            
                              
                                1
                                â
                                p
                              
                              
                                1
                                â
                                q
                              
                            
                          
                          )
                        
                      
                    
                    
                      e
                      
                        log
                        â¡
                        
                          
                            q
                            p
                          
                        
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                â
                q
                log
                â¡
                
                  
                    
                      1
                      â
                      p
                    
                    
                      1
                      â
                      q
                    
                  
                
                â
                q
                log
                â¡
                
                  
                    q
                    p
                  
                
                +
                log
                â¡
                
                  (
                  
                    1
                    â
                    p
                    +
                    p
                    
                      (
                      
                        
                          
                            1
                            â
                            p
                          
                          
                            1
                            â
                            q
                          
                        
                      
                      )
                    
                    
                      
                        q
                        p
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                â
                q
                log
                â¡
                
                  
                    
                      1
                      â
                      p
                    
                    
                      1
                      â
                      q
                    
                  
                
                â
                q
                log
                â¡
                
                  
                    q
                    p
                  
                
                +
                log
                â¡
                
                  (
                  
                    
                      
                        
                          (
                          1
                          â
                          p
                          )
                          (
                          1
                          â
                          q
                          )
                        
                        
                          1
                          â
                          q
                        
                      
                    
                    +
                    
                      
                        
                          (
                          1
                          â
                          p
                          )
                          q
                        
                        
                          1
                          â
                          q
                        
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                â
                q
                log
                â¡
                
                  
                    q
                    p
                  
                
                +
                
                  (
                  
                    â
                    q
                    log
                    â¡
                    
                      
                        
                          1
                          â
                          p
                        
                        
                          1
                          â
                          q
                        
                      
                    
                    +
                    log
                    â¡
                    
                      
                        
                          1
                          â
                          p
                        
                        
                          1
                          â
                          q
                        
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                â
                q
                log
                â¡
                
                  
                    q
                    p
                  
                
                +
                (
                1
                â
                q
                )
                log
                â¡
                
                  
                    
                      1
                      â
                      p
                    
                    
                      1
                      â
                      q
                    
                  
                
              
            
            
              
              
                
                =
                â
                D
                (
                q
                â¥
                p
                )
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\log \left(pe^{(1-q)t}+(1-p)e^{-qt}\right)&=\log \left(e^{-qt}(1-p+pe^{t})\right)\\&=\log \left(e^{-q\log \left({\frac {(1-p)q}{(1-q)p}}\right)}\right)+\log \left(1-p+pe^{\log \left({\frac {1-p}{1-q}}\right)}e^{\log {\frac {q}{p}}}\right)\\&=-q\log {\frac {1-p}{1-q}}-q\log {\frac {q}{p}}+\log \left(1-p+p\left({\frac {1-p}{1-q}}\right){\frac {q}{p}}\right)\\&=-q\log {\frac {1-p}{1-q}}-q\log {\frac {q}{p}}+\log \left({\frac {(1-p)(1-q)}{1-q}}+{\frac {(1-p)q}{1-q}}\right)\\&=-q\log {\frac {q}{p}}+\left(-q\log {\frac {1-p}{1-q}}+\log {\frac {1-p}{1-q}}\right)\\&=-q\log {\frac {q}{p}}+(1-q)\log {\frac {1-p}{1-q}}\\&=-D(q\parallel p).\end{aligned}}}
  

We now have our desired result, that


  
    
      
        Pr
        
          (
          
            
              
                
                  1
                  n
                
              
            
            â
            
              X
              
                i
              
            
            â¥
            p
            +
            Îµ
          
          )
        
        â¤
        
          e
          
            â
            D
            (
            p
            +
            Îµ
            â¥
            p
            )
            n
          
        
        .
      
    
    {\displaystyle \Pr \left({\tfrac {1}{n}}\sum X_{i}\geq p+\varepsilon \right)\leq e^{-D(p+\varepsilon \parallel p)n}.}
  

To complete the proof for the symmetric case, we simply define the random variable Yi = 1 â Xi, apply the same proof, and plug it into our bound.

See also[edit]
Bernstein inequalities
Concentration inequality - a summary of tail-bounds on random variables.
CramÃ©r's theorem
Entropic value at risk
Hoeffding's inequality
Matrix Chernoff bound
Moment generating function
References[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Chernoff, Herman (1952). "A Measure of Asymptotic Efficiency for Tests of a Hypothesis Based on the sum of Observations". The Annals of Mathematical Statistics. 23 (4): 493â507. doi:10.1214/aoms/1177729330. ISSNÂ 0003-4851. JSTORÂ 2236576.

^ Chernoff, Herman (2014). "A career in statistics" (PDF).  In Lin, Xihong; Genest, Christian; Banks, David L.; Molenberghs, Geert; Scott, David W.; Wang, Jane-Ling (eds.). Past, Present, and Future of Statistics. CRC Press. p.Â 35. ISBNÂ 9781482204964.

^ Jump up to: a b Boucheron, StÃ©phane (2013). Concentration Inequalities: a Nonasymptotic Theory of Independence. GÃ¡bor Lugosi, Pascal Massart. Oxford: Oxford University Press. p.Â 21. ISBNÂ 978-0-19-953525-5. OCLCÂ 837517674.

^ Wainwright, M. (January 22, 2015). "Basic tail and concentration bounds" (PDF). Archived (PDF) from the original on 2016-05-08.

^ Vershynin, Roman (2018). High-dimensional probabilityÂ : an introduction with applications in data science. Cambridge, United Kingdom. p.Â 19. ISBNÂ 978-1-108-41519-4. OCLCÂ 1029247498.

^ Tropp, Joel A. (2015-05-26). "An Introduction to Matrix Concentration Inequalities". Foundations and Trends in Machine Learning. 8 (1â2): 60. arXiv:1501.01571. doi:10.1561/2200000048. ISSNÂ 1935-8237. S2CIDÂ 5679583.

^ Mitzenmacher, Michael; Upfal, Eli (2005). Probability and Computing: Randomized Algorithms and Probabilistic Analysis. Cambridge University Press. ISBNÂ 978-0-521-83540-4.

^ Hoeffding, W. (1963). "Probability Inequalities for Sums of Bounded Random Variables" (PDF). Journal of the American Statistical Association. 58 (301): 13â30. doi:10.2307/2282952. JSTORÂ 2282952.

^ Jump up to: a b Refer to this book section for more info on the problem.

^ Kearns, M.; Vazirani, U. (1994). An Introduction to Computational Learning Theory. MIT Press. Chapter 9 (Appendix), pages 190â192. ISBNÂ 0-262-11193-4.

^ Alippi, C. (2014). "Randomized Algorithms". Intelligence for Embedded Systems. Springer. ISBNÂ 978-3-319-05278-6.

^ Sinclair, Alistair (Fall 2011). "Class notes for the course "Randomness and Computation"" (PDF). Archived from the original (PDF) on 31 October 2014. Retrieved 30 October 2014.

^ Ahlswede, R.; Winter, A. (2003). "Strong Converse for Identification via Quantum Channels". IEEE Transactions on Information Theory. 48 (3): 569â579. arXiv:quant-ph/0012127. doi:10.1109/18.985947. S2CIDÂ 523176.

^ Tropp, J. (2010). "User-friendly tail bounds for sums of random matrices". Foundations of Computational Mathematics. 12 (4): 389â434. arXiv:1004.4389. doi:10.1007/s10208-011-9099-z. S2CIDÂ 17735965.

^ Magen, A.; Zouzias, A. (2011). "Low Rank Matrix-Valued Chernoff Bounds and Approximate Matrix Multiplication". arXiv:1005.2724 [cs.DM].

^ Garg, Ankit; Lee, Yin Tat; Song, Zhao; Srivastava, Nikhil (2018). A Matrix Expander Chernoff Bound. STOC '18 Proceedings of the fifty annual ACM symposium on Theory of Computing. arXiv:1704.03864.

^ Kyng, Rasmus; Song, Zhao (2018). A Matrix Chernoff Bound for Strongly Rayleigh Distributions and Spectral Sparsifiers from a few Random Spanning Trees. FOCS '18 IEEE Symposium on Foundations of Computer Science. arXiv:1810.08345.

^ Goldberg, A. V.; Hartline, J. D. (2001). "Competitive Auctions for Multiple Digital Goods". Algorithms â ESA 2001. Lecture Notes in Computer Science. Vol.Â 2161. p.Â 416. CiteSeerXÂ 10.1.1.8.5115. doi:10.1007/3-540-44676-1_35. ISBNÂ 978-3-540-42493-2.; lemma 6.1

^ See graphs of: the bound as a function of r when k changes and the bound as a function of k when r changes.


Further reading[edit]
Chernoff, H. (1952). "A Measure of Asymptotic Efficiency for Tests of a Hypothesis Based on the sum of Observations". Annals of Mathematical Statistics. 23 (4): 493â507. doi:10.1214/aoms/1177729330. JSTORÂ 2236576. MRÂ 0057518. ZblÂ 0048.11804.
Chernoff, H. (1981). "A Note on an Inequality Involving the Normal Distribution". Annals of Probability. 9 (3): 533â535. doi:10.1214/aop/1176994428. JSTORÂ 2243541. MRÂ 0614640. ZblÂ 0457.60014.
Hagerup, T.; RÃ¼b, C. (1990). "A guided tour of Chernoff bounds". Information Processing Letters. 33 (6): 305. doi:10.1016/0020-0190(90)90214-I.
Nielsen, F. (2011). "An Information-Geometric Characterization of Chernoff Information". IEEE Signal Processing Letters. 20 (3): 269â272. arXiv:1102.2684. doi:10.1109/LSP.2013.2243726. S2CIDÂ 15034953.




<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=Chernoff_bound&oldid=1069586031"
		Categories: Probabilistic inequalitiesHidden categories: Articles with short descriptionPages with lower-case short descriptionShort description matches WikidataAll articles with unsourced statementsArticles with unsourced statements from November 2021
	
