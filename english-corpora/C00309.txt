
Title:
Amdahl's law
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Formula inÂ computer architecture
  The theoretical speedup of the latency of the execution of a program as a function of the number of processors executing it, according to Amdahl's law. The speedup is limited by the serial part of the program. For example, if 95% of the program can be parallelized, the theoretical maximum speedup using parallel computing would be 20 times.
In computer architecture, Amdahl's law (or Amdahl's argument[1]) is a formula which gives the theoretical speedup in latency of the execution of a task at fixed workload that can be expected of a system whose resources are improved. Specifically, it states that "the overall performance improvement gained by optimizing a single part of a system is limited by the fraction of time that the improved part is actually used".[2] It is named after computer scientist Gene Amdahl, and was presented at the AFIPS Spring Joint Computer Conference in 1967.
Amdahl's law is often used in parallel computing to predict the theoretical speedup when using multiple processors. For example, if a program needs 20 hours to complete using a single thread, but a one-hour portion of the program cannot be parallelized, therefore only the remaining 19 hours (p = 0.95) of execution time can be parallelized, then regardless of how many threads are devoted to a parallelized execution of this program, the minimum execution time cannot be less than one hour. Hence, the theoretical speedup is limited to at most 20 times the single thread performance, 
  
    
      
        
          (
          
            
              
                
                  1
                  
                    1
                    â
                    p
                  
                
              
            
            =
            20
          
          )
        
      
    
    {\displaystyle \left({\dfrac {1}{1-p}}=20\right)}
  
.

Contents

1 Definition
2 Derivation

2.1 Parallel programs
2.2 Serial programs
2.3 Optimizing the sequential part of parallel programs
2.4 Transforming sequential parts of parallel programs into parallelizable


3 Relation to the law of diminishing returns
4 See also
5 References
6 Further reading
7 External links



Definition[edit]
Amdahl's law can be formulated in the following way:[3]


  
    
      
        
          S
          
            latency
          
        
        (
        s
        )
        =
        
          
            1
            
              (
              1
              â
              p
              )
              +
              
                
                  p
                  s
                
              
            
          
        
      
    
    {\displaystyle S_{\text{latency}}(s)={\frac {1}{(1-p)+{\frac {p}{s}}}}}
  

where

Slatency is the theoretical speedup of the execution of the whole task;
s is the speedup of the part of the task that benefits from improved system resources;
p is the proportion of execution time that the part benefiting from improved resources originally occupied.
Furthermore,


  
    
      
        
          
            {
            
              
                
                  
                    S
                    
                      latency
                    
                  
                  (
                  s
                  )
                  â¤
                  
                    
                      
                        1
                        
                          1
                          â
                          p
                        
                      
                    
                  
                
              
              
                
                  
                    lim
                    
                      s
                      â
                      â
                    
                  
                  
                    S
                    
                      latency
                    
                  
                  (
                  s
                  )
                  =
                  
                    
                      
                        1
                        
                          1
                          â
                          p
                        
                      
                    
                  
                  .
                
              
            
            
          
        
      
    
    {\displaystyle {\begin{cases}S_{\text{latency}}(s)\leq {\dfrac {1}{1-p}}\\[8pt]\lim \limits _{s\to \infty }S_{\text{latency}}(s)={\dfrac {1}{1-p}}.\end{cases}}}
  

shows that the theoretical speedup of the execution of the whole task increases with the improvement of the resources of the system and that regardless of the magnitude of the improvement, the theoretical speedup is always limited by the part of the task that cannot benefit from the improvement.
Amdahl's law applies only to the cases where the problem size is fixed. In practice, as more computing resources become available, they tend to get used on larger problems (larger datasets), and the time spent in the parallelizable part often grows much faster than the inherently serial work. In this case, Gustafson's law gives a less pessimistic and more realistic assessment of the parallel performance.[4]

Derivation[edit]
A task executed by a system whose resources are improved compared to an initial similar system can be split up into two parts:

a part that does not benefit from the improvement of the resources of the system;
a part that benefits from the improvement of the resources of the system.
An example is a computer program that processes files . A part of that program may scan the directory of the disk and create a list of files internally in memory. After that, another part of the program passes each file to a separate thread for processing. The part that scans the directory and creates the file list cannot be sped up on a parallel computer, but the part that processes the files can.
The execution time of the whole task before the improvement of the resources of the system is denoted as 
  
    
      
        T
      
    
    {\displaystyle T}
  
. It includes the execution time of the part that would not benefit from the improvement of the resources and the execution time of the one that would benefit from it. The fraction of the execution time of the task that would benefit from the improvement of the resources is denoted by 
  
    
      
        p
      
    
    {\displaystyle p}
  
. The one concerning the part that would not benefit from it is therefore 
  
    
      
        1
        â
        p
      
    
    {\displaystyle 1-p}
  
. Then:


  
    
      
        T
        =
        (
        1
        â
        p
        )
        T
        +
        p
        T
        .
      
    
    {\displaystyle T=(1-p)T+pT.}
  

It is the execution of the part that benefits from the improvement of the resources that is accelerated by the factor 
  
    
      
        s
      
    
    {\displaystyle s}
  
 after the improvement of the resources. Consequently, the execution time of the part that does not benefit from it remains the same, while the part that benefits from it becomes:


  
    
      
        
          
            p
            s
          
        
        T
        .
      
    
    {\displaystyle {\frac {p}{s}}T.}
  

The theoretical execution time 
  
    
      
        T
        (
        s
        )
      
    
    {\displaystyle T(s)}
  
 of the whole task after the improvement of the resources is then:


  
    
      
        T
        (
        s
        )
        =
        (
        1
        â
        p
        )
        T
        +
        
          
            p
            s
          
        
        T
        .
      
    
    {\displaystyle T(s)=(1-p)T+{\frac {p}{s}}T.}
  

Amdahl's law gives the theoretical speedup in latency of the execution of the whole task at fixed workload 
  
    
      
        W
      
    
    {\displaystyle W}
  
, which yields


  
    
      
        
          S
          
            latency
          
        
        (
        s
        )
        =
        
          
            
              T
              W
            
            
              T
              (
              s
              )
              W
            
          
        
        =
        
          
            T
            
              T
              (
              s
              )
            
          
        
        =
        
          
            1
            
              1
              â
              p
              +
              
                
                  p
                  s
                
              
            
          
        
        .
      
    
    {\displaystyle S_{\text{latency}}(s)={\frac {TW}{T(s)W}}={\frac {T}{T(s)}}={\frac {1}{1-p+{\frac {p}{s}}}}.}
  

Parallel programs[edit]
If 30% of the execution time may be the subject of a speedup, p will be 0.3; if the improvement makes the affected part twice as fast, s will beÂ 2. Amdahl's law states that the overall speedup of applying the improvement will be:


  
    
      
        
          S
          
            latency
          
        
        =
        
          
            1
            
              1
              â
              p
              +
              
                
                  p
                  s
                
              
            
          
        
        =
        
          
            1
            
              1
              â
              0.3
              +
              
                
                  0.3
                  2
                
              
            
          
        
        =
        1.18.
      
    
    {\displaystyle S_{\text{latency}}={\frac {1}{1-p+{\frac {p}{s}}}}={\frac {1}{1-0.3+{\frac {0.3}{2}}}}=1.18.}
  

For example, assume that we are given a serial task which is split into four consecutive parts, whose percentages of execution time are p1 = 0.11, p2 = 0.18, p3 = 0.23, and p4 = 0.48 respectively. Then we are told that the 1st part is not sped up, so s1 = 1, while the 2nd part is sped up 5 times, so s2 = 5, the 3rd part is sped up 20 times, so s3 = 20, and the 4th part is sped up 1.6 times, so s4 = 1.6. By using Amdahl's law, the overall speedup is


  
    
      
        
          S
          
            latency
          
        
        =
        
          
            1
            
              
                
                  
                    p
                    1
                  
                  
                    s
                    1
                  
                
              
              +
              
                
                  
                    p
                    2
                  
                  
                    s
                    2
                  
                
              
              +
              
                
                  
                    p
                    3
                  
                  
                    s
                    3
                  
                
              
              +
              
                
                  
                    p
                    4
                  
                  
                    s
                    4
                  
                
              
            
          
        
        =
        
          
            1
            
              
                
                  0.11
                  1
                
              
              +
              
                
                  0.18
                  5
                
              
              +
              
                
                  0.23
                  20
                
              
              +
              
                
                  0.48
                  1.6
                
              
            
          
        
        =
        2.19.
      
    
    {\displaystyle S_{\text{latency}}={\frac {1}{{\frac {p1}{s1}}+{\frac {p2}{s2}}+{\frac {p3}{s3}}+{\frac {p4}{s4}}}}={\frac {1}{{\frac {0.11}{1}}+{\frac {0.18}{5}}+{\frac {0.23}{20}}+{\frac {0.48}{1.6}}}}=2.19.}
  

Notice how the 5 times and 20 times speedup on the 2nd and 3rd parts respectively don't have much effect on the overall speedup when the 4th part (48% of the execution time) is accelerated by only 1.6 times.

Serial programs[edit]
  Assume that a task has two independent parts, A and B. Part B takes roughly 25% of the time of the whole computation. By working very hard, one may be able to make this part 5 times faster, but this reduces the time of the whole computation only slightly. In contrast, one may need to perform less work to make part A perform twice as fast. This will make the computation much faster than by optimizing part B, even though part B's speedup is greater in terms of the ratio, (5 times versus 2 times).
For example, with a serial program in two parts A and B for which TA = 3 s and TB = 1 s,

if part B is made to run 5 times faster, that is s = 5 and p = TB/(TA + TB) = 0.25, then

  
    
      
        
          S
          
            latency
          
        
        =
        
          
            1
            
              1
              â
              0.25
              +
              
                
                  0.25
                  5
                
              
            
          
        
        =
        1.25
        ;
      
    
    {\displaystyle S_{\text{latency}}={\frac {1}{1-0.25+{\frac {0.25}{5}}}}=1.25;}
  

if part A is made to run 2 times faster, that is s = 2 and p = TA/(TA + TB) = 0.75, then

  
    
      
        
          S
          
            latency
          
        
        =
        
          
            1
            
              1
              â
              0.75
              +
              
                
                  0.75
                  2
                
              
            
          
        
        =
        1.60.
      
    
    {\displaystyle S_{\text{latency}}={\frac {1}{1-0.75+{\frac {0.75}{2}}}}=1.60.}
  

Therefore, making part A to run 2 times faster is better than making part B to run 5 times faster. The percentage improvement in speed can be calculated as


  
    
      
        
          percentage improvement
        
        =
        100
        
          (
          
            1
            â
            
              
                1
                
                  S
                  
                    latency
                  
                
              
            
          
          )
        
        .
      
    
    {\displaystyle {\text{percentage improvement}}=100\left(1-{\frac {1}{S_{\text{latency}}}}\right).}
  

Improving part A by a factor of 2 will increase overall program speed by a factor of 1.60, which makes it 37.5% faster than the original computation.
However, improving part B by a factor of 5, which presumably requires more effort, will achieve an overall speedup factor of 1.25 only, which makes it 20% faster.
Optimizing the sequential part of parallel programs[edit]
If the non-parallelizable part is optimized by a factor of 
  
    
      
        O
      
    
    {\displaystyle O}
  
, then


  
    
      
        T
        (
        O
        ,
        s
        )
        =
        (
        1
        â
        p
        )
        
          
            T
            O
          
        
        +
        
          
            p
            s
          
        
        T
        .
      
    
    {\displaystyle T(O,s)=(1-p){\frac {T}{O}}+{\frac {p}{s}}T.}
  

It follows from Amdahl's law that the speedup due to parallelism is given by


  
    
      
        
          S
          
            latency
          
        
        (
        O
        ,
        s
        )
        =
        
          
            
              T
              (
              O
              )
            
            
              T
              (
              O
              ,
              s
              )
            
          
        
        =
        
          
            
              (
              1
              â
              p
              )
              
                
                  1
                  O
                
              
              +
              
                p
              
            
            
              
                
                  
                    1
                    â
                    p
                  
                  O
                
              
              +
              
                
                  p
                  s
                
              
            
          
        
        .
      
    
    {\displaystyle S_{\text{latency}}(O,s)={\frac {T(O)}{T(O,s)}}={\frac {(1-p){\frac {1}{O}}+{p}}{{\frac {1-p}{O}}+{\frac {p}{s}}}}.}
  

When 
  
    
      
        s
        =
        1
      
    
    {\displaystyle s=1}
  
, we have 
  
    
      
        
          S
          
            latency
          
        
        (
        O
        ,
        s
        )
        =
        1
      
    
    {\displaystyle S_{\text{latency}}(O,s)=1}
  
, meaning that the speedup is
measured with respect to the execution time after the non-parallelizable part is optimized.
When  
  
    
      
        s
        =
        â
      
    
    {\displaystyle s=\infty }
  
, 


  
    
      
        
          S
          
            latency
          
        
        (
        O
        ,
        â
        )
        =
        
          
            
              T
              (
              O
              )
            
            
              T
              (
              O
              ,
              s
              )
            
          
        
        =
        
          
            
              (
              1
              â
              p
              )
              
                
                  1
                  O
                
              
              +
              
                p
              
            
            
              
                
                  
                    1
                    â
                    p
                  
                  O
                
              
              +
              
                
                  p
                  s
                
              
            
          
        
        =
        1
        +
        
          
            p
            
              1
              â
              p
            
          
        
        O
        .
      
    
    {\displaystyle S_{\text{latency}}(O,\infty )={\frac {T(O)}{T(O,s)}}={\frac {(1-p){\frac {1}{O}}+{p}}{{\frac {1-p}{O}}+{\frac {p}{s}}}}=1+{\frac {p}{1-p}}O.}
  

If  
  
    
      
        1
        â
        p
        =
        0.4
      
    
    {\displaystyle 1-p=0.4}
  
, 
  
    
      
        O
        =
        2
      
    
    {\displaystyle O=2}
  
 and 
  
    
      
        s
        =
        5
      
    
    {\displaystyle s=5}
  
, then:


  
    
      
        
          S
          
            latency
          
        
        (
        O
        ,
        s
        )
        =
        
          
            
              T
              (
              O
              )
            
            
              T
              (
              O
              ,
              s
              )
            
          
        
        =
        
          
            
              
                0.4
              
              
                
                  1
                  2
                
              
              +
              0.6
            
            
              
                
                  0.4
                  2
                
              
              +
              
                
                  0.6
                  5
                
              
            
          
        
        =
        2.5.
      
    
    {\displaystyle S_{\text{latency}}(O,s)={\frac {T(O)}{T(O,s)}}={\frac {{0.4}{\frac {1}{2}}+0.6}{{\frac {0.4}{2}}+{\frac {0.6}{5}}}}=2.5.}
  

Transforming sequential parts of parallel programs into parallelizable[edit]
Next, we consider the case wherein the non-parallelizable part is reduced by a factor of 
  
    
      
        
          O
          â²
        
      
    
    {\displaystyle O'}
  
, and the parallelizable part is correspondingly increased. Then


  
    
      
        
          T
          â²
        
        (
        
          O
          â²
        
        ,
        s
        )
        =
        
          
            
              1
              â
              p
            
            
              O
              â²
            
          
        
        T
        +
        
          (
          
            1
            â
            
              
                
                  1
                  â
                  p
                
                
                  O
                  â²
                
              
            
          
          )
        
        
          
            T
            s
          
        
        .
      
    
    {\displaystyle T'(O',s)={\frac {1-p}{O'}}T+\left(1-{\frac {1-p}{O'}}\right){\frac {T}{s}}.}
  

It follows from Amdahl's law that the speedup due to parallelism is given by


  
    
      
        
          S
          
            latency
          
          â²
        
        (
        
          O
          â²
        
        ,
        s
        )
        =
        
          
            
              
                T
                â²
              
              (
              
                O
                â²
              
              )
            
            
              
                T
                â²
              
              (
              
                O
                â²
              
              ,
              s
              )
            
          
        
        =
        
          
            1
            
              
                
                  
                    1
                    â
                    p
                  
                  
                    O
                    â²
                  
                
              
              +
              
                (
                
                  1
                  â
                  
                    
                      
                        1
                        â
                        p
                      
                      
                        O
                        â²
                      
                    
                  
                
                )
              
              
                
                  1
                  s
                
              
            
          
        
        .
      
    
    {\displaystyle S'_{\text{latency}}(O',s)={\frac {T'(O')}{T'(O',s)}}={\frac {1}{{\frac {1-p}{O'}}+\left(1-{\frac {1-p}{O'}}\right){\frac {1}{s}}}}.}
  

The derivation above is in agreement with Jakob Jenkov's analysis of the execution time vs. speedup tradeoff.[5]

Relation to the law of diminishing returns[edit]
Amdahl's law is often conflated with the law of diminishing returns, whereas only a special case of applying Amdahl's law demonstrates law of diminishing returns. If one picks optimally (in terms of the achieved speedup) what is to be improved, then one will see monotonically decreasing improvements as one improves. If, however, one picks non-optimally, after improving a sub-optimal component and moving on to improve a more optimal component, one can see an increase in the return. Note that it is often rational to improve a system in an order that is "non-optimal" in this sense, given that some improvements are more difficult or require larger development time than others.
Amdahl's law does represent the law of diminishing returns if on considering what sort of return one gets by adding more processors to a machine, if one is running a fixed-size computation that will use all available processors to their capacity. Each new processor added to the system will add less usable power than the previous one. Each time one doubles the number of processors the speedup ratio will diminish, as the total throughput heads toward the limit of 1/(1Â âÂ p).
This analysis neglects other potential bottlenecks such as memory bandwidth and I/O bandwidth. If these resources do not scale with the number of processors, then merely adding processors provides even lower returns.
An implication of Amdahl's law is that to speedup real applications which have both serial and parallel portions, heterogeneous computing techniques are required.[6]

See also[edit]
Gustafson's law
Analysis of parallel algorithms
Critical path method
Moore's law
References[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Rodgers, David P. (June 1985). "Improvements in multiprocessor system design". ACM SIGARCH Computer Architecture News. New York, NY, USA: ACM. 13 (3): 225â231 [p. 226]. doi:10.1145/327070.327215. ISBNÂ 0-8186-0634-7. ISSNÂ 0163-5964. S2CIDÂ 7083878.

^ "API Design for C++ | ScienceDirect". www.sciencedirect.com. Retrieved 2022-01-27.

^ Bryant, Randal E.; David, O'Hallaron (2016), Computer Systems: A Programmer's Perspective (3Â ed.), Pearson Education, p.Â 58, ISBNÂ 978-1-488-67207-1

^ McCool, Michael; Reinders, James; Robison, Arch (2013). Structured Parallel Programming: Patterns for Efficient Computation. Elsevier. p.Â 61. ISBNÂ 978-0-12-415993-8.

^ "Amdahl's Law".

^ Hill, Mark D.; Marty, Michael R. (2008). "Amdahl's Law in the Multicore Era". Computer. 41 (7): 33â38. doi:10.1109/MC.2008.209.


Further reading[edit]
Amdahl, Gene M. (1967). "Validity of the Single Processor Approach to Achieving Large-Scale Computing Capabilities" (PDF). AFIPS Conference Proceedings (30): 483â485. doi:10.1145/1465482.1465560.
External links[edit]



Wikimedia Commons has media related to Amdahl's law.

"Parallel Programming: When Amdahl's law is inapplicable?". 2011-06-25. Archived from the original on 2013-04-14. Retrieved 2011-06-26.
Gene M. Amdahl (1989), Oral history interview with Gene M. Amdahl, Charles Babbage Institute, University of Minnesota, hdl:11299/104341. Amdahl discusses his graduate work at the University of Wisconsin and his design of WISC. Discusses his role in the design of several computers for IBM including the STRETCH, IBM 701, and IBM 704. He discusses his work with Nathaniel Rochester and IBM's management of the design process. Mentions work with Ramo-Wooldridge, Aeronutronic, and Computer Sciences Corporation
Amdahl's Law: Not all performance improvements are created equal (2007)
"Amdahl's Law" by Joel F. Klein, Wolfram Demonstrations Project (2007)
Amdahl's Law in the Multicore Era (July 2008)
What the $#@! is Parallelism, Anyhow? (Charles Leiserson, May 2008)
Evaluation of the Intel Core i7 Turbo Boost feature, by James Charles, Preet Jassi, Ananth Narayan S, Abbas Sadat and Alexandra Fedorova (2009)
Calculation of the acceleration of parallel programs as a function of the number of threads, by George Popov, Valeri Mladenov and Nikos Mastorakis (January 2010)
Danny Hillis - Proving Amdahl's Law wrong, video recorded October 2016
.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}show.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vteComputer laws
Amdahl's law
Bell's law
Grosch's law
Gustafson's law
Haitz's law
Koomey's law
Linus's law
Metcalfe's law
Moore's law
Moore's second law
Pollack's rule
Wirth's law
* Semantically, computer laws are not hard and fast laws, but rules of thumbs or postulations
showvteParallel computingGeneral
Distributed computing
Parallel computing
Massively parallel
Cloud computing
High-performance computing
Multiprocessing
Manycore processor
GPGPU
Computer network
Systolic array
Levels
Bit
Instruction
Thread
Task
Data
Memory
Loop
Pipeline
Multithreading
Temporal
Simultaneous (SMT)
Speculative (SpMT)
Preemptive
Cooperative
Clustered multi-thread (CMT)
Hardware scout
Theory
PRAM model
PEM model
Analysis of parallel algorithms
Amdahl's law
Gustafson's law
Cost efficiency
KarpâFlatt metric
Slowdown
Speedup
Elements
Process
Thread
Fiber
Instruction window
Array data structure
Coordination
Multiprocessing
Memory coherency
Cache coherency
Cache invalidation
Barrier
Synchronization
Application checkpointing
Programming
Stream processing
Dataflow programming
Models
Implicit parallelism
Explicit parallelism
Concurrency
Non-blocking algorithm
Hardware
Flynn's taxonomy
SISD
SIMD
Array processing (SIMT)
Pipelined processing
Associative processing
MISD
MIMD
Dataflow architecture
Pipelined processor
Superscalar processor
Vector processor
Multiprocessor
symmetric
asymmetric
Memory
shared
distributed
distributed shared
UMA
NUMA
COMA
Massively parallel computer
Computer cluster
Grid computer
Hardware acceleration
APIs
Ateji PX
Boost
Chapel
HPX
Charm++
Cilk
Coarray Fortran
CUDA
Dryad
C++ AMP
Global Arrays
GPUOpen
MPI
OpenMP
OpenCL
OpenHMPP
OpenACC
Parallel Extensions
PVM
POSIX Threads
RaftLib
ROCm
UPC
TBB
ZPL
Problems
Automatic parallelization
Deadlock
Deterministic algorithm
Embarrassingly parallel
Parallel slowdown
Race condition
Software lockout
Scalability
Starvation

Â Category: Parallel computing





<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=Amdahl%27s_law&oldid=1069448921"
		Categories: Analysis of parallel algorithmsComputer architecture statementsHidden categories: Articles with short descriptionShort description is different from WikidataCommons category link from Wikidata
	
