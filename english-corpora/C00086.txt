
Title:
OpenAI
Text:

		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		Artificial intelligence research laboratory
.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}Not to be confused with OpenAL.
Coordinates: .mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}37Â°45â²44â³N 122Â°24â²53â³Wï»¿ / ï»¿37.7623Â°N 122.4148Â°Wï»¿ / 37.7623; -122.4148

.mw-parser-output .infobox-subbox{padding:0;border:none;margin:-3px;width:auto;min-width:100%;font-size:100%;clear:none;float:none;background-color:transparent}.mw-parser-output .infobox-3cols-child{margin:auto}.mw-parser-output .infobox .navbar{font-size:100%}body.skin-minerva .mw-parser-output .infobox-header,body.skin-minerva .mw-parser-output .infobox-subheader,body.skin-minerva .mw-parser-output .infobox-above,body.skin-minerva .mw-parser-output .infobox-title,body.skin-minerva .mw-parser-output .infobox-image,body.skin-minerva .mw-parser-output .infobox-full-data,body.skin-minerva .mw-parser-output .infobox-below{text-align:center}OpenAIIndustryArtificial intelligenceFoundedDecemberÂ 11, 2015; 6 years agoÂ (2015-12-11)FoundersElon MuskSam AltmanIlya SutskeverGreg BrockmanWojciech ZarembaJohn SchulmanHeadquartersPioneer Building, San Francisco, California, US[1][2]Key peopleIlya SutskeverGreg BrockmanSam AltmanProductsDALL-E, GPT-3, GPT-2, OpenAI GymNumber of employees>120 (as of 2020[update])[2]Websiteopenai.com
.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}

This article is part of a series aboutElon Musk

Awards and honors
Views

Companies
Zip2
X.com
PayPal
SpaceX
Tesla, Inc.
Tesla Energy
OpenAI
Neuralink
The Boring Company

In popular culture
Elon Musk
Ludicrous
Power Play
"Members Only"
"The Platonic Permutation"
"The Musk Who Fell to Earth"
"One Crew over the Crewcoo's Morty"

Related
Boring Test Tunnel
Criticism of Tesla
Maye Musk
Hyperloop
Tesla Roadster in space
TSLAQ

.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vte
OpenAI is an artificial intelligence (AI) research laboratory consisting of the for-profit corporation OpenAI LP and its parent company, the non-profit OpenAI Inc. The company, considered a competitor to DeepMind, conducts research in the field of AI with the stated goal of promoting and developing friendly AI in a way that benefits humanity as a whole. The organization was founded in San Francisco in late 2015 by Elon Musk, Sam Altman, and others, who collectively pledged US$1 billion. Musk resigned from the board in February 2018 but remained a donor. In 2019, OpenAI LP received a US$1 billion investment from Microsoft.

Contents

1 History

1.1 Participants


2 Motives
3 Strategy
4 Products and applications

4.1 Gym
4.2 RoboSumo
4.3 Debate Game
4.4 Dactyl
4.5 Generative models

4.5.1 GPT
4.5.2 GPT-2
4.5.3 GPT-3
4.5.4 Music
4.5.5 API
4.5.6 DALL-E and CLIP
4.5.7 Microscope
4.5.8 Codex


4.6 Video game bots and benchmarks

4.6.1 OpenAI Five
4.6.2 GYM Retro




5 See also
6 Notes
7 References
8 External links



History[edit]
  The Pioneer Building in San Francisco, housing the offices of OpenAI and Neuralink
In December 2015, Elon Musk, Sam Altman, and other investors announced the formation of OpenAI and pledged over US$1 billion to the venture. The organization stated they would "freely collaborate" with other institutions and researchers by making its patents and research open to the public.[3][4]
On April 27, 2016, OpenAI released a public beta of "OpenAI Gym", its platform for reinforcement learning research.[5]
On December 5, 2016, OpenAI released "Universe", a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites and other applications.[6][7][8][9]
On February 21, 2018, Musk resigned his board seat, citing "a potential future conflict (of interest)" with Tesla AI development for self driving cars, but remained a donor.[10]
In 2019, OpenAI transitioned from non-profit to for-profit. The company distributed equity to its employees[11] and partnered with Microsoft Corporation, who announced an investment package of US$1 billion into the company. OpenAI then announced its intention to commercially license its technologies, with Microsoft as its preferred partner.[12]
As of 2020, OpenAI is headquartered in San Francisco's Mission District, and shares the former Pioneer Trunk Factory building with Neuralink, another company co-founded by Musk.[13][2]
In June 2020, OpenAI announced GPT-3, a language model trained on trillions of words from the Internet. It also announced that an associated API, named simply "the API", would form the heart of its first commercial product. GPT-3 is aimed at natural language answering of questions, but it can also translate between languages and coherently generate improvised text.[14]

Participants[edit]
CEO:[15] Sam Altman, former president of the startup accelerator Y Combinator
Ilya Sutskever, Research director, a former Google expert on machine learning[16]
CTO:[17] Greg Brockman, former CTO, 3rd employee of Stripe[16]
Other backers of the project include:[16]

Reid Hoffman, LinkedIn co-founder[18]
Peter Thiel, PayPal co-founder[18]
Jessica Livingston, a founding partner of Y Combinator
Companies:

Infosys, one of the Indian IT firms[19]
Microsoft's cloud services division[20]
The group started in early January 2016 with nine researchers. According to Wired, Brockman met with Yoshua Bengio, one of the "founding fathers" of the deep learning movement, and drew up a list of the "best researchers in the field". Microsoft's Peter Lee stated that the cost of a top AI researcher exceeds the cost of a top NFL quarterback prospect. While OpenAI pays corporate-level (rather than nonprofit-level) salaries, it doesn't currently pay AI researchers salaries comparable to those of Facebook or Google. Nevertheless, Sutskever stated that he was willing to leave Google for OpenAI "partly of because of the very strong group of people and, to a very large extent, because of its mission." Brockman stated that "the best thing that I could imagine doing was moving humanity closer to building real AI in a safe way." OpenAI researcher Wojciech Zaremba stated that he turned down "borderline crazy" offers of two to three times his market value to join OpenAI instead.[21]

Motives[edit]
Some scientists, such as Stephen Hawking and Stuart Russell, have articulated concerns that if advanced AI someday gains the ability to re-design itself at an ever-increasing rate, an unstoppable "intelligence explosion" could lead to human extinction. Musk characterizes AI as humanity's "biggest existential threat."[22] OpenAI's founders structured it as a non-profit so that they could focus its research on creating a positive long-term human impact.[4]
Musk and Altman have stated they are motivated in part by concerns about the existential risk from artificial general intelligence.[23][21] OpenAI states that "it's hard to fathom how much human-level AI could benefit society," and that it is equally difficult to comprehend "how much it could damage society if built or used incorrectly".[4] Research on safety cannot safely be postponed: "because of AI's surprising history, it's hard to predict when human-level AI might come within reach."[24] OpenAI states that AI "should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible...",[4] and which sentiment has been expressed elsewhere in reference to a potentially enormous class of AI-enabled products: "Are we really willing to let our society be infiltrated by autonomous software and hardware agents whose details of operation are known only to a select few? Of course not."[25] Co-chair Sam Altman expects the decades-long project to surpass human intelligence.[26]
Vishal Sikka, former CEO of Infosys, stated that an "openness" where the endeavor would "produce results generally in the greater interest of humanity" was a fundamental requirement for his support, and that OpenAI "aligns very nicely with our long-held values" and their "endeavor to do purposeful work".[27] Cade Metz of Wired suggests that corporations such as Amazon may be motivated by a desire to use open-source software and data to level the playing field against corporations such as Google and Facebook that own enormous supplies of proprietary data. Altman states that Y Combinator companies will share their data with OpenAI.[26]
In 2019, OpenAI became a for profit company called OpenAI LP to secure additional funding while staying controlled by a non-profit called OpenAI Inc in a structure that OpenAI calls "capped-profit",[28] having previously been a 501(c)(3) nonprofit organization.[29][30]

Strategy[edit]
Musk posed the question: "what is the best thing we can do to ensure the future is good? We could sit on the sidelines or we can encourage regulatory oversight, or we could participate with the right structure with people who care deeply about developing AI in a way that is safe and is beneficial to humanity." Musk acknowledged that "there is always some risk that in actually trying to advance (friendly) AI we may create the thing we are concerned about"; nonetheless, the best defense is "to empower as many people as possible to have AI. If everyone has AI powers, then there's not any one person or a small set of individuals who can have AI superpower."[16]
Musk and Altman's counter-intuitive strategy of trying to reduce the risk that AI will cause overall harm, by giving AI to everyone, is controversial among those who are concerned with existential risk from artificial intelligence. Philosopher Nick Bostrom is skeptical of Musk's approach: "If you have a button that could do bad things to the world, you don't want to give it to everyone."[21] During a 2016 conversation about the technological singularity, Altman said that "we don't plan to release all of our source code" and mentioned a plan to "allow wide swaths of the world to elect representatives to a new governance board". Greg Brockman stated that "Our goal right now... is to do the best thing there is to do. It's a little vague."[31]
Conversely, OpenAI's initial decision to withhold GPT-2 due to a wish to "err on the side of caution" in the presence of potential misuse, has been criticized by advocates of openness. Delip Rao, an expert in text generation, stated "I don't think [OpenAI] spent enough time proving [GPT-2] was actually dangerous." Other critics argued that open publication is necessary to replicate the research and to be able to come up with countermeasures.[32]
In the 2017 tax year, OpenAI spent US$7.9 million, or a quarter of its functional expenses, on cloud computing alone.[33] In comparison, DeepMind's total expenses in 2017 were much larger, measuring US$442 million. In Summer 2018, simply training OpenAI's Dota 2 bots required renting 128,000 CPUs and 256 GPUs from Google for multiple weeks. According to OpenAI, the capped-profit model adopted in March 2019 allows OpenAI LP to legally attract investment from venture funds, and in addition, to grant employees stakes in the company, the goal being that they can say "I'm going to Open AI, but in the long term it's not going to be disadvantageous to us as a family."[34] Many top researchers work for Google Brain, DeepMind, or Facebook, Inc., which offer stock options that a nonprofit would be unable to.[35] In June 2019, OpenAI LP raised a billion dollars from Microsoft, a sum which OpenAI plans to have spent "within five years, and possibly much faster".[36] Altman has stated that even a billion dollars may turn out to be insufficient, and that the lab may ultimately need "more capital than any non-profit has ever raised" to achieve Artificial general intelligence .[37]
The transition from a nonprofit to a capped-profit company was viewed with skepticism by Oren Etzioni of the nonprofit Allen Institute for AI, who agreed that wooing top researchers to a nonprofit is difficult, but stated "I disagree with the notion that a nonprofit can't compete" and pointed to successful low-budget projects by OpenAI and others. "If bigger and better funded was always better, then IBM would still be number one." Following the transition, public disclosure of the compensation of top employees at OpenAI LP is no longer legally required. The nonprofit, OpenAI Inc., is the sole controlling shareholder of OpenAI LP. OpenAI LP, despite being a for-profit company, retains a formal fiduciary responsibility to OpenAI's Inc.'s nonprofit charter. A majority of OpenAI Inc.'s board is barred from having financial stakes in OpenAI LP.[34] In addition, minority members with a stake in OpenAI LP are barred from certain votes due to conflict of interest.[35] Some researchers have argued that OpenAI LP's switch to for-profit status is inconsistent with OpenAI's claims to be "democratizing" AI.[38] A journalist in Vice News wrote that "generally, we've never been able to rely on venture capitalists to better humanity".[39]

Products and applications[edit]
OpenAI's research tend to focus on reinforcement learning. OpenAI is viewed as an important competitor to DeepMind.[40]

Gym[edit]
Gym aims to provide an easy to set up, general-intelligence benchmark with a wide variety of different environmentsâsomewhat akin to, but broader than, the ImageNet Large Scale Visual Recognition Challenge used in supervised learning researchâand that hopes to standardize the way in which environments are defined in AI research publications, so that published research becomes more easily reproducible.[5][41] The project claims to provide the user with a simple interface. As of JuneÂ 2017, Gym can only be used with Python.[42] As of September 2017, the Gym documentation site was not maintained, and active work focused instead on its GitHub page.[43]

RoboSumo[edit]
In "RoboSumo", virtual humanoid "metalearning" robots initially lack knowledge of how to even walk, and given the goals of learning to move around, and pushing the opposing agent out of the ring. Through this adversarial learning process, the agents learn how to adapt to changing conditions; when an agent is then removed from this virtual environment and placed in a new virtual environment with high winds, the agent braces to remain upright, suggesting it had learned how to balance in a generalized way.[44][45] OpenAI's Igor Mordatch argues that competition between agents can create an intelligence "arms race" that can increase an agent's ability to function, even outside the context of the competition.

Debate Game[edit]
In 2018, OpenAI launched the Debate Game, which teaches machines to debate toy problems in front of a human judge. The purpose is to research whether such an approach may assist in auditing AI decisions and in developing explainable AI.[46][47]

Dactyl[edit]
Dactyl uses machine learning to train a robot Shadow Hand from scratch, using the same reinforcement learning algorithm code that OpenAI Five uses. The robot hand is trained entirely in physically inaccurate simulation.[48][49]

Generative models[edit]
GPT[edit]
The original paper on generative pre-training (GPT) of a language model was written by Alec Radford and colleagues, and published in preprint on OpenAI's website on June 11, 2018.[50] It showed how a generative model of language is able to acquire world knowledge and process long-range dependencies by pre-training on a diverse corpus with long stretches of contiguous text.

GPT-2[edit]
Main article: GPT-2
  An instance of GPT-2 writing a paragraph based on a prompt from its own Wikipedia article in February 2021.
Generative Pre-trained Transformer 2, commonly known by its abbreviated form GPT-2, is an unsupervised transformer language model and the successor to GPT. GPT-2 was first announced in February 2019, with only limited demonstrative versions initially released to the public. The full version of GPT-2 was not immediately released out of concern over potential misuse, including applications for writing fake news.[51] Some experts expressed skepticism that GPT-2 posed a significant threat. The Allen Institute for Artificial Intelligence responded to GPT-2 with a tool to detect "neural fake news".[52] Other researchers, such as Jeremy Howard, warned of "the technology to totally fill Twitter, email, and the web up with reasonable-sounding, context-appropriate prose, which would drown out all other speech and be impossible to filter".[53] In November 2019, OpenAI released the complete version of the GPT-2 language model.[54] Several websites host interactive demonstrations of different instances of GPT-2 and other transformer models.[55][56][57]
GPT-2's authors argue unsupervised language models to be general-purpose learners, illustrated by GPT-2 achieving state-of-the-art accuracy and perplexity on 7 of 8 zero-shot tasks (i.e. the model was not further trained on any task-specific input-output examples). The corpus it was trained on, called WebText, contains slightly over 8 million documents for a total of 40 GB of text from URLs shared in Reddit submissions with at least 3 upvotes. It avoids certain issues encoding vocabulary with word tokens by using byte pair encoding. This allows to represent any string of characters by encoding both individual characters and multiple-character tokens.[58]

GPT-3[edit]
Main article: GPT-3
Generative Pre-trained[a] Transformer 3, commonly known by its abbreviated form GPT-3, is an unsupervised Transformer language model and the successor to GPT-2.  It was first described in May 2020.[60][61][62]  OpenAI stated that full version of GPT-3 contains 175 billion parameters,[62] two orders of magnitude larger than the 1.5 billion parameters[63] in the full version of GPT-2 (although GPT-3 models with as few as 125 million parameters were also trained).[64] 
OpenAI stated that GPT-3 succeeds at certain "meta-learning" tasks. It can generalize the purpose of a single input-output pair. The paper gives an example of translation and cross-linguistic transfer learning between English and Romanian, and between English and German.[62] 
GPT-3 dramatically improved benchmark results  over GPT-2. OpenAI cautioned that such scaling up of language models could be approaching or encountering the fundamental capability limitations of predictive language models.[65] Pre-training GPT-3 required several thousand petaflop/s-days[b] of compute, compared to tens of petaflop/s-days for the full GPT-2 model.[62] Like that of its predecessor,[51] GPT-3's fully trained model was not immediately released to the public on the grounds of possible abuse, though OpenAI planned to allow access through a paid cloud API after a two-month free private beta that began in June 2020.[67][68]
On September 23, 2020, GPT-3 was licensed exclusively to Microsoft.[69][70]

Music[edit]
OpenAI's MuseNet (2019) is a deep neural net trained to predict subsequent musical notes in MIDI music files. It can generate songs with ten different instruments in fifteen different styles. According to The Verge, a song generated by MuseNet tends to start out reasonably but then fall into chaos the longer it plays.[71][72]
OpenAI's Jukebox (2020) is an open-sourced algorithm to generate music with vocals. After training on 1.2 million samples, the system accepts a genre, artist, and a snippet of lyrics, and outputs song samples. OpenAI stated the songs "show local musical coherence, follow traditional chord patterns" but acknowledged that the songs lack "familiar larger musical structures such as choruses that repeat" and that "there is a significant gap" between Jukebox and human-generated music. The Verge stated "It's technologically impressive, even if the results sound like mushy versions of songs that might feel familiar", while Business Insider stated "surprisingly, some of the resulting songs are catchy and sound legitimate".[73][74][75]

API[edit]
In June 2020, OpenAI announced a multi-purpose API which it said was "for accessing new AI models developed by OpenAI" to let developers call on it for "any English language AI task."[67][76]

DALL-E and CLIP[edit]
Main article: DALL-E
  Images produced by DALL-E when given the text prompt "a professional high quality illustration of a giraffe dragon chimera. a giraffe imitating a dragon. a giraffe made of dragon."
DALL-E is a Transformer model that creates images from textual descriptions, revealed by OpenAI in January 2021.[77] 
CLIP does the opposite: it creates a description for a given image.[78] DALL-E uses a 12-billion-parameter version of GPT-3 to interpret natural language inputs (such as "a green leather purse shaped like a pentagon" or "an isometric view of a sad capybara") and generate corresponding images. It is able to create images of realistic objects ("a stained glass window with an image of a blue strawberry") as well as objects that do not exist in reality ("a cube with the texture of a porcupine"). As of March 2021, no API or code is available.
In March 2021 OpenAI released a paper, titled Multimodal Neurons in Artificial Neural Networks,[79] where they showed detailed analysis of CLIP (and  GPT) models and their vulnerabilities. The new type of attacks on such models was described in this work.

.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}We refer to these attacks as typographic attacks. We believe attacks such as those described above are far from simply an academic concern. By exploiting the modelâs ability to read text robustly, we find that even photographs of hand-written text can often fool the model.ââMultimodal Neurons in Artificial Neural Networks, OpenAI
Microscope[edit]
OpenAI Microscope[80] is a collection of visualizations of every significant layer and neuron of eight different neural network models which are often studied in interpretability. Microscope was created for easy analysis of the features that form inside these neural networks.
The models included are AlexNet, VGG 19, different versions of Inception, and different versions of CLIP Resnet.[81]

Codex[edit]
Main article: OpenAI Codex
OpenAI Codex is a descendant of GPT-3 that has additionally been trained on code from 54 million GitHub repositories.[82][83] It was announced in mid-2021 as the AI powering the code autocompletion tool GitHub Copilot.[83] In August 2021, an API was released in private beta.[84] According to OpenAI, the model is able to create working code in over a dozen programming languages, most effectively in Python.[82] 
Several issues with glitches, design flaws, and security vulnerabilities have been brought up.[85][86]

Video game bots and benchmarks[edit]
OpenAI Five[edit]
Main article: OpenAI Five
OpenAI Five is the name of a team of five OpenAI-curated bots that are used in the competitive five-on-five video game Dota 2, who learn to play against human players at a high skill level entirely through trial-and-error algorithms. Before becoming a team of five, the first public demonstration occurred at The International 2017, the annual premiere championship tournament for the game, where Dendi, a professional Ukrainian player, lost against a bot in a live 1v1 matchup.[87][88] After the match, CTO Greg Brockman explained that the bot had learned by playing against itself for two weeks of real time, and that the learning software was a step in the direction of creating software that can handle complex tasks like a surgeon.[89][90] The system uses a form of reinforcement learning, as the bots learn over time by playing against themselves hundreds of times a day for months, and are rewarded for actions such as killing an enemy and taking map objectives.[91][92][93]
By June 2018, the ability of the bots expanded to play together as a full team of five and they were able to defeat teams of amateur and semi-professional players.[94][95][96][97] At The International 2018, OpenAI Five played in two exhibition matches against professional players, but ended up losing both games.[98][99][100] In April 2019, OpenAI Five defeated OG, the reigning world champions of the game at the time, 2:0 in a live exhibition match in San Francisco.[101][102] The bots' final public appearance came later that month, where they played in 42,729 total games in a four-day open online competition, winning 99.4% of those games.[103]

GYM Retro[edit]
Gym Retro is a platform for reinforcement learning research on games. Gym Retro is used to conduct research on RL algorithms and study generalization. Prior research in RL has mostly focused on optimizing agents to solve single tasks. Gym Retro gives the ability to generalize between games with similar concepts but different appearances.

See also[edit]
.mw-parser-output .portalbox{float:right;border:solid #aaa 1px;padding:0}.mw-parser-output .portalbox.tleft{margin:0.5em 1em 0.5em 0}.mw-parser-output .portalbox.tright{margin:0.5em 0 0.5em 1em}.mw-parser-output .portalbox>ul{display:table;box-sizing:border-box;padding:0.1em;max-width:175px;background:#f9f9f9;font-size:85%;line-height:110%;font-style:italic;font-weight:bold}.mw-parser-output .portalbox>ul>li{display:table-row}.mw-parser-output .portalbox>ul>li>span:first-child{display:table-cell;padding:0.2em;vertical-align:middle;text-align:center}.mw-parser-output .portalbox>ul>li>span:last-child{display:table-cell;padding:0.2em 0.2em 0.2em 0.3em;vertical-align:middle}

Companies portal
California portal
.mw-parser-output .div-col{margin-top:0.3em;column-width:30em}.mw-parser-output .div-col-small{font-size:90%}.mw-parser-output .div-col-rules{column-rule:1px solid #aaa}.mw-parser-output .div-col dl,.mw-parser-output .div-col ol,.mw-parser-output .div-col ul{margin-top:0}.mw-parser-output .div-col li,.mw-parser-output .div-col dd{page-break-inside:avoid;break-inside:avoid-column}
Future of Humanity Institute
Future of Life Institute
Machine Intelligence Research Institute
OpenCog
Open Neural Network Exchange
Open-source robotics
Partnership on AI
Vicarious (company)

Notes[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ The term "pre-training" refers to general language training as distinct from fine-tuning for specific tasks.[59]

^ One  petaflop/s-day is approximately equal to 1020 neural net operations.[66]


References[edit]


^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}Markoff, John (December 11, 2015). "Artificial-Intelligence Research Center Is Founded by Silicon Valley Investors". The New York Times. Retrieved December 12, 2015.

^ Jump up to: a b c Hao, Karen (2020-02-17). "The messy, secretive reality behind OpenAI's bid to save the world". MIT Technology Review. Retrieved 2020-03-09.

^ "Introducing OpenAI". OpenAI Blog. 12 December 2015.

^ Jump up to: a b c d "Tech giants pledge $1bn for 'altruistic AI' venture, OpenAI". BBC News. 12 December 2015. Retrieved 19 December 2015.

^ Jump up to: a b Dave Gershgorn (27 April 2016). "Elon Musk's Artificial Intelligence Group Opens A 'Gym' To Train A.I." Popular Science. Retrieved 29 April 2016.

^ Metz, Cade. "Elon Musk's Lab Wants to Teach Computers to Use Apps Just Like Humans Do". WIRED. Retrieved 31 December 2016.

^ Mannes, John. "OpenAI's Universe is the fun parent every artificial intelligence deserves". TechCrunch. Retrieved 31 December 2016.

^ "OpenAI - Universe". Retrieved 31 December 2016.

^ Claburn, Thomas. "Elon Musk-backed OpenAI reveals Universe â a universal training ground for computers". The Register. Retrieved 31 December 2016.

^ Vincent, James (February 21, 2018). "Elon Musk leaves board of AI safety group to avoid conflict of interest with Tesla". The Verge.

^ "OpenAI LP". OpenAI. 2019-03-11. Retrieved 2020-02-21.

^ "Microsoft Invests In and Partners with OpenAI to Support Us Building Beneficial AGI". OpenAI. 2019-07-22. Retrieved 2020-02-21.

^ Conger, Kate. "Elon Musk's Neuralink Sought to Open an Animal Testing Facility in San Francisco". Gizmodo. Retrieved 2018-10-11.

^ Vance, Ashlee (2020-06-11). "Trillions of Words Analyzed, OpenAI Sets Loose AI Language Colossus". Bloomberg News.

^ Bass, Dina (22 July 2019). "Microsoft to invest $1 billion in OpenAI". Los Angeles Times. Retrieved 22 July 2019.

^ Jump up to: a b c d "Silicon Valley investors to bankroll artificial-intelligence center". The Seattle Times. 13 December 2015. Retrieved 19 December 2015.

^ Etherington, Darrell (July 22, 2019). "Microsoft invests $1 billion in OpenAI in new multiyear partnership". TechCrunch. Retrieved July 22, 2019.

^ Jump up to: a b Liedtke, Michael. "Elon Musk, Peter Thiel, Reid Hoffman, others back $1 billion OpenAI research center". San Jose Mercury News. Retrieved 19 December 2015.

^ "Elon Musk, Infosys, others back OpenAI with $1 bn". Business Standard India. Business Standard. IANS. 12 December 2015. Retrieved 30 August 2019.

^ Vincent, James (22 July 2019). "Microsoft invests $1 billion in OpenAI to pursue holy grail of artificial intelligence". The Verge. Retrieved 23 July 2019.

^ Jump up to: a b c Cade Metz (27 April 2016). "Inside OpenAI, Elon Musk's Wild Plan to Set Artificial Intelligence Free". Wired magazine. Retrieved 28 April 2016.

^ Piper, Kelsey (2 November 2018). "Why Elon Musk fears artificial intelligence". Vox. Retrieved 10 March 2021.

^ Lewontin, Max (14 December 2015). "Open AI: Effort to democratize artificial intelligence research?". The Christian Science Monitor. Retrieved 19 December 2015.

^ Mendoza, Jessica. "Tech leaders launch nonprofit to save the world from killer robots". The Christian Science Monitor.

^ Glenn W. Smith (10 April 2018). "Re: Sex-BotsâLet Us Look before We Leap". Arts. 7 (2): 15. doi:10.3390/arts7020015.

^ Jump up to: a b Metz, Cade (15 December 2015). "Elon Musk's Billion-Dollar AI Plan Is About Far More Than Saving the World". Wired. Retrieved 19 December 2015. Altman said they expect this decades-long project to surpass human intelligence.

^ Vishal Sikka (14 December 2015). "OpenAI: AI for All". InfyTalk. Infosys. Archived from the original on 22 December 2015. Retrieved 22 December 2015.

^ "OpenAI shifts from nonprofit to 'capped-profit' to attract capital". TechCrunch. Retrieved 2019-05-10.

^ Levy, Steven (December 11, 2015). "How Elon Musk and Y Combinator Plan to Stop Computers From Taking Over". Medium/Backchannel. Retrieved December 11, 2015. Elon Musk: ...we came to the conclusion that having a 501(c)(3)... would probably be a good thing to do

^ Brockman, Greg (April 3, 2017). "Yes, we're a 501(c)(3). As you mention in /r/ControlProblem, we will file our 990 later this year as required. Not yet sure of exact date".

^ "Sam Altman's Manifest Destiny". The New Yorker. No.Â 10 October 2016. Retrieved 4 October 2016.

^ Vincent, James (21 February 2019). "AI researchers debate the ethics of sharing potentially harmful programs". The Verge. Retrieved 6 March 2020.

^ "Microsoft to invest $1 billion in OpenAI". Reuters. 22 July 2019. Retrieved 6 March 2020.

^ Jump up to: a b "To Compete With Google, OpenAI Seeks Investorsâand Profits". Wired. 3 December 2019. Retrieved 6 March 2020.

^ Jump up to: a b Kahn, Jeremy (11 March 2019). "AI Research Group Co-Founded by Elon Musk Starts For-Profit Arm". Bloomberg News. Retrieved 6 March 2020.

^ Murgia, Madhumita (7 August 2019). "DeepMind runs up higher losses and debts in race for AI". Financial Times. Retrieved 6 March 2020.

^ "OpenAI Will Need More Capital Than Any Non-Profit Has Ever Raised". Fortune. Retrieved 6 March 2020.

^ Vincent, James (22 July 2019). "Microsoft invests $1 billion in OpenAI to pursue holy grail of artificial intelligence". The Verge. Retrieved 6 March 2020.

^ Haskins, Caroline (12 March 2019). "OpenAI's Mission to Benefit Humanity Now Includes Seeking Profit". Vice News. Retrieved 6 March 2020.

^ Lee, Dave (15 October 2019). "Robot solves Rubik's cube, but not grand challenge". BBC News. Retrieved 29 February 2020.

^ Greg Brockman; John Schulman (27 April 2016). "OpenAI Gym Beta". OpenAI Blog. OpenAI. Retrieved 29 April 2016.

^ "OpenAI Gym". GitHub. Retrieved 8 May 2017.

^ Brockman, Greg (12 Sep 2017). "Yep, the Github repo has been the focus of the project for the past year. The Gym site looks cool but hasn't been maintained". @gdb. Retrieved 2017-11-07.

^ "AI Sumo Wrestlers Could Make Future Robots More Nimble". Wired. 11 October 2017. Retrieved 2 November 2017.

^ "OpenAI's Goofy Sumo-Wrestling Bots Are Smarter Than They Look". MIT Technology Review. Retrieved 2 November 2017.

^ Greene, Tristan (2018-05-04). "OpenAI's Debate Game teaches you and your friends how to lie like robots". The Next Web. Retrieved 2018-05-31.

^ "Why Scientists Think AI Systems Should Debate Each Other". Fast Company. 8 May 2018. Retrieved 2 June 2018.

^ "Learning Dexterity". Openai.com. OpenAI. Retrieved 27 August 2018.

^ Ryan, Mae (2018). "How Robot Hands Are Evolving to Do What Ours Can". Retrieved 1 September 2018.

^ "Improving Language Understanding by Generative Pre-Training" (PDF). Retrieved June 9, 2020.

^ Jump up to: a b Hern, Alex (14 February 2019). "New AI fake text generator may be too dangerous to release, say creators". The Guardian. Retrieved 14 February 2019.

^ Schwartz, Oscar (4 July 2019). "Could 'fake text' be the next global political threat?". The Guardian. Retrieved 16 July 2019.

^ Vincent, James (14 February 2019). "OpenAI's new multitalented AI writes, translates, and slanders". The Verge. Retrieved 16 July 2019.

^ "GPT-2: 1.5B Release". OpenAI. 2019-11-05. Retrieved 2019-11-14.

^ "Write With Transformer". Retrieved December 4, 2019.

^ "Talk to Transformer". Retrieved December 4, 2019.

^ "CreativeEngines". Retrieved June 25, 2021.

^ "Language Models are Unsupervised Multitask Learners" (PDF). Retrieved December 4, 2019. {{cite journal}}: Cite journal requires |journal= (help)

^ Ganesh, Prakhar (December 17, 2019). "Pre-trained Language Models: Simplified". Retrieved September 9, 2020. The intuition behind pre-trained language models is to create a black box which understands the language and can then be asked to do any specific task in that language.

^ "openai/gpt-3". OpenAI. 2020-05-29. Retrieved 2020-05-29.

^ Sagar, Ram (2020-06-03). "OpenAI Releases GPT-3, The Largest Model So Far". Analytics India Magazine. Retrieved 2020-06-14.

^ Jump up to: a b c d Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini (2020-06-01). "Language Models are Few-Shot Learners". p.Â appendix. arXiv:2005.14165 [cs.CL].

^ "Language Models are Unsupervised Multitask Learners" (PDF). Retrieved December 4, 2019. GPT-2, is a 1.5B parameter Transformer {{cite journal}}: Cite journal requires |journal= (help)

^ Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini (2020-06-01). "Language Models are Few-Shot Learners". arXiv:2005.14165 [cs.CL]. Since we increase the capacity by over two orders of magnitude from GPT-2 to GPT-3

^ Ray, Tiernan (2020). "OpenAI's gigantic GPT-3 hints at the limits of language models for AI". ZDNet. Retrieved 5 June 2020.

^ Amodei, Dario; Hernandez, Danny (May 16, 2018). "AI and Compute". A petaflop/s-day (pfs-day) consists of performing 1015 neural net operations per second for one day, or a total of about 1020 operations. The compute-time product serves as a mental convenience, similar to kW-hr for energy.

^ Jump up to: a b "OpenAI API". OpenAI. 2020-06-11. Retrieved 2020-06-14. Why did OpenAI choose to release an API instead of open-sourcing the models?There are three main reasons we did this. First, commercializing the technology helps us pay for our ongoing AI research, safety, and policy efforts. Second, many of the models underlying the API are very large, taking a lot of expertise to develop and deploy and making them very expensive to run. This makes it hard for anyone except larger companies to benefit from the underlying technology. Weâre hopeful that the API will make powerful AI systems more accessible to smaller businesses and organizations. Third, the API model allows us to more easily respond to misuse of the technology. Since it is hard to predict the downstream use cases of our models, it feels inherently safer to release them via an API and broaden access over time, rather than release an open source model where access cannot be adjusted if it turns out to have harmful applications.

^ Eadicicco, Lisa. "The artificial intelligence company that Elon Musk helped found is now selling the text-generation software it previously said was too dangerous to launch". Business Insider. Retrieved 2020-07-06.

^ "OpenAI is giving Microsoft exclusive access to its GPT-3 language model". MIT Technology Review. Retrieved 2020-09-24.

^ "Microsoft gets exclusive license for OpenAI's GPT-3 language model". VentureBeat. 2020-09-22. Retrieved 2020-09-24.

^ "OpenAI's MuseNet generates AI music at the push of a button". The Verge. April 2019. Retrieved 8 June 2020.

^ "MuseNet". OpenAI. 25 April 2019. Retrieved 8 June 2020.

^ "OpenAI introduces Jukebox, a new AI model that generates genre-specific music". The Verge. 30 April 2020. Retrieved 8 June 2020.

^ Stephen, Bijan (30 April 2020). "OpenAI introduces Jukebox, a new AI model that generates genre-specific music". Business Insider. Retrieved 8 June 2020.

^ "Jukebox". OpenAI. 30 April 2020. Retrieved 8 June 2020.

^ "TechCrunch â Startup and Technology News". TechCrunch. Retrieved 11 June 2020. If youâve ever wanted to try out OpenAIâs vaunted machine learning toolset, it just got a lot easier. The company has released an API that lets developers call its AI tools in on âvirtually any English language task.â

^ "DALLÂ·E: Creating Images from Text". 5 January 2021.

^ "CLIP: Connecting Text and Images". 5 January 2021.

^ "Multimodal Neurons in Artificial Neural Networks". 4 March 2021.

^ "OpenAI Microscope". 14 April 2020.

^ "The OpenAI Microscope: Models".

^ Jump up to: a b Alford, Anthony (August 31, 2021). "OpenAI Announces 12 Billion Parameter Code-Generation AI Codex". InfoQ. Retrieved 2021-09-03.

^ Jump up to: a b Wiggers, Kyle (July 8, 2021). "OpenAI warns AI behind GitHub's Copilot may be susceptible to bias". VentureBeat. Retrieved 2021-09-03.

^ Zaremba, Wojciech (August 10, 2021). "OpenAI Codex". OpenAI. Retrieved 2021-09-03.{{cite web}}:  CS1 maint: url-status (link)

^ Dickson, Ben (August 16, 2021). "What to expect from OpenAI's Codex API". VentureBeat. Retrieved 2021-09-03.

^ Claburn, Thomas (August 25, 2021). "GitHub's Copilot may steer you into dangerous waters about 40% of the time â study". The Register. Retrieved 2021-09-03.

^ Savov, Vlad (14 August 2017). "My favorite game has been invaded by killer AI bots and Elon Musk hype". The Verge. Retrieved June 25, 2018.

^ Frank, Blair Hanley. "OpenAI's bot beats top Dota 2 player so badly that he quits". Venture Beat. Archived from the original on August 12, 2017. Retrieved August 12, 2017.

^ "Dota 2". blog.openai.com. 11 August 2017. Retrieved 12 August 2017.

^ "More on Dota 2". blog.openai.com. 16 August 2017. Retrieved 16 August 2017.

^ Simonite, Tom. "Can Bots Outwit Humans in One of the Biggest Esports Games?". Wired. Retrieved June 25, 2018.

^ Kahn, Jeremy. "A Bot Backed by Elon Musk Has Made an AI Breakthrough in Video Game World". Bloomberg. Retrieved June 27, 2018.

^ Clifford, Catherine (28 June 2018). "Bill Gates says gamer bots from Elon Musk-backed nonprofit are 'huge milestone' in A.I." CNBC. Retrieved June 29, 2018.

^ "OpenAI Five Benchmark". blog.openai.com. 18 July 2018. Retrieved 25 August 2018.

^ Simonite, Tom. "Can Bots Outwit Humans in One of the Biggest Esports Games?". Wired. Retrieved 25 June 2018.

^ Vincent, James (25 June 2018). "AI bots trained for 180 years a day to beat humans at Dota 2". The Verge. Retrieved 25 June 2018.

^ Savov, Vlad (6 August 2018). "The OpenAI Dota 2 bots just defeated a team of former pros". The Verge. Retrieved August 7, 2018.

^ Simonite, Tom. "Pro Gamers Fend off Elon Musk-Backed AI Botsâfor Now". Wired. Retrieved 25 August 2018.

^ Quach, Katyanna. "Game over, machines: Humans defeat OpenAI bots once again at video games Olympics". The Register. Retrieved 25 August 2018.

^ "The International 2018: Results". blog.openai.com. 24 August 2018. Retrieved 25 August 2018.

^ Statt, Nick (13 April 2019). "OpenAI's Dota 2 AI steamrolls world champion e-sports team with back-to-back victories". The Verge. Retrieved 20 July 2019.

^ "How to Train Your OpenAI Five". OpenAI Blog. 15 April 2019. Retrieved 20 July 2019.

^ Wiggers, Kyle (22 April 2019). "OpenAI's Dota 2 bot defeated 99.4% of players in public matches". Venture Beat. Retrieved 22 April 2019.


External links[edit]
Official website 
.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}vteDifferentiable computingGeneral
Differentiable programming
Neural Turing machine
Differentiable neural computer
Automatic differentiation
Neuromorphic engineering
Cable theory
Pattern recognition
Computational learning theory
Tensor calculus
Concepts
Gradient descent
SGD
Clustering
Regression
Overfitting
Adversary
Attention
Convolution
Loss functions
Backpropagation
Normalization
Activation
Softmax
Sigmoid
Rectifier
Regularization
Datasets
Augmentation
Programming languages
Python
Julia
Application
Machine learning
Artificial neural network
Deep learning
Scientific computing
Artificial Intelligence
Hardware
IPU
TPU
VPU
Memristor
SpiNNaker
Software library
TensorFlow
PyTorch
Keras
Theano
ImplementationAudio-visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
Speech recognition
Facial recognition
AlphaFold
DALL-E
Verbal
Word2vec
Transformer
BERT
NMT
Project Debater
Watson
GPT-2
GPT-3
Decisional
AlphaGo
AlphaZero
Q-learning
SARSA
OpenAI Five
Self-driving car
MuZero
Action selection
Robot control
People
Alex Graves
Ian Goodfellow
Yoshua Bengio
Geoffrey Hinton
Yann LeCun
Andrew Ng
Demis Hassabis
David Silver
Fei-Fei Li
Organizations
DeepMind
OpenAI
MIT CSAIL
Mila
Google Brain
FAIR

 Portals
Computer programming
Technology
 Category
Artificial neural networks
Machine learning

vteExistential risk from artificial intelligenceConcepts
Accelerating change
AI box
AI takeover
Control problem
Existential risk from artificial general intelligence
Friendly artificial intelligence
Instrumental convergence
Intelligence explosion
Machine ethics
Superintelligence
Technological singularity
Organizations
Allen Institute for AI
Center for Applied Rationality
Center for Human-Compatible Artificial Intelligence
Centre for the Study of Existential Risk
DeepMind
Foundational Questions Institute
Future of Humanity Institute
Future of Life Institute
Humanity+
Institute for Ethics and Emerging Technologies
Leverhulme Centre for the Future of Intelligence
Machine Intelligence Research Institute
OpenAI
People
Scott Alexander
Nick Bostrom
Eric Drexler
Sam Harris
Stephen Hawking
Bill Hibbard
Bill Joy
Elon Musk
Steve Omohundro
Huw Price
Martin Rees
Stuart J. Russell
Jaan Tallinn
Max Tegmark
Frank Wilczek
Roman Yampolskiy
Andrew Yang
Eliezer Yudkowsky
Other
Artificial intelligence as a global catastrophic risk
Controversies and dangers of artificial general intelligence
Ethics of artificial intelligence
Suffering risks
Human Compatible
Open Letter on Artificial Intelligence
Our Final Invention
The Precipice
Superintelligence: Paths, Dangers, Strategies
Do You Trust This Computer?
 Category
vteElon MuskCompanies
Zip2 (1995â1999)
X.com (later PayPal, 1999â2000)
SpaceX (2002âpresent)
Tesla, Inc. (2004âpresent)
Tesla Energy (previously SolarCity, 2006âpresent)
OpenAI (2015âpresent)
Neuralink (2016âpresent)
The Boring Company (2016âpresent)
Related topics
Hyperloop
Tesla Roadster in space
Boring Test Tunnel
Views
Awards and honors
Criticism of Tesla
TSLAQ
Family
Maye Musk (mother)
Justine Musk (first wife)
Talulah Riley (second wife)
Grimes (former partner)
Kimbal Musk (brother)
Tosca Musk (sister)
Lyndon Rive (cousin)
In culture
Elon Musk: Tesla, SpaceX, and the Quest for a Fantastic Future (2015 biography)
"The Musk Who Fell to Earth" (The Simpsons, 2015)
Ludicrous: The Unvarnished Story of Tesla Motors (2019)
"One Crew over the Crewcoo's Morty" (Rick and Morty, 2019)
Power Play: Tesla, Elon Musk, and the Bet of the Century (2021)
 Categories
Authority control General
Integrated Authority File (Germany)
VIAF
1
WorldCat
Other
SUDOC (France)
1





<img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" />
Retrieved from "https://en.wikipedia.org/w/index.php?title=OpenAI&oldid=1068813255"
		Categories: 2015 establishments in California2015 in San Francisco501(c)(3) organizationsAmerican companies established in 2015Artificial intelligence associationsArtificial intelligence laboratoriesElon MuskExistential risk from artificial general intelligenceExistential risk organizationsNon-profit organizations based in San FranciscoOpen-source artificial intelligenceResearch institutes in the United StatesHidden categories: CS1 errors: missing periodicalCS1 maint: url-statusArticles with short descriptionShort description is different from WikidataCoordinates on WikidataArticles containing potentially dated statements from 2020All articles containing potentially dated statementsPages using div col with small parameterArticles with GND identifiersArticles with VIAF identifiersArticles with WORLDCATID identifiersArticles with SUDOC identifiers
	
