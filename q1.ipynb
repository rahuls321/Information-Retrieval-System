{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import unidecode\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('english-corpora/C00001.txt') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "1. Text is splitted by '\\t' first.\n",
    "2. Remove extra spaces\n",
    "3. Tokenize the strings\n",
    "4. Remove Punctuations from tokenize words\n",
    "5. Remove Number\n",
    "6. Remove Double quotations from tokens\n",
    "7. Replace URL with url tag\n",
    "8. Remove ascents from string using decode like A&deg;\n",
    "9. Split camelCase word into 'camel' and 'Case'\n",
    "10. Remove number from token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isExtraSpace(text):\n",
    "    return len(text)==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isASCII(token):\n",
    "    if token in ['.','+','*','?','[','/', '//','\\\\','^','%',']', '$','(',')','{','}','=', '!', '|',':','-']:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_special_token(token):\n",
    "    if token in ['^^^^', '==']:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_change(token):\n",
    "    if token in ['Objective-J', 'I/O']:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(token):\n",
    "    num_regex = re.compile('^[+-]?[0-9]+\\.?[0-9]*$')\n",
    "    return bool(num_regex.match(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_url(token):\n",
    "    replaced_text = re.sub('(http[s]?://)?((www)\\.)?([a-zA-Z0-9]+)\\.{1}((com)(\\.(cn))?|(org))', '<url>', token)\n",
    "    return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_camelCase(token):\n",
    "    return re.findall(r'[a-zA-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers_from_token(token):\n",
    "    return re.sub(r'[0-9]+', '', token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmer(stemmer_type):\n",
    "    if(stemmer_type=='porter_stemmer'): stemmer = nltk.PorterStemmer()\n",
    "    elif(stemmer_type=='snowball_stemmer'): stemmer = nltk.SnowballStemmer(language = 'english')\n",
    "    return stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(tokens):\n",
    "    new_tokens=[]\n",
    "    for token in tokens:\n",
    "        token=token.replace('\"', '')\n",
    "        token = replace_url(token)\n",
    "        token = unidecode.unidecode(token)\n",
    "        if isASCII(token) or is_number(token) or len(token)==0: continue\n",
    "        if no_change(token):\n",
    "            new_tokens.append(token)\n",
    "            continue\n",
    "        tt = re.split(', |:|\\+|%|\\|/|\\*|$|&|@|_|-|!|;|,', token)\n",
    "        for t in tt:\n",
    "            t=t.replace('\"', '')\n",
    "            t=unidecode.unidecode(t)\n",
    "            #Removing ASCII character and extra space after splitting\n",
    "            if isASCII(t) or len(t)<=1 or is_number(t): continue\n",
    "            #Removing '.' from the word\n",
    "            split_by_dot = t.split('.')\n",
    "            final_token = sorted(split_by_dot, key=len, reverse=True)[0]\n",
    "            #If still some non-ascii char left in the token \n",
    "            if not final_token.isalnum():\n",
    "                final_token = re.sub('[^A-Za-z0-9]+', '', final_token)\n",
    "            #Remove number from token\n",
    "            final_token = remove_numbers_from_token(final_token)\n",
    "            #CamelCase condition based on first char if it is lower or not\n",
    "            if final_token and final_token[0].islower():\n",
    "                camel_tokens = split_camelCase(final_token)\n",
    "                for tt in camel_tokens: \n",
    "                    if len(tt) <= 1: continue\n",
    "                    new_tokens.append(tt.lower())\n",
    "            else: \n",
    "                if len(final_token) <= 1: continue\n",
    "                new_tokens.append(final_token.lower())\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 P_386.txt\n",
      "1 T00921.txt\n",
      "2 D00585.txt\n",
      "3 L00119.txt\n",
      "4 T00755.txt\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "file_path='english-corpora/'\n",
    "vocab_doc_wise_tokenization={}\n",
    "vocab_doc_wise_stemming={}\n",
    "i=0\n",
    "for file in os.listdir(file_path):\n",
    "    print(i, end=' ')\n",
    "    print(file)\n",
    "    with codecs.open(os.path.join(file_path, file), mode='r', encoding='utf-8') as input_file:\n",
    "        next(input_file)\n",
    "        d=[]\n",
    "        for line in input_file:\n",
    "            text = line.strip().split('\\t')[0]\n",
    "            if(isExtraSpace(text)): continue\n",
    "            text=text.replace('\"', '')\n",
    "            ####################################### TOKENIZATION ################################################\n",
    "            tokens = tokenize(text)\n",
    "            pun_free_token = text_cleaning(tokens)\n",
    "            if pun_free_token:\n",
    "#                 print(\"Final: \", pun_free_token)\n",
    "                d = sum([], d+pun_free_token)\n",
    "#     d = sorted(set(d), key=lambda x:d.index(x))\n",
    "    doc_name = os.path.splitext(file)[0]\n",
    "    vocab_doc_wise_tokenization[doc_name] = d\n",
    "    ####################################### STEMMING ################################################\n",
    "    stemmer = get_stemmer('snowball_stemmer')\n",
    "    st = [stemmer.stem(word) for word in d]\n",
    "    vocab_doc_wise_stemming[doc_name] = st\n",
    "    i+=1\n",
    "    if i>=5: break;\n",
    "np.save('vocab_doc_wise_tokenization.npy', vocab_doc_wise_tokenization) \n",
    "np.save('vocab_doc_wise_stemming.npy', vocab_doc_wise_stemming) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
