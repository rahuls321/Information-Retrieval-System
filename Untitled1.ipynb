{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading preprocessed document wise vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_doc_wise_tokenization = np.load('vocab_doc_wise_tokenization.npy', allow_pickle='TRUE').item()\n",
    "vocab_doc_wise_stemming = np.load('vocab_doc_wise_stemming.npy', allow_pickle='TRUE').item()\n",
    "# print(vocab_doc_wise_tokenization)\n",
    "# print(vocab_doc_wise_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Node which has three sub-nodes containing document ID, freq of word in that docID \n",
    "#and next to link with next docID\n",
    "class Node:\n",
    "    def __init__(self, docID, freq=None):\n",
    "        self.docID = docID\n",
    "        self.freq = freq\n",
    "        self.next = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating word freq for each doc\n",
    "def get_word_freq(vocab):\n",
    "    word_freq={}\n",
    "    for word in vocab:\n",
    "        if word in word_freq.keys():\n",
    "            word_freq[word]+=1\n",
    "        else: word_freq[word]=1\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Postings list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_list = {}\n",
    "doc_index = {}\n",
    "ind=0\n",
    "doc_lengths={}\n",
    "for doc_id, vocab in vocab_doc_wise_stemming.items():\n",
    "    word_freq = get_word_freq(vocab)\n",
    "    for word, freq in word_freq.items():\n",
    "        if word in postings_list.keys():\n",
    "            firstNode = postings_list[word]\n",
    "            while firstNode.next is not None:\n",
    "                firstNode = firstNode.next\n",
    "            firstNode.next = Node(ind, freq)\n",
    "        else:\n",
    "            postings_list[word] = Node(ind, freq)\n",
    "    doc_index[ind] = doc_id\n",
    "    doc_lengths[ind] = len(vocab)\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 623, 1: 4948, 2: 13006, 3: 4788, 4: 2307}\n"
     ]
    }
   ],
   "source": [
    "print(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word, node in postings_list.items():\n",
    "#     print(word, end='->')\n",
    "#     while node is not None:\n",
    "#         print(node.docID, end='->')\n",
    "#         print(node.freq, end=' ')\n",
    "#         node=node.next\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps followed\n",
    "1. Tokenize the query\n",
    "2. Convert infix query expression to postfix query expression using stack approach\n",
    "        a. Check if the given expression is balanced or not\n",
    "        b. Check is there any extra parenthesis in the expression\n",
    "3. Processing two operator only in the query **\\&**(and) , **\\|** (or) and **\\~**(negation) and giving higeher precedence to the former\n",
    "4. Using **snowball_stemmer** as a stemmer algorithm to find the stem word in the given query\n",
    "5. Generate binary vector based on document size and consider negation sign as well while processing\n",
    "6. Find document which contains the query word using **find_matched_doc** function and return a binary vector that shows which document contains that word\n",
    "7. Remove stop words from query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_operator(token):\n",
    "    if token in ['&' , '|']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Precedence of operators\n",
    "def precedence_oper(token):\n",
    "    if token=='&': return 2\n",
    "    elif token=='|': return 1\n",
    "    else: return -1\n",
    "\n",
    "def get_postfix_list(tokens):\n",
    "    stack = []\n",
    "    postfix_list = []\n",
    "    for token in tokens:\n",
    "        #If token is left small bracket '('\n",
    "        if token == '(': stack.append(token)\n",
    "        elif token == ')':\n",
    "            while(len(stack)>0 and stack[-1]!='('):\n",
    "                postfix_list.append(stack.pop())\n",
    "            if len(stack)==0 and token==')':\n",
    "                raise ValueError('Either unnecessary parenthesis or Not a balanced query')\n",
    "            stack.pop()\n",
    "            if len(stack)>0 and stack[-1] == '(':\n",
    "                raise ValueError('Either unnecessary parenthesis or Not a balanced query')\n",
    "        elif is_operator(token):\n",
    "            while(len(stack)>0 and precedence_oper(token) <= precedence_oper(stack[-1])):\n",
    "                postfix_list.append(stack.pop())\n",
    "            stack.append(token)\n",
    "        else: \n",
    "            postfix_list.append(token)\n",
    "    while len(stack)>0:\n",
    "        postfix_list.append(stack.pop())\n",
    "    return postfix_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_preprocessing(q):\n",
    "    #Remove stop words from query\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #Tokenize query first\n",
    "    q_tokens = word_tokenize(q)\n",
    "    new_q_tokens = [word for word in q_tokens if word not in stop_words]\n",
    "    #Convert this infix list into postfix list to process operator in right way\n",
    "    q_tokens = get_postfix_list(new_q_tokens)\n",
    "    return q_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmer(stemmer_type):\n",
    "    if(stemmer_type=='porter_stemmer'): stemmer = nltk.PorterStemmer()\n",
    "    elif(stemmer_type=='snowball_stemmer'): stemmer = nltk.SnowballStemmer(language = 'english')\n",
    "    return stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_vec(token, postings_list, doc_size):\n",
    "    word_embedd = np.zeros(doc_size, dtype=int)\n",
    "    vocab = postings_list.keys()\n",
    "    negation = False\n",
    "    if token[0]=='~':\n",
    "        negation=True\n",
    "        token=token[1:]\n",
    "    if token not in vocab:\n",
    "        print(\"'\"+token + \"' was not found in the corpus\")\n",
    "        return word_embedd\n",
    "    node = postings_list[token]\n",
    "    while node is not None:\n",
    "        word_embedd[node.docID] = 1\n",
    "        node=node.next\n",
    "    if negation:\n",
    "        word_embedd = np.invert(word_embedd)\n",
    "    return word_embedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matched_doc(query_tokens, postings_list, doc_index, top_k):\n",
    "    \n",
    "    word_embedd_stack = []\n",
    "    doc_size = len(doc_index)\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        if is_operator(token):\n",
    "            if(len(word_embedd_stack)<2): \n",
    "                raise ValueError(\"Query is not correct or use more stopping words\")\n",
    "            first_operand = word_embedd_stack.pop()\n",
    "            second_operand = word_embedd_stack.pop()\n",
    "            \n",
    "            if token=='&': word_embedd_stack.append(first_operand & second_operand)\n",
    "            elif token=='|': word_embedd_stack.append(first_operand | second_operand)\n",
    "            else:\n",
    "                raise ValueError('Can\\'t process this operator: ', token)\n",
    "        else:\n",
    "            stemmer = get_stemmer('snowball_stemmer')\n",
    "            st = stemmer.stem(token)\n",
    "            \n",
    "            token_embedd = get_binary_vec(token, postings_list, doc_size)\n",
    "            word_embedd_stack.append(token_embedd)\n",
    "    matched_doc = [doc_index[docID] for docID in np.where(word_embedd_stack[-1])[0]]\n",
    "    return matched_doc[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents retrieved\n",
      "'technology' was not found in the corpus\n",
      "['P_386', 'D00585']\n",
      "['T00921', 'D00585']\n",
      "Avg time takes to run Boolean Retrieval system for one query: 0.00185 sec\n"
     ]
    }
   ],
   "source": [
    "queries_list = ['person | technology', 'man | indiashow']\n",
    "top_k=5\n",
    "end_time=0\n",
    "print(\"Top {} documents retrieved\".format(top_k))\n",
    "for query in queries_list:\n",
    "    st_time = time.time()\n",
    "    query_tokens = query_preprocessing(query)\n",
    "    matched_doc = find_matched_doc(query_tokens, postings_list, doc_index, top_k)\n",
    "    end_time+=time.time()-st_time\n",
    "    print(matched_doc)\n",
    "print(\"Avg time takes to run Boolean Retrieval system for one query: {:.5f} sec\".format(end_time/len(queries_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['P_386', 'T00921', 'D00585', 'L00119', 'T00755'])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_doc_wise_stemming.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(postings_list.keys()).index('return')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to consider\n",
    "1. Tokenize the query first and remove the stopwords from the query\n",
    "2. Find the query vector where each dimension represents freq. of token present in the query\n",
    "3. For fast query processing, find the tf-idf for those token which are present in the query only\n",
    "4. Return top-k documents only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_tf(doc, query_tokens, postings_list, doc_doc_index):\n",
    "    doc_vector = np.zeros(len(postings_list.keys()), dtype=int)\n",
    "    for token in doc:\n",
    "        if token not in postings_list.keys():\n",
    "            print(\"'\"+token + \"' was not found in corpus\")\n",
    "        elif token not in query_tokens:\n",
    "            continue\n",
    "        else:\n",
    "            node = postings_list[token]\n",
    "            while node is not None:\n",
    "                if node.docID == doc_doc_index: break\n",
    "                node=node.next\n",
    "            idx = list(postings_list.keys()).index(token)\n",
    "            doc_vector[idx]=node.freq\n",
    "    return doc_vector\n",
    "\n",
    "def get_doc_idf(doc, query_tokens, postings_list, doc_size):\n",
    "    inverse_freq={}\n",
    "    for token in doc:\n",
    "        if (token not in postings_list.keys()) or (token not in query_tokens):\n",
    "            inverse_freq[token]=0\n",
    "        else:\n",
    "            node = postings_list[token]\n",
    "            dft=0\n",
    "            while node is not None:\n",
    "                dft+=1\n",
    "                node=node.next\n",
    "            inverse_freq[token] = math.log(doc_size / dft)\n",
    "    return inverse_freq\n",
    "\n",
    "def get_doc_tf_idf(doc_vector, postings_list, inv_doc_freq):\n",
    "    for token, token_freq in inv_doc_freq.items():\n",
    "        idx = list(postings_list.keys()).index(token)\n",
    "        doc_vector[idx] = doc_vector[idx]*token_freq\n",
    "    doc_len = np.linalg.norm(doc_vector)\n",
    "    if doc_len!=0:\n",
    "        doc_vector = doc_vector/doc_len\n",
    "    return doc_vector\n",
    "\n",
    "def get_query_vector(query_tokens, postings_list):\n",
    "    query_vector = np.zeros(len(postings_list.keys()), dtype=int)\n",
    "    for token in query_tokens:\n",
    "        if token not in postings_list.keys():\n",
    "            print(\"'\"+token + \"' was not found in corpus\")\n",
    "        else:\n",
    "            idx = list(postings_list.keys()).index(token)\n",
    "            query_vector[idx] += 1\n",
    "    return query_vector\n",
    "\n",
    "def get_scoring_vec(tf_idf_matrix, doc_size):\n",
    "    score_vector = np.zeros(doc_size, dtype=float)\n",
    "    for _, vec in tf_idf_matrix.items():\n",
    "        score_vector += vec\n",
    "    return score_vector\n",
    "\n",
    "def get_mapped_doc(score_vec, doc_index, top_k):\n",
    "    doc_idx_mapping = np.arange(len(doc_index))\n",
    "    get_matched_doc = [doc_index[docID] for score, docID in sorted(zip(score_vec, doc_idx_mapping), reverse=True)]\n",
    "    return get_matched_doc[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_tf_idf_vector(query_tokens, vocab_doc_wise_stemming, postings_list, doc_index):\n",
    "    doc_tf_idf_vector={}\n",
    "    for doc, doc_vocab in vocab_doc_wise_stemming.items():\n",
    "#         print(doc)\n",
    "        idx = list(doc_index.values()).index(doc)\n",
    "        doc_vector = get_doc_tf(doc_vocab, query_tokens, postings_list, idx)\n",
    "#         print(\"Doc vector: \", doc_vector)\n",
    "        doc_inv_term_freq = get_doc_idf(doc_vocab, query_tokens, postings_list, len(doc_index))\n",
    "#         print(\"Doc inv freq: \", doc_inv_term_freq)\n",
    "        doc_tf_idf = get_doc_tf_idf(doc_vector,postings_list, doc_inv_term_freq)\n",
    "        doc_tf_idf_vector[doc] = doc_tf_idf\n",
    "#         print(\"doc_tf_idf: \", doc_tf_idf)\n",
    "    return doc_tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents retrieved\n",
      "'technology' was not found in corpus\n",
      "'movie' was not found in corpus\n",
      "['P_386', 'D00585', 'T00921', 'T00755', 'L00119']\n",
      "['T00921', 'T00755', 'P_386', 'L00119', 'D00585']\n",
      "Avg time takes to run Boolean Retrieval system for one query: 0.78387 sec\n"
     ]
    }
   ],
   "source": [
    "queries_list = ['person and technology and brahmana but not movie', 'man or indiashow']\n",
    "top_k=5\n",
    "end_time=0\n",
    "print(\"Top {} documents retrieved\".format(top_k))\n",
    "for query in queries_list:\n",
    "    st_time = time.time()\n",
    "    #Tokenize query first\n",
    "    q_tokens = word_tokenize(query)\n",
    "    #Remove stop words from query\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_q_tokens = [word for word in q_tokens if word not in stop_words]\n",
    "    V_q = get_query_vector(new_q_tokens, postings_list)\n",
    "#     print(new_q_tokens)\n",
    "#     print(V_q)\n",
    "    q_sim_score=[]\n",
    "    q_sim_score_map_doc_name=[]\n",
    "    doc_tf_idf_vector = get_docs_tf_idf_vector(new_q_tokens, vocab_doc_wise_stemming, postings_list, doc_index)\n",
    "    for doc, v_d in doc_tf_idf_vector.items():\n",
    "#         print(v_d)\n",
    "        if np.linalg.norm(V_q)==0 or np.linalg.norm(v_d)==0:\n",
    "            cos_sim=0\n",
    "        else: cos_sim = np.dot(V_q, v_d)/(np.linalg.norm(V_q) * np.linalg.norm(v_d))\n",
    "        q_sim_score.append(cos_sim)\n",
    "        q_sim_score_map_doc_name.append(doc)\n",
    "#         print(cos_sim)\n",
    "#     print(q_sim_score_map_doc_name)\n",
    "    sim_score = [docName for score, docName in sorted(zip(q_sim_score, q_sim_score_map_doc_name), reverse=True)]\n",
    "    end_time+=time.time()-st_time\n",
    "    print(sim_score[:top_k])\n",
    "print(\"Avg time takes to run Boolean Retrieval system for one query: {:.5f} sec\".format(end_time/len(queries_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BM25(query_tokens, postings_list, doc_lengths, k1=1.2, b=0.75):\n",
    "    #Each dimension corresponding to one document\n",
    "    RSVd_vec = np.zeros(len(doc_lengths), dtype=float)\n",
    "    for token in query_tokens:\n",
    "        dft=0\n",
    "        doc_vector = np.zeros(len(doc_lengths), dtype=int)\n",
    "        if (token not in postings_list.keys()):\n",
    "            print(\"'\"+token + \"' was not found in corpus\")\n",
    "        else:\n",
    "            node = postings_list[token]\n",
    "            while node is not None:\n",
    "                dft+=1\n",
    "                doc_vector[node.docID]=node.freq\n",
    "                node=node.next\n",
    "        doc_vector = np.array(doc_vector, dtype=float)\n",
    "#         print(doc_vector)\n",
    "        if dft==0: \n",
    "            RSVd_vec += doc_vector\n",
    "        else:\n",
    "            #Get Local weight\n",
    "            L_d = np.array(list(doc_lengths.values()), dtype=float)\n",
    "            L_avg = np.mean(list(doc_lengths.values()))\n",
    "            local_wt_num = (k1+1)*doc_vector\n",
    "            local_wt_den = k1*(1 - b + (b/L_avg)*L_d) + doc_vector\n",
    "            local_wt = np.divide(local_wt_num, local_wt_den)\n",
    "#             print(\"local_wt: \", local_wt)\n",
    "            #Get Global weight\n",
    "            doc_size = len(doc_lengths)\n",
    "            global_wt = math.log(doc_size / dft)\n",
    "#             print(\"global_wt: \", global_wt)\n",
    "#         print(\"RSVd: \", RSVd_vec)\n",
    "        #Multiply local weigth with global weight\n",
    "        RSVd_vec += local_wt*global_wt\n",
    "    return RSVd_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents retrieved\n",
      "'technology' was not found in corpus\n",
      "'movie' was not found in corpus\n",
      "['P_386', 'D00585', 'T00921', 'T00755', 'L00119']\n",
      "['T00921', 'D00585', 'T00755', 'P_386', 'L00119']\n",
      "Avg time takes to run Boolean Retrieval system for one query: 0.00251 sec\n"
     ]
    }
   ],
   "source": [
    "queries_list = ['person and technology and brahmana but not movie', 'man or indiashow']\n",
    "top_k=5\n",
    "end_time=0\n",
    "print(\"Top {} documents retrieved\".format(top_k))\n",
    "k1 = np.random.uniform(1.2, 2.0)\n",
    "b=0.75\n",
    "for query in queries_list:\n",
    "    st_time=time.time()\n",
    "    #Tokenize query first\n",
    "    q_tokens = word_tokenize(query)\n",
    "    #Remove stop words from query\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_q_tokens = [word for word in q_tokens if word not in stop_words]\n",
    "    #print(new_q_tokens)\n",
    "    RSVd_vec = get_BM25(new_q_tokens, postings_list, doc_lengths, k1, b)\n",
    "#     print(\"Final RSVd_vec: \", RSVd_vec)\n",
    "#     print(list(doc_index.values()))\n",
    "    mapped_doc = [docName for score, docName in sorted(zip(RSVd_vec, list(doc_index.values())), reverse=True)]\n",
    "    end_time+=time.time()-st_time\n",
    "    print(mapped_doc[:top_k])\n",
    "print(\"Avg time takes to run Boolean Retrieval system for one query: {:.5f} sec\".format(end_time/len(queries_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
