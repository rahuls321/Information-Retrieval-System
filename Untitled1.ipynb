{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "import math\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading preprocessed document wise vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vocab_doc_wise_tokenization = np.load('vocab_doc_wise_tokenization.npy', allow_pickle='TRUE').item()\n",
    "vocab_doc_wise_stemming = np.load('vocab_doc_wise_stemming.npy', allow_pickle='TRUE').item()\n",
    "# print(vocab_doc_wise_tokenization)\n",
    "# print(vocab_doc_wise_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_doc_wise_stemming))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmer(stemmer_type):\n",
    "    if(stemmer_type=='porter_stemmer'): stemmer = nltk.PorterStemmer()\n",
    "    elif(stemmer_type=='snowball_stemmer'): stemmer = nltk.SnowballStemmer(language = 'english')\n",
    "    return stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing Snowball stemmer (advanced version of porter_stemmer)\n",
    "stemmer = get_stemmer('snowball_stemmer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Node which has three sub-nodes containing document ID, freq of word in that docID \n",
    "#and next to link with next docID\n",
    "class Node:\n",
    "    def __init__(self, docID, freq=None):\n",
    "        self.docID = docID\n",
    "        self.freq = freq\n",
    "        self.next = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating word freq for each doc\n",
    "def get_word_freq(vocab):\n",
    "    word_freq={}\n",
    "    for word in vocab:\n",
    "        if word in word_freq.keys():\n",
    "            word_freq[word]+=1\n",
    "        else: word_freq[word]=1\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Postings list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_list = {}\n",
    "doc_index = {}\n",
    "ind=0\n",
    "doc_lengths={}\n",
    "for doc_id, vocab in vocab_doc_wise_stemming.items():\n",
    "    word_freq = get_word_freq(vocab)\n",
    "    for word, freq in word_freq.items():\n",
    "        if word in postings_list.keys():\n",
    "            firstNode = postings_list[word]\n",
    "            while firstNode.next is not None:\n",
    "                firstNode = firstNode.next\n",
    "            firstNode.next = Node(ind, freq)\n",
    "        else:\n",
    "            postings_list[word] = Node(ind, freq)\n",
    "    doc_index[ind] = doc_id\n",
    "    doc_lengths[ind] = len(vocab)\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"postings_list.npy\", postings_list)\n",
    "# filehandler = open(b\"../postings_list.pkl\",\"wb\")\n",
    "# pickle.dump(postings_list,filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"postings_list.pkl\",'rb')\n",
    "# postings_list = pickle.load(file)\n",
    "# file.close()\n",
    "# postings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 623, 1: 4948, 2: 13006, 3: 4788, 4: 2307}\n"
     ]
    }
   ],
   "source": [
    "print(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word, node in pp.items():\n",
    "#     print(word, end='->')\n",
    "#     while node is not None:\n",
    "#         print(node.docID, end='->')\n",
    "#         print(node.freq, end=' ')\n",
    "#         node=node.next\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps followed\n",
    "1. Tokenize the query\n",
    "2. Convert infix query expression to postfix query expression using stack approach\n",
    "        a. Check if the given expression is balanced or not\n",
    "        b. Check is there any extra parenthesis in the expression\n",
    "3. Processing two operator only in the query **\\&**(and) , **\\|** (or) and **\\~**(negation) and giving higeher precedence to the former\n",
    "4. Using **snowball_stemmer** as a stemmer algorithm to find the stem word in the given query\n",
    "5. Generate binary vector based on document size and consider negation sign as well while processing\n",
    "6. Find document which contains the query word using **find_matched_doc** function and return a binary vector that shows which document contains that word\n",
    "7. Remove stop words from query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_operator(token):\n",
    "    if token in ['&' , '|']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Precedence of operators\n",
    "def precedence_oper(token):\n",
    "    if token=='&': return 2\n",
    "    elif token=='|': return 1\n",
    "    else: return -1\n",
    "\n",
    "def get_postfix_list(tokens):\n",
    "    stack = []\n",
    "    postfix_list = []\n",
    "    for token in tokens:\n",
    "        #If token is left small bracket '('\n",
    "        if token == '(': stack.append(token)\n",
    "        elif token == ')':\n",
    "            while(len(stack)>0 and stack[-1]!='('):\n",
    "                postfix_list.append(stack.pop())\n",
    "            if len(stack)==0 and token==')':\n",
    "                raise ValueError('Either unnecessary parenthesis or Not a balanced query')\n",
    "            stack.pop()\n",
    "            if len(stack)>0 and stack[-1] == '(':\n",
    "                raise ValueError('Either unnecessary parenthesis or Not a balanced query')\n",
    "        elif is_operator(token):\n",
    "            while(len(stack)>0 and precedence_oper(token) <= precedence_oper(stack[-1])):\n",
    "                postfix_list.append(stack.pop())\n",
    "            stack.append(token)\n",
    "        else: \n",
    "            postfix_list.append(token)\n",
    "    while len(stack)>0:\n",
    "        postfix_list.append(stack.pop())\n",
    "    return postfix_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_preprocessing(q):\n",
    "    #Remove stop words from query\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #Tokenize query first\n",
    "    q_tokens = word_tokenize(q)\n",
    "    updated_q_tokens=[]\n",
    "    connecting_words = {'and':'&', 'or':'|', 'not':'~'}\n",
    "    for t, token in enumerate(q_tokens):\n",
    "        if token in list(connecting_words.keys()):\n",
    "            if token=='not':\n",
    "                if t+1>=len(q_tokens):\n",
    "                    raise ValueError(\"Invalid query!\")\n",
    "                else:\n",
    "                    updated_q_tokens.append('~'+q_tokens[t+1])\n",
    "                    q_tokens.remove(q_tokens[t+1])\n",
    "            else: updated_q_tokens.append(connecting_words[token])\n",
    "        else:\n",
    "            updated_q_tokens.append(token)\n",
    "#     print(updated_q_tokens)\n",
    "    new_q_tokens = [stemmer.stem(word.lower()) for word in updated_q_tokens if word not in stop_words]\n",
    "#     print(new_q_tokens)\n",
    "    #Convert this infix list into postfix list to process operator in right way\n",
    "    q_tokens = get_postfix_list(new_q_tokens)\n",
    "    return q_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_vec(token, postings_list, doc_size):\n",
    "    word_embedd = np.zeros(doc_size, dtype=int)\n",
    "    vocab = postings_list.keys()\n",
    "    negation = False\n",
    "    if token[0]=='~':\n",
    "        negation=True\n",
    "        token=token[1:]\n",
    "    if token not in vocab:\n",
    "        print(\"'\"+token + \"' was not found in the corpus\")\n",
    "        return word_embedd\n",
    "    node = postings_list[token]\n",
    "    while node is not None:\n",
    "        word_embedd[node.docID] = 1\n",
    "        node=node.next\n",
    "    if negation:\n",
    "        word_embedd = np.invert(word_embedd)\n",
    "    return word_embedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matched_doc(query_tokens, postings_list, doc_index, top_k):\n",
    "    \n",
    "    word_embedd_stack = []\n",
    "    doc_size = len(doc_index)\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        if is_operator(token):\n",
    "            if(len(word_embedd_stack)<2): \n",
    "                raise ValueError(\"Query is not correct or use more stopping words\")\n",
    "            first_operand = word_embedd_stack.pop()\n",
    "            second_operand = word_embedd_stack.pop()\n",
    "            \n",
    "            if token=='&': word_embedd_stack.append(first_operand & second_operand)\n",
    "            elif token=='|': word_embedd_stack.append(first_operand | second_operand)\n",
    "            else:\n",
    "                raise ValueError('Can\\'t process this operator: ', token)\n",
    "        else:\n",
    "            st = stemmer.stem(token)\n",
    "            \n",
    "            token_embedd = get_binary_vec(token, postings_list, doc_size)\n",
    "            word_embedd_stack.append(token_embedd)\n",
    "    matched_doc = [doc_index[docID] for docID in np.where(word_embedd_stack[-1])[0]]\n",
    "    return matched_doc[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents retrieved\n",
      "'technolog' was not found in the corpus\n",
      "['P_386', 'T00921', 'D00585', 'L00119', 'T00755']\n",
      "['T00921', 'D00585']\n",
      "Avg time takes to run Boolean Retrieval system for one query: 0.00174 sec\n"
     ]
    }
   ],
   "source": [
    "queries_list = ['person but or technology not human', 'man or indiashow']\n",
    "top_k=5\n",
    "end_time=0\n",
    "print(\"Top {} documents retrieved\".format(top_k))\n",
    "for query in queries_list:\n",
    "    st_time = time.time()\n",
    "    query_tokens = query_preprocessing(query)\n",
    "    matched_doc = find_matched_doc(query_tokens, postings_list, doc_index, top_k)\n",
    "    end_time+=time.time()-st_time\n",
    "    print(matched_doc)\n",
    "print(\"Avg time takes to run Boolean Retrieval system for one query: {:.5f} sec\".format(end_time/len(queries_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['P_386', 'T00921', 'D00585', 'L00119', 'T00755'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_doc_wise_stemming.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(postings_list.keys()).index('return')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to consider\n",
    "1. Tokenize the query first and remove the stopwords from the query\n",
    "2. Find the query vector where each dimension represents freq. of token present in the query\n",
    "3. For fast query processing, find the tf-idf for those token which are present in the query only\n",
    "4. Return top-k documents only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnonASCII(token):\n",
    "    if token in ['.','+','*','?','[','/', '//','\\\\','^','%',']', '$','(',')','{','}','=', '!', '|',':','-']:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_tf_idf(doc, postings_list, doc_doc_index, doc_size):\n",
    "    doc_vector = np.zeros(len(postings_list.keys()), dtype=int)\n",
    "    for token in doc:\n",
    "        if token not in postings_list.keys():\n",
    "            print(\"'\"+token + \"' was not found in corpus\")\n",
    "        else:\n",
    "            node = postings_list[token]\n",
    "            get_docID_freq = 0\n",
    "            dft=0\n",
    "            while node is not None:\n",
    "                if node.docID == doc_doc_index:\n",
    "                    get_docID_freq = node.freq\n",
    "                node=node.next\n",
    "                dft+=1\n",
    "            idx = list(postings_list.keys()).index(token)\n",
    "            doc_vector[idx]=get_docID_freq * math.log(doc_size / dft)\n",
    "    doc_len = np.linalg.norm(doc_vector)\n",
    "    if doc_len!=0:\n",
    "        doc_vector = doc_vector/doc_len\n",
    "    return doc_vector\n",
    "\n",
    "def get_query_vector(query_tokens, postings_list):\n",
    "    query_vector = np.zeros(len(postings_list.keys()), dtype=int)\n",
    "    for token in query_tokens:\n",
    "        if token not in postings_list.keys():\n",
    "            print(\"'\"+token + \"' was not found in corpus\")\n",
    "        else:\n",
    "            idx = list(postings_list.keys()).index(token)\n",
    "            query_vector[idx] += 1\n",
    "    return query_vector\n",
    "\n",
    "def get_scoring_vec(tf_idf_matrix, doc_size):\n",
    "    score_vector = np.zeros(doc_size, dtype=float)\n",
    "    for _, vec in tf_idf_matrix.items():\n",
    "        score_vector += vec\n",
    "    return score_vector\n",
    "\n",
    "def get_mapped_doc(score_vec, doc_index, top_k):\n",
    "    doc_idx_mapping = np.arange(len(doc_index))\n",
    "    get_matched_doc = [doc_index[docID] for score, docID in sorted(zip(score_vec, doc_idx_mapping), reverse=True)]\n",
    "    return get_matched_doc[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_champion_lists(vocab_doc_wise_stemming, postings_list, doc_index):\n",
    "    champion_lists={token:[] for token in list(postings_list.keys())}\n",
    "    doc_name=[]\n",
    "    full_doc_tf_idf=[]\n",
    "    for doc, doc_vocab in vocab_doc_wise_stemming.items():\n",
    "        idx = list(doc_index.values()).index(doc)\n",
    "        doc_tf_idf = get_doc_tf_idf(doc_vocab, postings_list, idx, len(doc_index))\n",
    "        full_doc_tf_idf.append(doc_tf_idf)\n",
    "        doc_name.append(doc)\n",
    "    full_doc_tf_idf = np.array(full_doc_tf_idf)\n",
    "#     print(doc_name)\n",
    "    token_idx=0\n",
    "    for token in list(champion_lists.keys()):\n",
    "#         print(full_doc_tf_idf[:, token_idx])\n",
    "#         print(doc_name)\n",
    "        get_doc_cos_sim_vec = [(score, d_name) for score, d_name in sorted(zip(full_doc_tf_idf[:, token_idx], doc_name), reverse=True)]\n",
    "        champion_lists[token] = get_doc_cos_sim_vec\n",
    "#         print(get_doc_cos_sim_vec)\n",
    "        token_idx+=1\n",
    "    return champion_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_R_ranked_doc(query_tokens, champion_lists, top_r):\n",
    "    top_R_doc_vec = []\n",
    "    for token in query_tokens:\n",
    "        if token not in champion_lists.keys():\n",
    "            print(\"'\"+token + \"' was not found in corpus\")\n",
    "        else:\n",
    "            doc_vec = champion_lists[token][:top_r]\n",
    "            top_R_doc_vec = sum([], top_R_doc_vec+[score_doc_tuple for score_doc_tuple in doc_vec])\n",
    "#     print(top_R_doc_vec)\n",
    "    top_R_doc_vec = sorted(top_R_doc_vec, key=lambda x: x[0], reverse=True)\n",
    "#     print(top_R_doc_vec)\n",
    "    return top_R_doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_score(query_tokens, V_q, vocab_doc_wise_stemming, postings_list, doc_index):\n",
    "    doc_tf_idf_vector={}\n",
    "    q_sim_score=[]\n",
    "    q_sim_score_map_doc_name=[]\n",
    "    for doc, doc_vocab in vocab_doc_wise_stemming.items():\n",
    "        idx = list(doc_index.values()).index(doc)\n",
    "        doc_tf_idf = get_doc_tf_idf(doc_vocab, query_tokens, postings_list, idx, len(doc_index))\n",
    "#         doc_tf_idf_vector[doc] = doc_tf_idf\n",
    "        \n",
    "        v_d = doc_tf_idf\n",
    "        if np.linalg.norm(V_q)==0 or np.linalg.norm(v_d)==0:\n",
    "            cos_sim=0\n",
    "        else: cos_sim = np.dot(V_q, v_d)/(np.linalg.norm(V_q) * np.linalg.norm(v_d))\n",
    "        q_sim_score.append(cos_sim)\n",
    "        q_sim_score_map_doc_name.append(doc)\n",
    "#     print(q_sim_score)\n",
    "#     print(q_sim_score_map_doc_name)\n",
    "    return q_sim_score, q_sim_score_map_doc_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_lists = get_champion_lists(vocab_doc_wise_stemming, postings_list, doc_index)\n",
    "# champion_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents retrieved\n",
      "'technolog' was not found in corpus\n",
      "'movi' was not found in corpus\n",
      "'technolog' was not found in corpus\n",
      "'movi' was not found in corpus\n",
      "['P_386', 'D00585', 'T00921', 'T00755', 'L00119']\n",
      "['T00921', 'T00755', 'P_386', 'L00119', 'D00585']\n",
      "['D00585', 'T00921', 'T00755', 'P_386', 'L00119']\n",
      "['L00119', 'T00755', 'D00585', 'T00921', 'P_386']\n",
      "Avg time takes to run TF-IDF Retrieval system for one query: 0.00304 sec\n"
     ]
    }
   ],
   "source": [
    "queries_list = ['person and technology and brahmana but not movie', 'man or indiashow', \n",
    "                'Treatment of otherwise healthy people is usually not needed',\n",
    "               'Soon after being discharged from the Army Laurents met ballerina Nora Kaye\\\n",
    "               and the two became involved in an on again off again romantic relationship.']\n",
    "top_k=5\n",
    "top_r=10\n",
    "end_time=0\n",
    "print(\"Top {} documents retrieved\".format(top_k))\n",
    "for query in queries_list:\n",
    "    st_time = time.time()\n",
    "    #Tokenize query first\n",
    "    q_tokens = word_tokenize(query)\n",
    "    #Remove stop words from query\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_q_tokens = [stemmer.stem(word.lower()) for word in q_tokens if word not in stop_words and not isnonASCII(word)]\n",
    "#     print(new_q_tokens)\n",
    "    V_q = get_query_vector(new_q_tokens, postings_list)\n",
    "    top_r_doc_union = top_R_ranked_doc(new_q_tokens, champion_lists, top_r)\n",
    "    mapped_doc=[]\n",
    "    for score_vec_t in top_r_doc_union:\n",
    "        if score_vec_t[1] not in mapped_doc:\n",
    "            mapped_doc.append(score_vec_t[1])\n",
    "    end_time+=time.time()-st_time\n",
    "    print(mapped_doc[:top_k])\n",
    "print(\"Avg time takes to run TF-IDF Retrieval system for one query: {:.5f} sec\".format(end_time/len(queries_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the steps considered\n",
    "1. Tokenize the query into tokens and remove the stop words and also remove if there's any non-ascii characters\n",
    "2. Get local weight by modified term frequency formula $$\\frac{(k_1+1)tf_d}{k_1(1-b+b\\frac{L_d}{L_avg}) + tf_d}$$\n",
    "3. Get global weight by inverse doc frequency as the priors aren't given by given formula $$\\log \\frac{n}{df_t}$$\n",
    "4. Get RSVd score using below formula and based on this score, select top k documents $$RSVd = \\sum_{\\forall t \\in q} \\left(\\log \\frac{n}{df_t}\\right) . \\frac{(k_1+1)tf_d}{k_1(1-b+b\\frac{L_d}{L_avg}) + tf_d}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BM25(query_tokens, postings_list, doc_lengths, k1=1.2, b=0.75):\n",
    "    #Each dimension corresponding to one document\n",
    "    RSVd_vec = np.zeros(len(doc_lengths), dtype=float)\n",
    "    for token in query_tokens:\n",
    "        dft=0\n",
    "        doc_vector = np.zeros(len(doc_lengths), dtype=int)\n",
    "        if (token not in postings_list.keys()):\n",
    "            print(\"'\"+token + \"' was not found in corpus\")\n",
    "        else:\n",
    "            node = postings_list[token]\n",
    "            while node is not None:\n",
    "                dft+=1\n",
    "                doc_vector[node.docID]=node.freq\n",
    "                node=node.next\n",
    "        doc_vector = np.array(doc_vector, dtype=float)\n",
    "#         print(doc_vector)\n",
    "        if dft==0: \n",
    "            RSVd_vec += doc_vector\n",
    "        else:\n",
    "            #Get Local weight\n",
    "            L_d = np.array(list(doc_lengths.values()), dtype=float)\n",
    "            L_avg = np.mean(list(doc_lengths.values()))\n",
    "            local_wt_num = (k1+1)*doc_vector\n",
    "            local_wt_den = k1*(1 - b + (b/L_avg)*L_d) + doc_vector\n",
    "            local_wt = np.divide(local_wt_num, local_wt_den)\n",
    "#             print(\"local_wt: \", local_wt)\n",
    "            #Get Global weight\n",
    "            doc_size = len(doc_lengths)\n",
    "            global_wt = math.log(doc_size / dft)\n",
    "#             print(\"global_wt: \", global_wt)\n",
    "#         print(\"RSVd: \", RSVd_vec)\n",
    "        #Multiply local weigth with global weight\n",
    "        RSVd_vec += local_wt*global_wt\n",
    "    return RSVd_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents retrieved\n",
      "'technolog' was not found in corpus\n",
      "'movi' was not found in corpus\n",
      "['P_386', 'D00585', 'T00921', 'T00755', 'L00119']\n",
      "['T00921', 'D00585', 'T00755', 'P_386', 'L00119']\n",
      "['D00585', 'T00921', 'L00119', 'T00755', 'P_386']\n",
      "['L00119', 'T00921', 'D00585', 'T00755', 'P_386']\n",
      "Avg time takes to run BM25 Retrieval system for one query: 0.00144 sec\n"
     ]
    }
   ],
   "source": [
    "queries_list = ['person and technology and brahmana but not movie', 'man or indiashow',\n",
    "                'Treatment of otherwise healthy people is usually not needed',\n",
    "               'Soon after being discharged from the Army Laurents met ballerina Nora Kaye\\\n",
    "               and the two became involved in an on again off again romantic relationship.']\n",
    "top_k=5\n",
    "end_time=0\n",
    "print(\"Top {} documents retrieved\".format(top_k))\n",
    "k1 = np.random.uniform(1.2, 2.0)\n",
    "b=0.75\n",
    "for query in queries_list:\n",
    "    st_time=time.time()\n",
    "    #Tokenize query first\n",
    "    q_tokens = word_tokenize(query)\n",
    "    #Remove stop words from query\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_q_tokens = [stemmer.stem(word.lower()) for word in q_tokens if word not in stop_words and not isnonASCII(word)]\n",
    "    #print(new_q_tokens)\n",
    "    RSVd_vec = get_BM25(new_q_tokens, postings_list, doc_lengths, k1, b)\n",
    "#     print(\"Final RSVd_vec: \", RSVd_vec)\n",
    "#     print(list(doc_index.values()))\n",
    "    mapped_doc = [docName for score, docName in sorted(zip(RSVd_vec, list(doc_index.values())), reverse=True)]\n",
    "    end_time+=time.time()-st_time\n",
    "    print(mapped_doc[:top_k])\n",
    "print(\"Avg time takes to run BM25 Retrieval system for one query: {:.5f} sec\".format(end_time/len(queries_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
