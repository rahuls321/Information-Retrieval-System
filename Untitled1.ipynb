{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# for file in os.listdir('../champion_folder'):\n",
    "#     c_list = np.load(os.path.join('../champion_folder', file), allow_pickle=True)\n",
    "#     print(c_list)\n",
    "#     print(len(c_list[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.arange(10)\n",
    "a[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading preprocessed document wise vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vocab_doc_wise_tokenization = np.load('vocab_doc_wise_tokenization.npy', allow_pickle='TRUE').item()\n",
    "vocab_doc_wise_stemming = np.load('vocab_doc_wise_stemming.npy', allow_pickle='TRUE').item()\n",
    "# print(vocab_doc_wise_tokenization)\n",
    "# print(vocab_doc_wise_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_doc_wise_stemming))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmer(stemmer_type):\n",
    "    if(stemmer_type=='porter_stemmer'): stemmer = nltk.PorterStemmer()\n",
    "    elif(stemmer_type=='snowball_stemmer'): stemmer = nltk.SnowballStemmer(language = 'english')\n",
    "    return stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing Snowball stemmer (advanced version of porter_stemmer)\n",
    "stemmer = get_stemmer('snowball_stemmer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Node which has three sub-nodes containing document ID, freq of word in that docID \n",
    "#and next to link with next docID\n",
    "class Node:\n",
    "    def __init__(self, docID, freq=None):\n",
    "        self.docID = docID\n",
    "        self.freq = freq\n",
    "        self.next = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating word freq for each doc\n",
    "def get_word_freq(vocab):\n",
    "    word_freq={}\n",
    "    for word in vocab:\n",
    "        if word in word_freq.keys():\n",
    "            word_freq[word]+=1\n",
    "        else: word_freq[word]=1\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Postings list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_list = {}\n",
    "doc_index = {}\n",
    "ind=0\n",
    "doc_lengths={}\n",
    "for doc_id, vocab in vocab_doc_wise_stemming.items():\n",
    "    word_freq = get_word_freq(vocab)\n",
    "    for word, freq in word_freq.items():\n",
    "        if word in postings_list.keys():\n",
    "            firstNode = postings_list[word]\n",
    "            while firstNode.next is not None:\n",
    "                firstNode = firstNode.next\n",
    "            firstNode.next = Node(ind, freq)\n",
    "        else:\n",
    "            postings_list[word] = Node(ind, freq)\n",
    "    doc_index[ind] = doc_id\n",
    "    doc_lengths[ind] = len(vocab)\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"postings_list.npy\", postings_list)\n",
    "# filehandler = open(b\"../postings_list.pkl\",\"wb\")\n",
    "# pickle.dump(postings_list,filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"postings_list.pkl\",'rb')\n",
    "# postings_list = pickle.load(file)\n",
    "# file.close()\n",
    "# postings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 623, 1: 4948, 2: 13006, 3: 4788, 4: 2307}\n"
     ]
    }
   ],
   "source": [
    "print(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word, node in postings_list.items():\n",
    "#     print(word, end='->')\n",
    "#     while node is not None:\n",
    "#         print(node.docID, end='->')\n",
    "#         print(node.freq, end=' ')\n",
    "#         node=node.next\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps followed\n",
    "1. Tokenize the query\n",
    "2. Convert infix query expression to postfix query expression using stack approach\n",
    "        a. Check if the given expression is balanced or not\n",
    "        b. Check is there any extra parenthesis in the expression\n",
    "3. Processing two operator only in the query **\\&**(and) , **\\|** (or) and **\\~**(negation) and giving higeher precedence to the former\n",
    "4. Using **snowball_stemmer** as a stemmer algorithm to find the stem word in the given query\n",
    "5. Generate binary vector based on document size and consider negation sign as well while processing\n",
    "6. Find document which contains the query word using **find_matched_doc** function and return a binary vector that shows which document contains that word\n",
    "7. Remove stop words from query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnonASCII(token):\n",
    "    if token in ['.','+','*','?','[','/', '//','\\\\','^','%',']', '$','(',')','{','}','=', '!', '|',':','-', ',', ';']:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_operator(token):\n",
    "    if token in ['&' , '|']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Precedence of operators\n",
    "def precedence_oper(token):\n",
    "    if token=='&': return 2\n",
    "    elif token=='|': return 1\n",
    "    else: return -1\n",
    "\n",
    "def get_postfix_list(tokens):\n",
    "    stack = []\n",
    "    postfix_list = []\n",
    "    for token in tokens:\n",
    "        #If token is left small bracket '('\n",
    "        if token == '(': stack.append(token)\n",
    "        elif token == ')':\n",
    "            while(len(stack)>0 and stack[-1]!='('):\n",
    "                postfix_list.append(stack.pop())\n",
    "            if len(stack)==0 and token==')':\n",
    "                raise ValueError('Either unnecessary parenthesis or Not a balanced query')\n",
    "            stack.pop()\n",
    "            if len(stack)>0 and stack[-1] == '(':\n",
    "                raise ValueError('Either unnecessary parenthesis or Not a balanced query')\n",
    "        elif is_operator(token):\n",
    "            while(len(stack)>0 and precedence_oper(token) <= precedence_oper(stack[-1])):\n",
    "                postfix_list.append(stack.pop())\n",
    "            stack.append(token)\n",
    "        else: \n",
    "            postfix_list.append(token)\n",
    "    while len(stack)>0:\n",
    "        postfix_list.append(stack.pop())\n",
    "    return postfix_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_preprocessing(q):\n",
    "    #Remove stop words from query\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #Tokenize query first\n",
    "    q_tokens = word_tokenize(q)\n",
    "    updated_q_tokens=[]\n",
    "    connecting_words = {'and':'&','AND':'&', 'or':'|','OR':'|', 'not':'~','NOT':'~'}\n",
    "    for t, token in enumerate(q_tokens):\n",
    "        if token in list(connecting_words.keys()):\n",
    "            if token=='not' or token=='NOT':\n",
    "                if t+1>=len(q_tokens):\n",
    "                    #raise ValueError(\"Invalid query!\")\n",
    "                    print(\"Invalid Query!!\")\n",
    "                    continue\n",
    "                else:\n",
    "                    updated_q_tokens.append('~'+q_tokens[t+1])\n",
    "                    q_tokens.remove(q_tokens[t+1])\n",
    "            else: updated_q_tokens.append(connecting_words[token])\n",
    "        else:\n",
    "            updated_q_tokens.append(token)\n",
    "#     print(updated_q_tokens)\n",
    "    new_q_tokens = [stemmer.stem(word.lower()) for word in updated_q_tokens if word not in stop_words and not isnonASCII(word) and len(word)>1]\n",
    "#     print(new_q_tokens)\n",
    "    #Convert this infix list into postfix list to process operator in right way\n",
    "    q_tokens = get_postfix_list(new_q_tokens)\n",
    "    return q_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_vec(token, postings_list, doc_size):\n",
    "    word_embedd = np.zeros(doc_size, dtype=int)\n",
    "    vocab = postings_list.keys()\n",
    "    negation = False\n",
    "    token_not_found=0\n",
    "    if token[0]=='~':\n",
    "        negation=True\n",
    "        token=token[1:]\n",
    "    if token not in vocab:\n",
    "#         print(\"'\"+token + \"' was not found in the corpus\")\n",
    "        token_not_found=1\n",
    "        return word_embedd, token_not_found\n",
    "    node = postings_list[token]\n",
    "    while node is not None:\n",
    "        word_embedd[node.docID] = 1\n",
    "        node=node.next\n",
    "    if negation:\n",
    "        word_embedd = np.invert(word_embedd)\n",
    "    return word_embedd, token_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matched_doc(query_tokens, postings_list, doc_index, top_k):\n",
    "    \n",
    "    word_embedd_stack = []\n",
    "    doc_size = len(doc_index)\n",
    "    token_not_found=[]\n",
    "    for token in query_tokens:\n",
    "        if is_operator(token):\n",
    "            if(len(word_embedd_stack)<2): \n",
    "                raise ValueError(\"Query is not correct or use more stopping words\")\n",
    "            first_operand = word_embedd_stack.pop()\n",
    "            second_operand = word_embedd_stack.pop()\n",
    "            \n",
    "            if token=='&': word_embedd_stack.append(first_operand & second_operand)\n",
    "            elif token=='|': word_embedd_stack.append(first_operand | second_operand)\n",
    "            else:\n",
    "                raise ValueError('Can\\'t process this operator: ', token)\n",
    "        else:\n",
    "            st = stemmer.stem(token)\n",
    "            \n",
    "            token_embedd, flag = get_binary_vec(token, postings_list, doc_size)\n",
    "            if(flag): token_not_found.append(token)\n",
    "            word_embedd_stack.append(token_embedd)\n",
    "    matched_doc = [doc_index[docID] for docID in np.where(word_embedd_stack[-1])[0]]\n",
    "    return matched_doc, token_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(mapped_doc, ground_truth):\n",
    "    if ground_truth in mapped_doc:\n",
    "        return 1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q02': 'Maze generation algorithm or conditional programming', 'Q03': 'Doubly linked list or stack not algorithms', 'Q04': 'Tasuku Honjo and his contriution to the society', 'Q05': 'Yelizaveta Pantueva or other Ukrainian list of young mothers', 'Q06': 'Project MKUltra and humans', 'Q07': 'Bachelor of Arts degree and opportunities available across the world', 'Q08': 'Confine itself to a single revolution of the sun, or but slightly to exceed this limit.', 'Q09': 'A rich widow lived with her daughter and her stepdaughter.', 'Q10': 'What is Overcompleteness in mathematics?', 'Q11': \"The two descendants of Raghu then took hold of her feet; but remembering Gautama's words, she on her part took hold of theirs. And with a collected mind she gave them water for their feet as well as Arghya, and extended to them the rites of hospitality.\", 'Q12': 'The sweet-speeched Subhadra also, saluting him in return and worshipping him repeatedly with bent head, told him all that she wished to be conveyed to her relatives on the paternal side.', 'Q13': 'Matthew Martin Lee Carpenter and baseball', 'Q14': \"Musa is a crater in the northern hemisphere of Saturn's moon Enceladus.\", 'Q15': \"Zeta is also known for its annual Women in Academia banquet which honors Trinity's female professors and college employees for their pursuits in academia, their roles as female leaders on campus, and their achievements for the Trinity College community.\", 'Q16': 'Weiner was raised in Pikesville, Maryland. He attended Wellwood Elementary School, Sudbrook Junior High, Pikesville High School the University of Pennsylvania. In college, he played drums and sang in a rock band called Droylesden Wake. His first produced work was the 1967 Pikesville High Junior Play, an original parody using music from H.M.S. Pinafore and other Gilbert and Sullivan operas.', 'Q17': 'What are the difficulties associated with using animal models for human disease?', 'Q18': 'Mechanical analog computer and computer', 'Q19': 'List of municipalities in Andhra Pradesh and population', 'Q20': 'Sirumalai Lake contains a small lake that was artificially created in the year 2010. In the region of Sirumalai, the Agasthiarpuram is a holy place where siddhas (monks) have lived since ancient times. The area is surrounded with medicinal herbs and plants.'}\n",
      "{'Q02': 'C00505', 'Q03': 'C00515', 'Q04': 'D00003', 'Q05': 'D00263', 'Q06': 'D00022', 'Q07': 'L00003', 'Q08': 'L00091', 'Q09': 'L00289', 'Q10': 'M00256', 'Q11': 'P01049', 'Q12': 'P_238', 'Q13': 'R00135', 'Q14': 'R00147', 'Q15': 'R00285', 'Q16': 'R00423', 'Q17': 'S00166', 'Q18': 'S00267', 'Q19': 'T00169', 'Q20': 'T00510'}\n"
     ]
    }
   ],
   "source": [
    "b_query_file='boolean_query.txt'\n",
    "b_query_ground_truth='boolean query answer.txt'\n",
    "with codecs.open(b_query_file, mode='r', encoding='utf-8') as input_file:\n",
    "    next(input_file)\n",
    "    b_queries={}\n",
    "    for line in input_file:\n",
    "        query = line.strip().split('\\t')\n",
    "        b_queries[query[0]] = query[1]\n",
    "print(b_queries)\n",
    "with codecs.open(b_query_ground_truth, mode='r', encoding='utf-8') as input_file:\n",
    "    next(input_file)\n",
    "    b_queries_o_answer={}\n",
    "    for line in input_file:\n",
    "        query = line.strip().split('\\t')\n",
    "        b_queries_o_answer[query[0]] = query[1]\n",
    "print(b_queries_o_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents retrieved\n",
      "Ground truth doc:  C00505\n",
      "['D00585']\n",
      "Ground truth doc:  C00515\n",
      "[]\n",
      "Ground truth doc:  D00003\n",
      "['D00585', 'T00755']\n",
      "Ground truth doc:  D00263\n",
      "['P_386', 'D00585', 'L00119']\n",
      "Ground truth doc:  D00022\n",
      "['D00585']\n",
      "Ground truth doc:  L00003\n",
      "['T00921', 'D00585', 'L00119']\n",
      "Ground truth doc:  L00091\n",
      "['T00921', 'D00585', 'L00119', 'T00755']\n",
      "Ground truth doc:  L00289\n",
      "[]\n",
      "Ground truth doc:  M00256\n",
      "['T00921']\n",
      "Ground truth doc:  P01049\n",
      "['D00585']\n",
      "Ground truth doc:  P_238\n",
      "['T00921', 'D00585', 'L00119', 'T00755']\n",
      "Ground truth doc:  R00135\n",
      "[]\n",
      "Ground truth doc:  R00147\n",
      "[]\n",
      "Ground truth doc:  R00285\n",
      "['T00921']\n",
      "Ground truth doc:  R00423\n",
      "[]\n",
      "Ground truth doc:  S00166\n",
      "['D00585']\n",
      "Ground truth doc:  S00267\n",
      "[]\n",
      "Ground truth doc:  T00169\n",
      "['D00585']\n",
      "Ground truth doc:  T00510\n",
      "[]\n",
      "Final score:  0\n",
      "Avg time takes to run Boolean Retrieval system for one query: 0.00237 sec\n"
     ]
    }
   ],
   "source": [
    "queries_list = b_queries ## ['person but or technology NOT', 'man or indiashow', 'daughter']\n",
    "top_k=5\n",
    "end_time=0\n",
    "score=0\n",
    "print(\"Top {} documents retrieved\".format(top_k))\n",
    "for query_idx, query in queries_list.items():\n",
    "    st_time = time.time()\n",
    "    query_tokens = query_preprocessing(query)\n",
    "    matched_doc, token_not_found = find_matched_doc(query_tokens, postings_list, doc_index, top_k)\n",
    "    end_time+=time.time()-st_time\n",
    "    print(\"Ground truth doc: \", b_queries_o_answer[query_idx])\n",
    "    score+=get_score(matched_doc[:top_k], b_queries_o_answer[query_idx])\n",
    "    print(matched_doc[:top_k])\n",
    "#     print(\"These Tokens not found in the corpus: \", token_not_found)\n",
    "print(\"Final score: \", score)\n",
    "print(\"Avg time takes to run Boolean Retrieval system for one query: {:.5f} sec\".format(end_time/len(queries_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['P_386', 'T00921', 'D00585', 'L00119', 'T00755'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_doc_wise_stemming.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(postings_list.keys()).index('return')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to consider\n",
    "1. Tokenize the query first and remove the stopwords from the query\n",
    "2. Find the query vector where each dimension represents freq. of token present in the query\n",
    "3. For fast query processing, find the tf-idf for those token which are present in the query only\n",
    "4. Return top-k documents only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q02': 'Define Static and Dynamic polymorphism in c++?', 'Q03': 'Computer software has to be \"loaded\" into the computer\\'s storage (such as the hard drive or memory). Once the software has loaded, the computer is able to execute the software. This involves passing instructions from the application software, through the system software, to the hardware which ultimately receives the instruction as machine code.', 'Q04': 'Who was Tasuku Honjo? What was his contriution to the society?', 'Q05': 'How harmful is coronavirus for animals? What are the symptoms of this virus on animals? How can we protect animals from this?', 'Q06': 'What is Project MKUltra? How is it related with humans?', 'Q07': 'What is the scope of Bachelor of Arts degree? What are the opportunities available across the world?', 'Q08': 'Epic poetry agrees with Tragedy in so far as it is an imitation in verse of characters of a higher type. They differ in that Epic poetry admits but one kind of meter and is narrative in form. They differ, again, in their length: for Tragedy endeavors, as far as possible, to confine itself to a single revolution of the sun, or but slightly to exceed this limit, whereas the Epic action has no limits of time. This, then, is a second point of difference', 'Q09': 'A rich widow lived with her daughter and her stepdaughter. The widow favored her younger biological daughter, allowing her to become spoiled and idle while her older stepdaughter was left to do all the work. Every day the stepdaughter would sit outside the cottage and spin beside the well. One day, she pricked her finger on the point of the spindle.', 'Q10': 'What is Overcompleteness in mathematics?', 'Q11': \"The two descendants of Raghu then took hold of her feet; but remembering Gautama's words, she on her part took hold of theirs. And with a collected mind she gave them water for their feet as well as Arghya, and extended to them the rites of hospitality.\", 'Q12': 'The sweet-speeched Subhadra also, saluting him in return and worshipping him repeatedly with bent head, told him all that she wished to be conveyed to her relatives on the paternal side. And bidding her farewell and uttering benedictions on his handsome sister, he of the Vrishni race, next saw Draupadi and Dhaumya. That best of men duly made obeisance unto Dhaumya, and consoling Draupadi obtained leave from her. Then the learned and mighty Krishna, accompanied by Partha, went to his cousins.', 'Q13': 'Who was Matthew Martin Lee Carpenter? Why he is so famous in baseball?', 'Q14': \"Musa is a crater in the northern hemisphere of Saturn's moon Enceladus.\", 'Q15': \"Zeta is also known for its annual Women in Academia banquet which honors Trinity's female professors and college employees for their pursuits in academia, their roles as female leaders on campus, and their achievements for the Trinity College community. Participants and speakers at the event explore the continuing issue of women's roles in higher education, and to celebrate the triumphs of the women in academia at our college. Interest in the work and achievements of Zeta Omega Eta have led to the publication of several articles regarding the organization in Bust magazine, the Hartford Courant, the Trinity Tripod, Bitch magazine, and salon.com among others.\", 'Q16': 'Weiner was raised in Pikesville, Maryland. He attended Wellwood Elementary School, Sudbrook Junior High, Pikesville High School and the University of Pennsylvania. In college, he played drums and sang in a rock band called Droylesden Wake. His first produced work was the 1967 Pikesville High Junior Play, an original parody using music from H.M.S. Pinafore and other Gilbert and Sullivan operas.', 'Q17': 'What are the difficulties associated with using animal models for human disease?', 'Q18': 'History of Mechanical analog computer. what is the necessity of this computer?', 'Q19': 'List of municipalities in Andhra Pradesh. what is the population of Andhra Pradesh?', 'Q20': 'Sirumalai Lake contains a small lake that was artificially created in the year 2010. In the region of Sirumalai, the Agasthiarpuram is a holy place where siddhas (monks) have lived since ancient times. The area is surrounded with medicinal herbs and plants.'}\n",
      "{'Q02': 'C00002', 'Q03': 'C00009', 'Q04': 'D00003', 'Q05': 'D00019', 'Q06': 'D00022', 'Q07': 'L00003', 'Q08': 'L00091', 'Q09': 'L00289', 'Q10': 'M00256', 'Q11': 'P01049', 'Q12': 'P_238', 'Q13': 'R00135', 'Q14': 'R00147', 'Q15': 'R00285', 'Q16': 'R00423', 'Q17': 'S00166', 'Q18': 'S00267', 'Q19': 'T00169', 'Q20': 'T00510'}\n"
     ]
    }
   ],
   "source": [
    "query_file='query.txt'\n",
    "query_ground_truth='query answer.txt'\n",
    "with codecs.open(query_file, mode='r', encoding='utf-8') as input_file:\n",
    "    next(input_file)\n",
    "    queries={}\n",
    "    for line in input_file:\n",
    "        query = line.strip().split('\\t')\n",
    "        queries[query[0]] = query[1]\n",
    "print(queries)\n",
    "with codecs.open(query_ground_truth, mode='r', encoding='utf-8') as input_file:\n",
    "    next(input_file)\n",
    "    queries_o_answer={}\n",
    "    for line in input_file:\n",
    "        query = line.strip().split('\\t')\n",
    "        queries_o_answer[query[0]] = query[1]\n",
    "print(queries_o_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_tf_idf(doc, postings_list, doc_doc_index, doc_size):\n",
    "    doc_vector = np.zeros(len(postings_list.keys()), dtype=int)\n",
    "    for token in doc:\n",
    "        if token not in postings_list.keys():\n",
    "            print(\"'\"+token + \"' was not found in corpus\")\n",
    "        else:\n",
    "            node = postings_list[token]\n",
    "            get_docID_freq = 0\n",
    "            dft=0\n",
    "            while node is not None:\n",
    "                if node.docID == doc_doc_index:\n",
    "                    get_docID_freq = node.freq\n",
    "                node=node.next\n",
    "                dft+=1\n",
    "            idx = list(postings_list.keys()).index(token)\n",
    "            doc_vector[idx]=get_docID_freq * math.log(doc_size / dft)\n",
    "    doc_len = np.linalg.norm(doc_vector)\n",
    "    if doc_len!=0:\n",
    "        doc_vector = doc_vector/doc_len\n",
    "    return doc_vector\n",
    "\n",
    "def get_query_vector(query_tokens, postings_list):\n",
    "    query_vector = np.zeros(len(postings_list.keys()), dtype=int)\n",
    "    for token in query_tokens:\n",
    "        if token not in postings_list.keys():\n",
    "            #print(\"'\"+token + \"' was not found in corpus\")\n",
    "            continue\n",
    "        else:\n",
    "            idx = list(postings_list.keys()).index(token)\n",
    "            query_vector[idx] += 1\n",
    "    return query_vector\n",
    "\n",
    "def get_scoring_vec(tf_idf_matrix, doc_size):\n",
    "    score_vector = np.zeros(doc_size, dtype=float)\n",
    "    for _, vec in tf_idf_matrix.items():\n",
    "        score_vector += vec\n",
    "    return score_vector\n",
    "\n",
    "def get_mapped_doc(score_vec, doc_index, top_k):\n",
    "    doc_idx_mapping = np.arange(len(doc_index))\n",
    "    get_matched_doc = [doc_index[docID] for score, docID in sorted(zip(score_vec, doc_idx_mapping), reverse=True)]\n",
    "    return get_matched_doc[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_champion_lists(vocab_doc_wise_stemming, postings_list, doc_index):\n",
    "    champion_lists={token:[] for token in list(postings_list.keys())}\n",
    "    doc_name=[]\n",
    "    full_doc_tf_idf=[]\n",
    "    for doc, doc_vocab in vocab_doc_wise_stemming.items():\n",
    "        idx = list(doc_index.values()).index(doc)\n",
    "        doc_tf_idf = get_doc_tf_idf(doc_vocab, postings_list, idx, len(doc_index))\n",
    "        full_doc_tf_idf.append(doc_tf_idf)\n",
    "        doc_name.append(doc)\n",
    "    full_doc_tf_idf = np.array(full_doc_tf_idf)\n",
    "#     print(doc_name)\n",
    "    token_idx=0\n",
    "    for token in list(champion_lists.keys()):\n",
    "#         print(full_doc_tf_idf[:, token_idx])\n",
    "#         print(doc_name)\n",
    "        get_doc_cos_sim_vec = [(score, d_name) for score, d_name in sorted(zip(full_doc_tf_idf[:, token_idx], doc_name), reverse=True)]\n",
    "        champion_lists[token] = get_doc_cos_sim_vec\n",
    "#         print(get_doc_cos_sim_vec)\n",
    "        token_idx+=1\n",
    "    return champion_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_R_ranked_doc(query_tokens, champion_lists, top_r):\n",
    "    top_R_doc_vec = []\n",
    "    token_not_found=[]\n",
    "    for token in query_tokens:\n",
    "        if token not in champion_lists.keys():\n",
    "            ##print(\"'\"+token + \"' was not found in corpus\")\n",
    "            token_not_found.append(token)\n",
    "        else:\n",
    "            doc_vec = champion_lists[token][:top_r]\n",
    "            top_R_doc_vec = sum([], top_R_doc_vec+[score_doc_tuple for score_doc_tuple in doc_vec])\n",
    "#     print(top_R_doc_vec)\n",
    "    top_R_doc_vec = sorted(top_R_doc_vec, key=lambda x: x[0], reverse=True)\n",
    "#     print(top_R_doc_vec)\n",
    "    return top_R_doc_vec, token_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_score(query_tokens, V_q, vocab_doc_wise_stemming, postings_list, doc_index):\n",
    "    doc_tf_idf_vector={}\n",
    "    q_sim_score=[]\n",
    "    q_sim_score_map_doc_name=[]\n",
    "    for doc, doc_vocab in vocab_doc_wise_stemming.items():\n",
    "        idx = list(doc_index.values()).index(doc)\n",
    "        doc_tf_idf = get_doc_tf_idf(doc_vocab, query_tokens, postings_list, idx, len(doc_index))\n",
    "#         doc_tf_idf_vector[doc] = doc_tf_idf\n",
    "        \n",
    "        v_d = doc_tf_idf\n",
    "        if np.linalg.norm(V_q)==0 or np.linalg.norm(v_d)==0:\n",
    "            cos_sim=0\n",
    "        else: cos_sim = np.dot(V_q, v_d)/(np.linalg.norm(V_q) * np.linalg.norm(v_d))\n",
    "        q_sim_score.append(cos_sim)\n",
    "        q_sim_score_map_doc_name.append(doc)\n",
    "#     print(q_sim_score)\n",
    "#     print(q_sim_score_map_doc_name)\n",
    "    return q_sim_score, q_sim_score_map_doc_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_lists = get_champion_lists(vocab_doc_wise_stemming, postings_list, doc_index)\n",
    "# champion_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents retrieved\n",
      "Ground truth doc:  C00002\n",
      "Mapped doc:  ['D00585', 'T00921', 'T00755', 'P_386', 'L00119']\n",
      "Ground truth doc:  C00009\n",
      "Mapped doc:  ['D00585', 'P_386', 'T00755', 'L00119', 'T00921']\n",
      "Ground truth doc:  D00003\n",
      "Mapped doc:  ['T00755', 'T00921', 'D00585', 'P_386', 'L00119']\n",
      "Ground truth doc:  D00019\n",
      "Mapped doc:  ['D00585', 'T00921', 'L00119', 'T00755', 'P_386']\n",
      "Ground truth doc:  D00022\n",
      "Mapped doc:  ['D00585', 'L00119', 'T00921', 'T00755', 'P_386']\n",
      "Ground truth doc:  L00003\n",
      "Mapped doc:  ['L00119', 'T00755', 'T00921', 'D00585', 'P_386']\n",
      "Ground truth doc:  L00091\n",
      "Mapped doc:  ['T00755', 'D00585', 'L00119', 'T00921', 'P_386']\n",
      "Ground truth doc:  L00289\n",
      "Mapped doc:  ['L00119', 'T00755', 'P_386', 'D00585', 'T00921']\n",
      "Ground truth doc:  M00256\n",
      "Mapped doc:  ['T00921', 'D00585', 'T00755', 'P_386', 'L00119']\n",
      "Ground truth doc:  P01049\n",
      "Mapped doc:  ['T00755', 'T00921', 'P_386', 'L00119', 'D00585']\n",
      "Ground truth doc:  P_238\n",
      "Mapped doc:  ['P_386', 'L00119', 'T00755', 'T00921', 'D00585']\n",
      "Ground truth doc:  R00135\n",
      "Mapped doc:  ['L00119', 'D00585', 'T00921', 'T00755', 'P_386']\n",
      "Ground truth doc:  R00147\n",
      "Mapped doc:  ['T00921', 'T00755', 'P_386', 'L00119', 'D00585']\n",
      "Ground truth doc:  R00285\n",
      "Mapped doc:  ['L00119', 'P_386', 'D00585', 'T00755', 'T00921']\n",
      "Ground truth doc:  R00423\n",
      "Mapped doc:  ['L00119', 'T00921', 'T00755', 'P_386', 'D00585']\n",
      "Ground truth doc:  S00166\n",
      "Mapped doc:  ['D00585', 'T00921', 'T00755', 'P_386', 'L00119']\n",
      "Ground truth doc:  S00267\n",
      "Mapped doc:  ['D00585', 'T00921', 'T00755', 'P_386', 'L00119']\n",
      "Ground truth doc:  T00169\n",
      "Mapped doc:  ['T00755', 'D00585', 'L00119', 'T00921', 'P_386']\n",
      "Ground truth doc:  T00510\n",
      "Mapped doc:  ['P_386', 'T00921', 'D00585', 'T00755', 'L00119']\n",
      "Final score:  0\n",
      "Avg time takes to run TF-IDF Retrieval system for one query: 0.00509 sec\n"
     ]
    }
   ],
   "source": [
    "# queries_list = ['person and technology and brahmana but not movie', 'man or indiashow', \n",
    "#                 'Treatment of otherwise healthy people is usually not needed',\n",
    "#                'Soon after being discharged from the Army Laurents met ballerina Nora Kaye\\\n",
    "#                and the two became involved in an on again off again romantic relationship.']\n",
    "queries_list = queries #list(queries.values())\n",
    "top_k=5\n",
    "top_r=10\n",
    "end_time=0\n",
    "print(\"Top {} documents retrieved\".format(top_k))\n",
    "score=0\n",
    "for query_idx, query in queries_list.items():\n",
    "    st_time = time.time()\n",
    "    #Tokenize query first\n",
    "    q_tokens = word_tokenize(query)\n",
    "    #Remove stop words from query\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_q_tokens = [stemmer.stem(word.lower()) for word in q_tokens if word not in stop_words and not isnonASCII(word)]\n",
    "#     print(new_q_tokens)\n",
    "    V_q = get_query_vector(new_q_tokens, postings_list)\n",
    "    top_r_doc_union, token_not_found = top_R_ranked_doc(new_q_tokens, champion_lists, top_r)\n",
    "    mapped_doc=[]\n",
    "    for score_vec_t in top_r_doc_union:\n",
    "        if score_vec_t[1] not in mapped_doc:\n",
    "            mapped_doc.append(score_vec_t[1])\n",
    "    end_time+=time.time()-st_time\n",
    "    print(\"Ground truth doc: \", queries_o_answer[query_idx])\n",
    "    score+=get_score(mapped_doc[:top_k], queries_o_answer[query_idx])\n",
    "    print(\"Mapped doc: \", mapped_doc[:top_k])\n",
    "#     print(\"These Tokens not found in the corpus: \", token_not_found)\n",
    "print(\"Final score: \", score)\n",
    "print(\"Avg time takes to run TF-IDF Retrieval system for one query: {:.5f} sec\".format(end_time/len(queries_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the steps considered\n",
    "1. Tokenize the query into tokens and remove the stop words and also remove if there's any non-ascii characters\n",
    "2. Get local weight by modified term frequency formula $$\\frac{(k_1+1)tf_d}{k_1(1-b+b\\frac{L_d}{L_avg}) + tf_d}$$\n",
    "3. Get global weight by inverse doc frequency as the priors aren't given by given formula $$\\log \\frac{n}{df_t}$$\n",
    "4. Get RSVd score using below formula and based on this score, select top k documents $$RSVd = \\sum_{\\forall t \\in q} \\left(\\log \\frac{n}{df_t}\\right) . \\frac{(k_1+1)tf_d}{k_1(1-b+b\\frac{L_d}{L_avg}) + tf_d}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BM25(query_tokens, postings_list, doc_lengths, k1=1.2, b=0.75):\n",
    "    #Each dimension corresponding to one document\n",
    "    RSVd_vec = np.zeros(len(doc_lengths), dtype=float)\n",
    "    token_not_found=[]\n",
    "    for token in query_tokens:\n",
    "        dft=0\n",
    "        doc_vector = np.zeros(len(doc_lengths), dtype=int)\n",
    "        if (token not in postings_list.keys()):\n",
    "            #print(\"'\"+token + \"' was not found in corpus\")\n",
    "            token_not_found.append(token)\n",
    "        else:\n",
    "            node = postings_list[token]\n",
    "            while node is not None:\n",
    "                dft+=1\n",
    "                doc_vector[node.docID]=node.freq\n",
    "                node=node.next\n",
    "        doc_vector = np.array(doc_vector, dtype=float)\n",
    "#         print(doc_vector)\n",
    "        if dft==0: \n",
    "            RSVd_vec += doc_vector\n",
    "        else:\n",
    "            #Get Local weight\n",
    "            L_d = np.array(list(doc_lengths.values()), dtype=float)\n",
    "            L_avg = np.mean(list(doc_lengths.values()))\n",
    "            local_wt_num = (k1+1)*doc_vector\n",
    "            local_wt_den = k1*(1 - b + (b/L_avg)*L_d) + doc_vector\n",
    "            local_wt = np.divide(local_wt_num, local_wt_den)\n",
    "#             print(\"local_wt: \", local_wt)\n",
    "            #Get Global weight\n",
    "            doc_size = len(doc_lengths)\n",
    "            global_wt = math.log(doc_size / dft)\n",
    "#             print(\"global_wt: \", global_wt)\n",
    "#         print(\"RSVd: \", RSVd_vec)\n",
    "        #Multiply local weigth with global weight\n",
    "            RSVd_vec += local_wt*global_wt\n",
    "    return RSVd_vec, token_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents retrieved\n",
      "Ground truth doc:  C00002\n",
      "Mapped doc:  ['D00585', 'T00921', 'T00755', 'P_386', 'L00119']\n",
      "Ground truth doc:  C00009\n",
      "Mapped doc:  ['D00585', 'L00119', 'T00921', 'P_386', 'T00755']\n",
      "Ground truth doc:  D00003\n",
      "Mapped doc:  ['D00585', 'T00755', 'P_386', 'T00921', 'L00119']\n",
      "Ground truth doc:  D00019\n",
      "Mapped doc:  ['D00585', 'L00119', 'P_386', 'T00921', 'T00755']\n",
      "Ground truth doc:  D00022\n",
      "Mapped doc:  ['D00585', 'L00119', 'T00921', 'P_386', 'T00755']\n",
      "Ground truth doc:  L00003\n",
      "Mapped doc:  ['D00585', 'P_386', 'T00921', 'T00755', 'L00119']\n",
      "Ground truth doc:  L00091\n",
      "Mapped doc:  ['D00585', 'L00119', 'T00921', 'P_386', 'T00755']\n",
      "Ground truth doc:  L00289\n",
      "Mapped doc:  ['P_386', 'D00585', 'L00119', 'T00755', 'T00921']\n",
      "Ground truth doc:  M00256\n",
      "Mapped doc:  ['T00921', 'P_386', 'D00585', 'T00755', 'L00119']\n",
      "Ground truth doc:  P01049\n",
      "Mapped doc:  ['T00921', 'P_386', 'T00755', 'D00585', 'L00119']\n",
      "Ground truth doc:  P_238\n",
      "Mapped doc:  ['L00119', 'P_386', 'T00921', 'D00585', 'T00755']\n",
      "Ground truth doc:  R00135\n",
      "Mapped doc:  ['L00119', 'D00585', 'T00921', 'T00755', 'P_386']\n",
      "Ground truth doc:  R00147\n",
      "Mapped doc:  ['L00119', 'T00921', 'T00755', 'P_386', 'D00585']\n",
      "Ground truth doc:  R00285\n",
      "Mapped doc:  ['D00585', 'L00119', 'T00921', 'T00755', 'P_386']\n",
      "Ground truth doc:  R00423\n",
      "Mapped doc:  ['L00119', 'D00585', 'T00921', 'T00755', 'P_386']\n",
      "Ground truth doc:  S00166\n",
      "Mapped doc:  ['D00585', 'L00119', 'T00921', 'P_386', 'T00755']\n",
      "Ground truth doc:  S00267\n",
      "Mapped doc:  ['D00585', 'T00921', 'T00755', 'L00119', 'P_386']\n",
      "Ground truth doc:  T00169\n",
      "Mapped doc:  ['T00755', 'T00921', 'D00585', 'L00119', 'P_386']\n",
      "Ground truth doc:  T00510\n",
      "Mapped doc:  ['D00585', 'T00921', 'T00755', 'P_386', 'L00119']\n",
      "Final score:  0\n",
      "Avg time takes to run BM25 Retrieval system for one query: 0.00322 sec\n"
     ]
    }
   ],
   "source": [
    "queries_list = ['person and technology and brahmana but not movie', 'man or indiashow',\n",
    "                'Treatment of otherwise healthy people is usually not needed',\n",
    "               'Soon after being discharged from the Army Laurents met ballerina Nora Kaye\\\n",
    "               and the two became involved in an on again off again romantic relationship.']\n",
    "\n",
    "queries_list = queries\n",
    "top_k=5\n",
    "end_time=0\n",
    "print(\"Top {} documents retrieved\".format(top_k))\n",
    "k1 = np.random.uniform(1.2, 2.0)\n",
    "b=0.75\n",
    "score=0\n",
    "for query_idx, query in queries_list.items():\n",
    "    st_time=time.time()\n",
    "    #Tokenize query first\n",
    "    q_tokens = word_tokenize(query)\n",
    "    #Remove stop words from query\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_q_tokens = [stemmer.stem(word.lower()) for word in q_tokens if word not in stop_words and not isnonASCII(word)]\n",
    "    #print(new_q_tokens)\n",
    "    RSVd_vec, token_not_found = get_BM25(new_q_tokens, postings_list, doc_lengths, k1, b)\n",
    "#     print(\"Final RSVd_vec: \", RSVd_vec)\n",
    "#     print(list(doc_index.values()))\n",
    "    mapped_doc = [docName for score, docName in sorted(zip(RSVd_vec, list(doc_index.values())), reverse=True)]\n",
    "    print(\"Ground truth doc: \", queries_o_answer[query_idx])\n",
    "    score+=get_score(mapped_doc[:top_k], queries_o_answer[query_idx])\n",
    "    end_time+=time.time()-st_time\n",
    "    print(\"Mapped doc: \", mapped_doc[:top_k])\n",
    "#     print(\"These Tokens not found in the corpus: \", token_not_found)\n",
    "print(\"Final score: \", score)\n",
    "print(\"Avg time takes to run BM25 Retrieval system for one query: {:.5f} sec\".format(end_time/len(queries_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
